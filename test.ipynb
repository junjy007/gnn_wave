{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1313e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py\n",
    "%run dataset.py\n",
    "%run models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "832c301e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import GNN\n",
    "from dataset import MyGraphDataset, load_wave_data, normalise_data\n",
    "from config import Config\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "from scipy.io import savemat\n",
    "\n",
    "cfg = Config(\n",
    "    snapshots=3,\n",
    "    data_dir=\"sample_data\",\n",
    "    data_fname=\"sample_wave_data.nc\", \n",
    "    neighbourhood_size=5,\n",
    "    normalization=\"norm_01\",\n",
    "    train_period=(0, 90),\n",
    "    test_period=(90, 119),\n",
    "    convolution_kernels = (64, 128),\n",
    "    node_var_observ= ['hs', 'ub_bot', 'wlen', 'pwave_bot', 'tpeak', 'dirm'],\n",
    "    node_var_target=['hs', 'ub_bot', 'wlen', 'pwave_bot', 'tpeak', 'dirm'],\n",
    "    train_batch_size=4,\n",
    "    forward_time=1,\n",
    "    max_epoches=10,\n",
    "    test_batch_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f7b5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = load_wave_data(cfg)\n",
    "d['tpeak'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746bd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)   \n",
    "    \n",
    "def main(config:Config):\n",
    "    trn_dl, test_dl = prepare_train_test_dataloaders(cfg)\n",
    "    # setup the model and optimiser\n",
    "    model = GNN(cfg)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    r2_scores = []\n",
    "    mse =[]\n",
    " \n",
    "    # Create lists to store training and testing data\n",
    "    #train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for epoch in range(cfg.max_epoches):\n",
    "        for i, (data_x, y) in enumerate(trn_dl):\n",
    "            optimizer.zero_grad()            #clar gradients from previous iteration\n",
    "\n",
    "            # compute the loss for this batch\n",
    "            # TODO: to wrap in a proper loss function\n",
    "            tmp = model(data_x.x, data_x.edge_index)\n",
    "            batch_pred = unbatch(tmp, data_x.batch)\n",
    "            \n",
    "            loss = 0\n",
    "            for pi, yi in zip(batch_pred, y):\n",
    "                loss += loss_fn(pi, yi)\n",
    "\n",
    "            \n",
    "            loss.backward()                   #gradients of the model's parameters are computed with respect to the loss using backpropagation\n",
    "            optimizer.step()                  #The optimizer updates the model's parameters based on the computed gradients using the chosen optimization algorithm (e.g., Adam)\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            y_true_int = y.detach().numpy()\n",
    "            y_true = y_true_int.reshape(-1, 6)\n",
    "            y_pred = torch.cat(batch_pred).detach().numpy()\n",
    "            ss_res = np.sum((y_true - y_pred)**2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            r2_scores.append(r2)\n",
    "            \n",
    "    \n",
    "            print(f\"epoch {epoch}, batch {i}: loss {loss.item():.5f}, R2 {r2:.5f}\")\n",
    "    \n",
    "    # Test the model using test data\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_r2 = 0\n",
    "    test_loss = 0\n",
    "    for i, (data_x, y) in enumerate(test_dl):\n",
    "        test_data_out = model(data_x.x, data_x.edge_index)\n",
    "        batch_pred = unbatch(test_data_out, data_x.batch)\n",
    "        \n",
    "        y_true_int = y.detach().numpy()\n",
    "        y_true = y_true_int.reshape(-1, 6)\n",
    "        y_pred = torch.cat(batch_pred).detach().numpy()\n",
    "        ss_res = np.sum((y_true - y_pred)**2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        test_r2 += r2\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        test_loss += mse\n",
    "        \n",
    "        # Create dictionaries for lon, lat, time, and variable data\n",
    "        # Save for prediction results\n",
    "        data = load_wave_data(cfg)\n",
    "        lon = data['lon']\n",
    "        lat = data['lat']\n",
    "        time= data['t']\n",
    "        lon_lat_time_data = {\n",
    "            'lon': lon,  # Use data_x.node_index as an index to extract lon values\n",
    "            'lat': lat,  # Use data_x.node_index as an index to extract lat values\n",
    "            'time': time,  # Use data_x.time as an index to extract time values\n",
    "        }\n",
    "        \n",
    "        variable_pred_data = {\n",
    "            'hs': y_pred[:, 0],  # Adjust the index as needed\n",
    "            'ub_bot': y_pred[:, 1],  # Adjust the index as needed\n",
    "            'wlen': y_pred[:, 2],  # Adjust the index as needed\n",
    "            'pwave_bot': y_pred[:, 3],  # Adjust the index as needed\n",
    "            'tpeak': y_pred[:, 4],  # Adjust the index as needed\n",
    "            'dirm' : y_pred[:, 5],\n",
    "        }\n",
    "        \n",
    "        variable_true_data = {\n",
    "            'hs': y_true[:, 0],  # Adjust the index as needed\n",
    "            'ub_bot': y_true[:, 1],  # Adjust the index as needed\n",
    "            'wlen': y_true[:, 2],  # Adjust the index as needed\n",
    "            'pwave_bot': y_true[:, 3],  # Adjust the index as needed\n",
    "            'tpeak': y_true[:, 4],  # Adjust the index as needed\n",
    "            'dirm' : y_true[:, 5],\n",
    "        }\n",
    "        \n",
    "        test_data.append({\n",
    "            'lon_lat_time_data': lon_lat_time_data,\n",
    "            'variable_pred_data': variable_pred_data,\n",
    "            'variable_true_data': variable_true_data\n",
    "        })\n",
    "                \n",
    "    test_r2 /= len(test_dl)    \n",
    "    # Calculate average test loss (mean squared error)\n",
    "    average_mse = test_loss / len(test_dl)\n",
    "    # Calculate RMSE from average MSE\n",
    "    rmse = math.sqrt(average_mse)\n",
    "\n",
    "    print(f\"Test R2: {test_r2:.5f}, loss {rmse:.5f}\")     \n",
    "    \n",
    "    # Convert the lists to tensors if needed\n",
    "    test_data = np.array(test_data)\n",
    "    \n",
    "    # Save training and testing data to separate files\n",
    "    save_data(test_data, 'test.pkl')\n",
    "\n",
    "    print(\"testing data saved.\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f7c88",
   "metadata": {},
   "source": [
    "Verify the algorithm on a small dataset of 120 time steps, 90 for training and 30 \n",
    "for test.\n",
    "\n",
    "On this toy data, the R2 on the test split is ~0.75.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
