{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1313e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py\n",
    "%run dataset.py\n",
    "%run models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832c301e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0: loss 0.31040, R2 -0.33919\n",
      "epoch 0, batch 1: loss 0.28469, R2 -0.20515\n",
      "epoch 0, batch 2: loss 0.13278, R2 -0.14074\n",
      "epoch 1, batch 0: loss 0.24465, R2 -0.03911\n",
      "epoch 1, batch 1: loss 0.21357, R2 0.07718\n",
      "epoch 1, batch 2: loss 0.11397, R2 0.03028\n",
      "epoch 2, batch 0: loss 0.21774, R2 0.11463\n",
      "epoch 2, batch 1: loss 0.15349, R2 0.32378\n",
      "epoch 2, batch 2: loss 0.06740, R2 0.39455\n",
      "epoch 3, batch 0: loss 0.13557, R2 0.40690\n",
      "epoch 3, batch 1: loss 0.12073, R2 0.48895\n",
      "epoch 3, batch 2: loss 0.07551, R2 0.36839\n",
      "epoch 4, batch 0: loss 0.08789, R2 0.60945\n",
      "epoch 4, batch 1: loss 0.11446, R2 0.52712\n",
      "epoch 4, batch 2: loss 0.04817, R2 0.58897\n",
      "epoch 5, batch 0: loss 0.07902, R2 0.65593\n",
      "epoch 5, batch 1: loss 0.06749, R2 0.70473\n",
      "epoch 5, batch 2: loss 0.05639, R2 0.55257\n",
      "epoch 6, batch 0: loss 0.08315, R2 0.64410\n",
      "epoch 6, batch 1: loss 0.06944, R2 0.69771\n",
      "epoch 6, batch 2: loss 0.03500, R2 0.71087\n",
      "epoch 7, batch 0: loss 0.06986, R2 0.70528\n",
      "epoch 7, batch 1: loss 0.06552, R2 0.71231\n",
      "epoch 7, batch 2: loss 0.04654, R2 0.61071\n",
      "epoch 8, batch 0: loss 0.07827, R2 0.67179\n",
      "epoch 8, batch 1: loss 0.06506, R2 0.71705\n",
      "epoch 8, batch 2: loss 0.03066, R2 0.73556\n",
      "epoch 9, batch 0: loss 0.06122, R2 0.73909\n",
      "epoch 9, batch 1: loss 0.07123, R2 0.69997\n",
      "epoch 9, batch 2: loss 0.02541, R2 0.77378\n",
      "epoch 10, batch 0: loss 0.05229, R2 0.77864\n",
      "epoch 10, batch 1: loss 0.06441, R2 0.72464\n",
      "epoch 10, batch 2: loss 0.02360, R2 0.79344\n",
      "epoch 11, batch 0: loss 0.05917, R2 0.75117\n",
      "epoch 11, batch 1: loss 0.04737, R2 0.79590\n",
      "epoch 11, batch 2: loss 0.02026, R2 0.82308\n",
      "epoch 12, batch 0: loss 0.05424, R2 0.76893\n",
      "epoch 12, batch 1: loss 0.04504, R2 0.80933\n",
      "epoch 12, batch 2: loss 0.01792, R2 0.84204\n",
      "epoch 13, batch 0: loss 0.05775, R2 0.75775\n",
      "epoch 13, batch 1: loss 0.03646, R2 0.84453\n",
      "epoch 13, batch 2: loss 0.01465, R2 0.86859\n",
      "epoch 14, batch 0: loss 0.05018, R2 0.78462\n",
      "epoch 14, batch 1: loss 0.03629, R2 0.84803\n",
      "epoch 14, batch 2: loss 0.01408, R2 0.87496\n",
      "epoch 15, batch 0: loss 0.03416, R2 0.85645\n",
      "epoch 15, batch 1: loss 0.03284, R2 0.85653\n",
      "epoch 15, batch 2: loss 0.02500, R2 0.78731\n",
      "epoch 16, batch 0: loss 0.04274, R2 0.81740\n",
      "epoch 16, batch 1: loss 0.02906, R2 0.87393\n",
      "epoch 16, batch 2: loss 0.01504, R2 0.87452\n",
      "epoch 17, batch 0: loss 0.02679, R2 0.88281\n",
      "epoch 17, batch 1: loss 0.03254, R2 0.86272\n",
      "epoch 17, batch 2: loss 0.02164, R2 0.81778\n",
      "epoch 18, batch 0: loss 0.02880, R2 0.87849\n",
      "epoch 18, batch 1: loss 0.03732, R2 0.84112\n",
      "epoch 18, batch 2: loss 0.01145, R2 0.89816\n",
      "epoch 19, batch 0: loss 0.03137, R2 0.86928\n",
      "epoch 19, batch 1: loss 0.03130, R2 0.86449\n",
      "epoch 19, batch 2: loss 0.01168, R2 0.89706\n",
      "epoch 20, batch 0: loss 0.02700, R2 0.88526\n",
      "epoch 20, batch 1: loss 0.03356, R2 0.85862\n",
      "epoch 20, batch 2: loss 0.01051, R2 0.90584\n",
      "epoch 21, batch 0: loss 0.02385, R2 0.89746\n",
      "epoch 21, batch 1: loss 0.03195, R2 0.86443\n",
      "epoch 21, batch 2: loss 0.01231, R2 0.89401\n",
      "epoch 22, batch 0: loss 0.03069, R2 0.87330\n",
      "epoch 22, batch 1: loss 0.02365, R2 0.89665\n",
      "epoch 22, batch 2: loss 0.01136, R2 0.89967\n",
      "epoch 23, batch 0: loss 0.02435, R2 0.89606\n",
      "epoch 23, batch 1: loss 0.02435, R2 0.89517\n",
      "epoch 23, batch 2: loss 0.01438, R2 0.87794\n",
      "epoch 24, batch 0: loss 0.02079, R2 0.90863\n",
      "epoch 24, batch 1: loss 0.02377, R2 0.89901\n",
      "epoch 24, batch 2: loss 0.01673, R2 0.86220\n",
      "epoch 25, batch 0: loss 0.02216, R2 0.90352\n",
      "epoch 25, batch 1: loss 0.02267, R2 0.90183\n",
      "epoch 25, batch 2: loss 0.01487, R2 0.87977\n",
      "epoch 26, batch 0: loss 0.02165, R2 0.90575\n",
      "epoch 26, batch 1: loss 0.02505, R2 0.89545\n",
      "epoch 26, batch 2: loss 0.01141, R2 0.90088\n",
      "epoch 27, batch 0: loss 0.02063, R2 0.90943\n",
      "epoch 27, batch 1: loss 0.02152, R2 0.90816\n",
      "epoch 27, batch 2: loss 0.01445, R2 0.88179\n",
      "epoch 28, batch 0: loss 0.02050, R2 0.91081\n",
      "epoch 28, batch 1: loss 0.02261, R2 0.90440\n",
      "epoch 28, batch 2: loss 0.01214, R2 0.89709\n",
      "epoch 29, batch 0: loss 0.01659, R2 0.92588\n",
      "epoch 29, batch 1: loss 0.02415, R2 0.89766\n",
      "epoch 29, batch 2: loss 0.01314, R2 0.89449\n",
      "epoch 30, batch 0: loss 0.02273, R2 0.90491\n",
      "epoch 30, batch 1: loss 0.01914, R2 0.91673\n",
      "epoch 30, batch 2: loss 0.01084, R2 0.90614\n",
      "epoch 31, batch 0: loss 0.02033, R2 0.91143\n",
      "epoch 31, batch 1: loss 0.02329, R2 0.90379\n",
      "epoch 31, batch 2: loss 0.00796, R2 0.92942\n",
      "epoch 32, batch 0: loss 0.02202, R2 0.90708\n",
      "epoch 32, batch 1: loss 0.02084, R2 0.91159\n",
      "epoch 32, batch 2: loss 0.00755, R2 0.93235\n",
      "epoch 33, batch 0: loss 0.02078, R2 0.91281\n",
      "epoch 33, batch 1: loss 0.01755, R2 0.92303\n",
      "epoch 33, batch 2: loss 0.01117, R2 0.90537\n",
      "epoch 34, batch 0: loss 0.01935, R2 0.91799\n",
      "epoch 34, batch 1: loss 0.01781, R2 0.92219\n",
      "epoch 34, batch 2: loss 0.01155, R2 0.90341\n",
      "epoch 35, batch 0: loss 0.01981, R2 0.91541\n",
      "epoch 35, batch 1: loss 0.01894, R2 0.91908\n",
      "epoch 35, batch 2: loss 0.00930, R2 0.91986\n",
      "epoch 36, batch 0: loss 0.02176, R2 0.91017\n",
      "epoch 36, batch 1: loss 0.01642, R2 0.92767\n",
      "epoch 36, batch 2: loss 0.00920, R2 0.92012\n",
      "epoch 37, batch 0: loss 0.02042, R2 0.91346\n",
      "epoch 37, batch 1: loss 0.01931, R2 0.91843\n",
      "epoch 37, batch 2: loss 0.00705, R2 0.93685\n",
      "epoch 38, batch 0: loss 0.01659, R2 0.92772\n",
      "epoch 38, batch 1: loss 0.02061, R2 0.91401\n",
      "epoch 38, batch 2: loss 0.00895, R2 0.92229\n",
      "epoch 39, batch 0: loss 0.01926, R2 0.91879\n",
      "epoch 39, batch 1: loss 0.01756, R2 0.92356\n",
      "epoch 39, batch 2: loss 0.00882, R2 0.92496\n",
      "epoch 40, batch 0: loss 0.01549, R2 0.93171\n",
      "epoch 40, batch 1: loss 0.02082, R2 0.91380\n",
      "epoch 40, batch 2: loss 0.00894, R2 0.92291\n",
      "epoch 41, batch 0: loss 0.01624, R2 0.93015\n",
      "epoch 41, batch 1: loss 0.02025, R2 0.91505\n",
      "epoch 41, batch 2: loss 0.00837, R2 0.92630\n",
      "epoch 42, batch 0: loss 0.01588, R2 0.93024\n",
      "epoch 42, batch 1: loss 0.01998, R2 0.91644\n",
      "epoch 42, batch 2: loss 0.00851, R2 0.92773\n",
      "epoch 43, batch 0: loss 0.01349, R2 0.93997\n",
      "epoch 43, batch 1: loss 0.01914, R2 0.91803\n",
      "epoch 43, batch 2: loss 0.01129, R2 0.91038\n",
      "epoch 44, batch 0: loss 0.01728, R2 0.92609\n",
      "epoch 44, batch 1: loss 0.01827, R2 0.92262\n",
      "epoch 44, batch 2: loss 0.00804, R2 0.92981\n",
      "epoch 45, batch 0: loss 0.01820, R2 0.92326\n",
      "epoch 45, batch 1: loss 0.01477, R2 0.93455\n",
      "epoch 45, batch 2: loss 0.01037, R2 0.91462\n",
      "epoch 46, batch 0: loss 0.01671, R2 0.92834\n",
      "epoch 46, batch 1: loss 0.01718, R2 0.92686\n",
      "epoch 46, batch 2: loss 0.00901, R2 0.92246\n",
      "epoch 47, batch 0: loss 0.01809, R2 0.92337\n",
      "epoch 47, batch 1: loss 0.01490, R2 0.93555\n",
      "epoch 47, batch 2: loss 0.00969, R2 0.91732\n",
      "epoch 48, batch 0: loss 0.01419, R2 0.93719\n",
      "epoch 48, batch 1: loss 0.02015, R2 0.91629\n",
      "epoch 48, batch 2: loss 0.00794, R2 0.93250\n",
      "epoch 49, batch 0: loss 0.01721, R2 0.92680\n",
      "epoch 49, batch 1: loss 0.01807, R2 0.92370\n",
      "epoch 49, batch 2: loss 0.00667, R2 0.94066\n",
      "epoch 50, batch 0: loss 0.01688, R2 0.92897\n",
      "epoch 50, batch 1: loss 0.01718, R2 0.92612\n",
      "epoch 50, batch 2: loss 0.00769, R2 0.93271\n",
      "epoch 51, batch 0: loss 0.01524, R2 0.93444\n",
      "epoch 51, batch 1: loss 0.01557, R2 0.93289\n",
      "epoch 51, batch 2: loss 0.01069, R2 0.91080\n",
      "epoch 52, batch 0: loss 0.01636, R2 0.93001\n",
      "epoch 52, batch 1: loss 0.01573, R2 0.93156\n",
      "epoch 52, batch 2: loss 0.00918, R2 0.92402\n",
      "epoch 53, batch 0: loss 0.01522, R2 0.93370\n",
      "epoch 53, batch 1: loss 0.01686, R2 0.92819\n",
      "epoch 53, batch 2: loss 0.00899, R2 0.92512\n",
      "epoch 54, batch 0: loss 0.01488, R2 0.93561\n",
      "epoch 54, batch 1: loss 0.01470, R2 0.93572\n",
      "epoch 54, batch 2: loss 0.01141, R2 0.90840\n",
      "epoch 55, batch 0: loss 0.01633, R2 0.93038\n",
      "epoch 55, batch 1: loss 0.01567, R2 0.93183\n",
      "epoch 55, batch 2: loss 0.00866, R2 0.92770\n",
      "epoch 56, batch 0: loss 0.01682, R2 0.92758\n",
      "epoch 56, batch 1: loss 0.01638, R2 0.93136\n",
      "epoch 56, batch 2: loss 0.00734, R2 0.93533\n",
      "epoch 57, batch 0: loss 0.01456, R2 0.93633\n",
      "epoch 57, batch 1: loss 0.01979, R2 0.91893\n",
      "epoch 57, batch 2: loss 0.00600, R2 0.94616\n",
      "epoch 58, batch 0: loss 0.01593, R2 0.93194\n",
      "epoch 58, batch 1: loss 0.01662, R2 0.92821\n",
      "epoch 58, batch 2: loss 0.00767, R2 0.93540\n",
      "epoch 59, batch 0: loss 0.01401, R2 0.93942\n",
      "epoch 59, batch 1: loss 0.01779, R2 0.92501\n",
      "epoch 59, batch 2: loss 0.00831, R2 0.92834\n",
      "epoch 60, batch 0: loss 0.01472, R2 0.93660\n",
      "epoch 60, batch 1: loss 0.01759, R2 0.92464\n",
      "epoch 60, batch 2: loss 0.00761, R2 0.93595\n",
      "epoch 61, batch 0: loss 0.01499, R2 0.93464\n",
      "epoch 61, batch 1: loss 0.01747, R2 0.92633\n",
      "epoch 61, batch 2: loss 0.00741, R2 0.93714\n",
      "epoch 62, batch 0: loss 0.01470, R2 0.93675\n",
      "epoch 62, batch 1: loss 0.01692, R2 0.92826\n",
      "epoch 62, batch 2: loss 0.00812, R2 0.93005\n",
      "epoch 63, batch 0: loss 0.01655, R2 0.92881\n",
      "epoch 63, batch 1: loss 0.01470, R2 0.93671\n",
      "epoch 63, batch 2: loss 0.00828, R2 0.93075\n",
      "epoch 64, batch 0: loss 0.01541, R2 0.93413\n",
      "epoch 64, batch 1: loss 0.01296, R2 0.94262\n",
      "epoch 64, batch 2: loss 0.01119, R2 0.91012\n",
      "epoch 65, batch 0: loss 0.01320, R2 0.94262\n",
      "epoch 65, batch 1: loss 0.01745, R2 0.92546\n",
      "epoch 65, batch 2: loss 0.00868, R2 0.92770\n",
      "epoch 66, batch 0: loss 0.01796, R2 0.92454\n",
      "epoch 66, batch 1: loss 0.01406, R2 0.93848\n",
      "epoch 66, batch 2: loss 0.00728, R2 0.93820\n",
      "epoch 67, batch 0: loss 0.01680, R2 0.92990\n",
      "epoch 67, batch 1: loss 0.01604, R2 0.93062\n",
      "epoch 67, batch 2: loss 0.00637, R2 0.94384\n",
      "epoch 68, batch 0: loss 0.01661, R2 0.92975\n",
      "epoch 68, batch 1: loss 0.01404, R2 0.93924\n",
      "epoch 68, batch 2: loss 0.00848, R2 0.92751\n",
      "epoch 69, batch 0: loss 0.01724, R2 0.92754\n",
      "epoch 69, batch 1: loss 0.01476, R2 0.93553\n",
      "epoch 69, batch 2: loss 0.00696, R2 0.94080\n",
      "epoch 70, batch 0: loss 0.01565, R2 0.93331\n",
      "epoch 70, batch 1: loss 0.01506, R2 0.93550\n",
      "epoch 70, batch 2: loss 0.00833, R2 0.92827\n",
      "epoch 71, batch 0: loss 0.01853, R2 0.92271\n",
      "epoch 71, batch 1: loss 0.01323, R2 0.94166\n",
      "epoch 71, batch 2: loss 0.00722, R2 0.93874\n",
      "epoch 72, batch 0: loss 0.01494, R2 0.93486\n",
      "epoch 72, batch 1: loss 0.01591, R2 0.93339\n",
      "epoch 72, batch 2: loss 0.00788, R2 0.93213\n",
      "epoch 73, batch 0: loss 0.01379, R2 0.93945\n",
      "epoch 73, batch 1: loss 0.01895, R2 0.92236\n",
      "epoch 73, batch 2: loss 0.00595, R2 0.94709\n",
      "epoch 74, batch 0: loss 0.01950, R2 0.91935\n",
      "epoch 74, batch 1: loss 0.01189, R2 0.94692\n",
      "epoch 74, batch 2: loss 0.00723, R2 0.93901\n",
      "epoch 75, batch 0: loss 0.01768, R2 0.92694\n",
      "epoch 75, batch 1: loss 0.01465, R2 0.93623\n",
      "epoch 75, batch 2: loss 0.00608, R2 0.94596\n",
      "epoch 76, batch 0: loss 0.01537, R2 0.93442\n",
      "epoch 76, batch 1: loss 0.01557, R2 0.93371\n",
      "epoch 76, batch 2: loss 0.00748, R2 0.93501\n",
      "epoch 77, batch 0: loss 0.01493, R2 0.93622\n",
      "epoch 77, batch 1: loss 0.01540, R2 0.93428\n",
      "epoch 77, batch 2: loss 0.00802, R2 0.93084\n",
      "epoch 78, batch 0: loss 0.01595, R2 0.93136\n",
      "epoch 78, batch 1: loss 0.01641, R2 0.93173\n",
      "epoch 78, batch 2: loss 0.00581, R2 0.94793\n",
      "epoch 79, batch 0: loss 0.01840, R2 0.92327\n",
      "epoch 79, batch 1: loss 0.01357, R2 0.94129\n",
      "epoch 79, batch 2: loss 0.00619, R2 0.94542\n",
      "epoch 80, batch 0: loss 0.01662, R2 0.92988\n",
      "epoch 80, batch 1: loss 0.01356, R2 0.94048\n",
      "epoch 80, batch 2: loss 0.00793, R2 0.93366\n",
      "epoch 81, batch 0: loss 0.01537, R2 0.93455\n",
      "epoch 81, batch 1: loss 0.01580, R2 0.93284\n",
      "epoch 81, batch 2: loss 0.00676, R2 0.94082\n",
      "epoch 82, batch 0: loss 0.01718, R2 0.92887\n",
      "epoch 82, batch 1: loss 0.01471, R2 0.93609\n",
      "epoch 82, batch 2: loss 0.00601, R2 0.94660\n",
      "epoch 83, batch 0: loss 0.01353, R2 0.94083\n",
      "epoch 83, batch 1: loss 0.01731, R2 0.92728\n",
      "epoch 83, batch 2: loss 0.00700, R2 0.94062\n",
      "epoch 84, batch 0: loss 0.01803, R2 0.92478\n",
      "epoch 84, batch 1: loss 0.01253, R2 0.94455\n",
      "epoch 84, batch 2: loss 0.00724, R2 0.93904\n",
      "epoch 85, batch 0: loss 0.01658, R2 0.93003\n",
      "epoch 85, batch 1: loss 0.01414, R2 0.93821\n",
      "epoch 85, batch 2: loss 0.00704, R2 0.94061\n",
      "epoch 86, batch 0: loss 0.01412, R2 0.93830\n",
      "epoch 86, batch 1: loss 0.01570, R2 0.93346\n",
      "epoch 86, batch 2: loss 0.00782, R2 0.93458\n",
      "epoch 87, batch 0: loss 0.01366, R2 0.94031\n",
      "epoch 87, batch 1: loss 0.01584, R2 0.93385\n",
      "epoch 87, batch 2: loss 0.00808, R2 0.93041\n",
      "epoch 88, batch 0: loss 0.01443, R2 0.93708\n",
      "epoch 88, batch 1: loss 0.01611, R2 0.93191\n",
      "epoch 88, batch 2: loss 0.00699, R2 0.94104\n",
      "epoch 89, batch 0: loss 0.01136, R2 0.94923\n",
      "epoch 89, batch 1: loss 0.01671, R2 0.92873\n",
      "epoch 89, batch 2: loss 0.00942, R2 0.92529\n",
      "epoch 90, batch 0: loss 0.01406, R2 0.93876\n",
      "epoch 90, batch 1: loss 0.01510, R2 0.93565\n",
      "epoch 90, batch 2: loss 0.00826, R2 0.93122\n",
      "epoch 91, batch 0: loss 0.01297, R2 0.94280\n",
      "epoch 91, batch 1: loss 0.01606, R2 0.93218\n",
      "epoch 91, batch 2: loss 0.00827, R2 0.93153\n",
      "epoch 92, batch 0: loss 0.01714, R2 0.92822\n",
      "epoch 92, batch 1: loss 0.01445, R2 0.93823\n",
      "epoch 92, batch 2: loss 0.00568, R2 0.94912\n",
      "epoch 93, batch 0: loss 0.01420, R2 0.93822\n",
      "epoch 93, batch 1: loss 0.01481, R2 0.93683\n",
      "epoch 93, batch 2: loss 0.00824, R2 0.93141\n",
      "epoch 94, batch 0: loss 0.01537, R2 0.93462\n",
      "epoch 94, batch 1: loss 0.01518, R2 0.93539\n",
      "epoch 94, batch 2: loss 0.00668, R2 0.94150\n",
      "epoch 95, batch 0: loss 0.01671, R2 0.93079\n",
      "epoch 95, batch 1: loss 0.01383, R2 0.93972\n",
      "epoch 95, batch 2: loss 0.00650, R2 0.94274\n",
      "epoch 96, batch 0: loss 0.01574, R2 0.93336\n",
      "epoch 96, batch 1: loss 0.01444, R2 0.93714\n",
      "epoch 96, batch 2: loss 0.00689, R2 0.94182\n",
      "epoch 97, batch 0: loss 0.01117, R2 0.95008\n",
      "epoch 97, batch 1: loss 0.01721, R2 0.92800\n",
      "epoch 97, batch 2: loss 0.00857, R2 0.92940\n",
      "epoch 98, batch 0: loss 0.01115, R2 0.95016\n",
      "epoch 98, batch 1: loss 0.01530, R2 0.93515\n",
      "epoch 98, batch 2: loss 0.01052, R2 0.91555\n",
      "epoch 99, batch 0: loss 0.01616, R2 0.93077\n",
      "epoch 99, batch 1: loss 0.01468, R2 0.93820\n",
      "epoch 99, batch 2: loss 0.00600, R2 0.94712\n",
      "Test R2: 0.90061, loss 0.08794\n",
      "testing data saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, batch 149: loss 0.03064, R2 0.92531\n",
      "epoch 9, batch 150: loss 0.04074, R2 0.89079\n",
      "epoch 9, batch 151: loss 0.04196, R2 0.89657\n",
      "epoch 9, batch 152: loss 0.08150, R2 0.80253\n",
      "epoch 9, batch 153: loss 0.08225, R2 0.84428\n",
      "epoch 9, batch 154: loss 0.10709, R2 0.77475\n",
      "epoch 9, batch 155: loss 0.05314, R2 0.85694\n",
      "epoch 9, batch 156: loss 0.07940, R2 0.82284\n",
      "epoch 9, batch 157: loss 0.04347, R2 0.89351\n",
      "epoch 9, batch 158: loss 0.03977, R2 0.90049\n",
      "epoch 9, batch 159: loss 0.03787, R2 0.90833\n",
      "epoch 9, batch 160: loss 0.02090, R2 0.93696\n",
      "epoch 9, batch 161: loss 0.07874, R2 0.80416\n",
      "epoch 9, batch 162: loss 0.06430, R2 0.84322\n",
      "epoch 9, batch 163: loss 0.04545, R2 0.88557\n",
      "epoch 9, batch 164: loss 0.03405, R2 0.89709\n",
      "epoch 9, batch 165: loss 0.02060, R2 0.94029\n",
      "epoch 9, batch 166: loss 0.03426, R2 0.92610\n",
      "epoch 9, batch 167: loss 0.04926, R2 0.89180\n",
      "epoch 9, batch 168: loss 0.04607, R2 0.87959\n",
      "epoch 9, batch 169: loss 0.04586, R2 0.88853\n",
      "epoch 9, batch 170: loss 0.04610, R2 0.86293\n",
      "epoch 9, batch 171: loss 0.06694, R2 0.85838\n",
      "epoch 9, batch 172: loss 0.05231, R2 0.86480\n",
      "epoch 9, batch 173: loss 0.02928, R2 0.90130\n",
      "epoch 9, batch 174: loss 0.08336, R2 0.79849\n",
      "epoch 9, batch 175: loss 0.02851, R2 0.93580\n",
      "epoch 9, batch 176: loss 0.03304, R2 0.90852\n",
      "epoch 9, batch 177: loss 0.09764, R2 0.79643\n",
      "epoch 9, batch 178: loss 0.02700, R2 0.91624\n",
      "epoch 9, batch 179: loss 0.11087, R2 0.72412\n",
      "epoch 9, batch 180: loss 0.05845, R2 0.85544\n",
      "epoch 9, batch 181: loss 0.02607, R2 0.91212\n",
      "epoch 9, batch 182: loss 0.04934, R2 0.89561\n",
      "epoch 9, batch 183: loss 0.07238, R2 0.85273\n",
      "epoch 9, batch 184: loss 0.07270, R2 0.83005\n",
      "epoch 9, batch 185: loss 0.08054, R2 0.85262\n",
      "epoch 9, batch 186: loss 0.06563, R2 0.84123\n",
      "epoch 9, batch 187: loss 0.09209, R2 0.80414\n",
      "epoch 9, batch 188: loss 0.05877, R2 0.80377\n",
      "epoch 9, batch 189: loss 0.03929, R2 0.90472\n",
      "epoch 9, batch 190: loss 0.10823, R2 0.76303\n",
      "epoch 9, batch 191: loss 0.05135, R2 0.87022\n",
      "epoch 9, batch 192: loss 0.04841, R2 0.86541\n",
      "epoch 9, batch 193: loss 0.03946, R2 0.90229\n",
      "epoch 9, batch 194: loss 0.06681, R2 0.85414\n",
      "epoch 9, batch 195: loss 0.03979, R2 0.90449\n",
      "epoch 9, batch 196: loss 0.05118, R2 0.87187\n",
      "epoch 9, batch 197: loss 0.05623, R2 0.88403\n",
      "epoch 9, batch 198: loss 0.10164, R2 0.73772\n",
      "epoch 9, batch 199: loss 0.10673, R2 0.73679\n",
      "epoch 9, batch 200: loss 0.04131, R2 0.87862\n",
      "epoch 9, batch 201: loss 0.06515, R2 0.86866\n",
      "epoch 9, batch 202: loss 0.05042, R2 0.86804\n",
      "epoch 9, batch 203: loss 0.05533, R2 0.87577\n",
      "epoch 9, batch 204: loss 0.08703, R2 0.78912\n",
      "epoch 9, batch 205: loss 0.07362, R2 0.82924\n",
      "epoch 9, batch 206: loss 0.02422, R2 0.94087\n",
      "epoch 9, batch 207: loss 0.04289, R2 0.88706\n",
      "epoch 9, batch 208: loss 0.04199, R2 0.91949\n",
      "epoch 9, batch 209: loss 0.06040, R2 0.88568\n",
      "epoch 9, batch 210: loss 0.04792, R2 0.89421\n",
      "epoch 9, batch 211: loss 0.05560, R2 0.86638\n",
      "epoch 9, batch 212: loss 0.04521, R2 0.89910\n",
      "epoch 9, batch 213: loss 0.03271, R2 0.92273\n",
      "epoch 9, batch 214: loss 0.08361, R2 0.79073\n",
      "epoch 9, batch 215: loss 0.04801, R2 0.84164\n",
      "epoch 9, batch 216: loss 0.07991, R2 0.78107\n",
      "epoch 9, batch 217: loss 0.08213, R2 0.85471\n",
      "epoch 9, batch 218: loss 0.05220, R2 0.88768\n",
      "epoch 9, batch 219: loss 0.03761, R2 0.90972\n",
      "epoch 9, batch 220: loss 0.03645, R2 0.89057\n",
      "epoch 9, batch 221: loss 0.04614, R2 0.88593\n",
      "epoch 9, batch 222: loss 0.03236, R2 0.90884\n",
      "epoch 9, batch 223: loss 0.08140, R2 0.84539\n",
      "epoch 9, batch 224: loss 0.09959, R2 0.79821\n",
      "epoch 9, batch 225: loss 0.06484, R2 0.85148\n",
      "epoch 9, batch 226: loss 0.03804, R2 0.89344\n",
      "epoch 9, batch 227: loss 0.04521, R2 0.88564\n",
      "epoch 9, batch 228: loss 0.05549, R2 0.86858\n",
      "epoch 9, batch 229: loss 0.04172, R2 0.91341\n",
      "epoch 9, batch 230: loss 0.05582, R2 0.87834\n",
      "epoch 9, batch 231: loss 0.03367, R2 0.92278\n",
      "epoch 9, batch 232: loss 0.09685, R2 0.77994\n",
      "epoch 9, batch 233: loss 0.05018, R2 0.89553\n",
      "epoch 9, batch 234: loss 0.03169, R2 0.91936\n",
      "epoch 9, batch 235: loss 0.03909, R2 0.89091\n",
      "epoch 9, batch 236: loss 0.07525, R2 0.85089\n",
      "epoch 9, batch 237: loss 0.04258, R2 0.86589\n",
      "epoch 9, batch 238: loss 0.08414, R2 0.81775\n",
      "epoch 9, batch 239: loss 0.05613, R2 0.87413\n",
      "epoch 9, batch 240: loss 0.06300, R2 0.84922\n",
      "epoch 9, batch 241: loss 0.06624, R2 0.85995\n",
      "epoch 9, batch 242: loss 0.03691, R2 0.91464\n",
      "epoch 9, batch 243: loss 0.06122, R2 0.82878\n",
      "epoch 9, batch 244: loss 0.04032, R2 0.88722\n",
      "epoch 9, batch 245: loss 0.04703, R2 0.90424\n",
      "epoch 9, batch 246: loss 0.01096, R2 0.95165\n",
      "epoch 9, batch 247: loss 0.02422, R2 0.91286\n",
      "epoch 9, batch 248: loss 0.01911, R2 0.94532\n",
      "epoch 9, batch 249: loss 0.05052, R2 0.87919\n",
      "epoch 10, batch 0: loss 0.04834, R2 0.87408\n",
      "epoch 10, batch 1: loss 0.03649, R2 0.89824\n",
      "epoch 10, batch 2: loss 0.03831, R2 0.83597\n",
      "epoch 10, batch 3: loss 0.05648, R2 0.85641\n",
      "epoch 10, batch 4: loss 0.05888, R2 0.82327\n",
      "epoch 10, batch 5: loss 0.05175, R2 0.88973\n",
      "epoch 10, batch 6: loss 0.05438, R2 0.87378\n",
      "epoch 10, batch 7: loss 0.04183, R2 0.89863\n",
      "epoch 10, batch 8: loss 0.04122, R2 0.86581\n",
      "epoch 10, batch 9: loss 0.05181, R2 0.88050\n",
      "epoch 10, batch 10: loss 0.02411, R2 0.92720\n",
      "epoch 10, batch 11: loss 0.05723, R2 0.86725\n",
      "epoch 10, batch 12: loss 0.03064, R2 0.87251\n",
      "epoch 10, batch 13: loss 0.01384, R2 0.94593\n",
      "epoch 10, batch 14: loss 0.01168, R2 0.95473\n",
      "epoch 10, batch 15: loss 0.08520, R2 0.82776\n",
      "epoch 10, batch 16: loss 0.04627, R2 0.89433\n",
      "epoch 10, batch 17: loss 0.09750, R2 0.78601\n",
      "epoch 10, batch 18: loss 0.02020, R2 0.94747\n",
      "epoch 10, batch 19: loss 0.10913, R2 0.78141\n",
      "epoch 10, batch 20: loss 0.09772, R2 0.80259\n",
      "epoch 10, batch 21: loss 0.03980, R2 0.89358\n",
      "epoch 10, batch 22: loss 0.03272, R2 0.89319\n",
      "epoch 10, batch 23: loss 0.08498, R2 0.80969\n",
      "epoch 10, batch 24: loss 0.06606, R2 0.80651\n",
      "epoch 10, batch 25: loss 0.07395, R2 0.80850\n",
      "epoch 10, batch 26: loss 0.03252, R2 0.90861\n",
      "epoch 10, batch 27: loss 0.03628, R2 0.90882\n",
      "epoch 10, batch 28: loss 0.06273, R2 0.84599\n",
      "epoch 10, batch 29: loss 0.06333, R2 0.86661\n",
      "epoch 10, batch 30: loss 0.05124, R2 0.88160\n",
      "epoch 10, batch 31: loss 0.01694, R2 0.94818\n",
      "epoch 10, batch 32: loss 0.03072, R2 0.93259\n",
      "epoch 10, batch 33: loss 0.06461, R2 0.80898\n",
      "epoch 10, batch 34: loss 0.03624, R2 0.90447\n",
      "epoch 10, batch 35: loss 0.02613, R2 0.93386\n",
      "epoch 10, batch 36: loss 0.07107, R2 0.84755\n",
      "epoch 10, batch 37: loss 0.06910, R2 0.85905\n",
      "epoch 10, batch 38: loss 0.05421, R2 0.87280\n",
      "epoch 10, batch 39: loss 0.05529, R2 0.86497\n",
      "epoch 10, batch 40: loss 0.03373, R2 0.90900\n",
      "epoch 10, batch 41: loss 0.05669, R2 0.87504\n",
      "epoch 10, batch 42: loss 0.04341, R2 0.87191\n",
      "epoch 10, batch 43: loss 0.04775, R2 0.89558\n",
      "epoch 10, batch 44: loss 0.06177, R2 0.86598\n",
      "epoch 10, batch 45: loss 0.06986, R2 0.82496\n",
      "epoch 10, batch 46: loss 0.05059, R2 0.87974\n",
      "epoch 10, batch 47: loss 0.04074, R2 0.90001\n",
      "epoch 10, batch 48: loss 0.08988, R2 0.80697\n",
      "epoch 10, batch 49: loss 0.03265, R2 0.90998\n",
      "epoch 10, batch 50: loss 0.03635, R2 0.89082\n",
      "epoch 10, batch 51: loss 0.03479, R2 0.91286\n",
      "epoch 10, batch 52: loss 0.05887, R2 0.87125\n",
      "epoch 10, batch 53: loss 0.03901, R2 0.88389\n",
      "epoch 10, batch 54: loss 0.04037, R2 0.89871\n",
      "epoch 10, batch 55: loss 0.02349, R2 0.92673\n",
      "epoch 10, batch 56: loss 0.05612, R2 0.85828\n",
      "epoch 10, batch 57: loss 0.02881, R2 0.88118\n",
      "epoch 10, batch 58: loss 0.06474, R2 0.85592\n",
      "epoch 10, batch 59: loss 0.05705, R2 0.87979\n",
      "epoch 10, batch 60: loss 0.05073, R2 0.88655\n",
      "epoch 10, batch 61: loss 0.04932, R2 0.86438\n",
      "epoch 10, batch 62: loss 0.08253, R2 0.82812\n",
      "epoch 10, batch 63: loss 0.02931, R2 0.91991\n",
      "epoch 10, batch 64: loss 0.05483, R2 0.87173\n",
      "epoch 10, batch 65: loss 0.03225, R2 0.91360\n",
      "epoch 10, batch 66: loss 0.01751, R2 0.95155\n",
      "epoch 10, batch 67: loss 0.02724, R2 0.93003\n",
      "epoch 10, batch 68: loss 0.05212, R2 0.86314\n",
      "epoch 10, batch 69: loss 0.03280, R2 0.86244\n",
      "epoch 10, batch 70: loss 0.12128, R2 0.74262\n",
      "epoch 10, batch 71: loss 0.03577, R2 0.91242\n",
      "epoch 10, batch 72: loss 0.07582, R2 0.83349\n",
      "epoch 10, batch 73: loss 0.08775, R2 0.79294\n",
      "epoch 10, batch 74: loss 0.02816, R2 0.91171\n",
      "epoch 10, batch 75: loss 0.04314, R2 0.89821\n",
      "epoch 10, batch 76: loss 0.03556, R2 0.91076\n",
      "epoch 10, batch 77: loss 0.02164, R2 0.94852\n",
      "epoch 10, batch 78: loss 0.02825, R2 0.92055\n",
      "epoch 10, batch 79: loss 0.03230, R2 0.92476\n",
      "epoch 10, batch 80: loss 0.05048, R2 0.84603\n",
      "epoch 10, batch 81: loss 0.04214, R2 0.89198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, batch 82: loss 0.06474, R2 0.82870\n",
      "epoch 10, batch 83: loss 0.03837, R2 0.91325\n",
      "epoch 10, batch 84: loss 0.07331, R2 0.86077\n",
      "epoch 10, batch 85: loss 0.06936, R2 0.85607\n",
      "epoch 10, batch 86: loss 0.01662, R2 0.95832\n",
      "epoch 10, batch 87: loss 0.09331, R2 0.80457\n",
      "epoch 10, batch 88: loss 0.04868, R2 0.88214\n",
      "epoch 10, batch 89: loss 0.02303, R2 0.93099\n",
      "epoch 10, batch 90: loss 0.02457, R2 0.91466\n",
      "epoch 10, batch 91: loss 0.04049, R2 0.89730\n",
      "epoch 10, batch 92: loss 0.07729, R2 0.82915\n",
      "epoch 10, batch 93: loss 0.09579, R2 0.78506\n",
      "epoch 10, batch 94: loss 0.04308, R2 0.87098\n",
      "epoch 10, batch 95: loss 0.09551, R2 0.80184\n",
      "epoch 10, batch 96: loss 0.04177, R2 0.88829\n",
      "epoch 10, batch 97: loss 0.02115, R2 0.93381\n",
      "epoch 10, batch 98: loss 0.04612, R2 0.89142\n",
      "epoch 10, batch 99: loss 0.03908, R2 0.90397\n",
      "epoch 10, batch 100: loss 0.02538, R2 0.93582\n",
      "epoch 10, batch 101: loss 0.02688, R2 0.93399\n",
      "epoch 10, batch 102: loss 0.06229, R2 0.82226\n",
      "epoch 10, batch 103: loss 0.05452, R2 0.88122\n",
      "epoch 10, batch 104: loss 0.04396, R2 0.90730\n",
      "epoch 10, batch 105: loss 0.06046, R2 0.87972\n",
      "epoch 10, batch 106: loss 0.02447, R2 0.92977\n",
      "epoch 10, batch 107: loss 0.07253, R2 0.85002\n",
      "epoch 10, batch 108: loss 0.02713, R2 0.88715\n",
      "epoch 10, batch 109: loss 0.11364, R2 0.75035\n",
      "epoch 10, batch 110: loss 0.01968, R2 0.93694\n",
      "epoch 10, batch 111: loss 0.04386, R2 0.82777\n",
      "epoch 10, batch 112: loss 0.04051, R2 0.84463\n",
      "epoch 10, batch 113: loss 0.02284, R2 0.93210\n",
      "epoch 10, batch 114: loss 0.07861, R2 0.85476\n",
      "epoch 10, batch 115: loss 0.13198, R2 0.70447\n",
      "epoch 10, batch 116: loss 0.02643, R2 0.92301\n",
      "epoch 10, batch 117: loss 0.04310, R2 0.90846\n",
      "epoch 10, batch 118: loss 0.03447, R2 0.91631\n",
      "epoch 10, batch 119: loss 0.03008, R2 0.92173\n",
      "epoch 10, batch 120: loss 0.03889, R2 0.89762\n",
      "epoch 10, batch 121: loss 0.07345, R2 0.79725\n",
      "epoch 10, batch 122: loss 0.06476, R2 0.82813\n",
      "epoch 10, batch 123: loss 0.03863, R2 0.85092\n",
      "epoch 10, batch 124: loss 0.05437, R2 0.88098\n",
      "epoch 10, batch 125: loss 0.03846, R2 0.91001\n",
      "epoch 10, batch 126: loss 0.05870, R2 0.85786\n",
      "epoch 10, batch 127: loss 0.05859, R2 0.86203\n",
      "epoch 10, batch 128: loss 0.04204, R2 0.88896\n",
      "epoch 10, batch 129: loss 0.05228, R2 0.84349\n",
      "epoch 10, batch 130: loss 0.05077, R2 0.89612\n",
      "epoch 10, batch 131: loss 0.03715, R2 0.91829\n",
      "epoch 10, batch 132: loss 0.02487, R2 0.93778\n",
      "epoch 10, batch 133: loss 0.03525, R2 0.91291\n",
      "epoch 10, batch 134: loss 0.05237, R2 0.88973\n",
      "epoch 10, batch 135: loss 0.07811, R2 0.84902\n",
      "epoch 10, batch 136: loss 0.04340, R2 0.89748\n",
      "epoch 10, batch 137: loss 0.02734, R2 0.92854\n",
      "epoch 10, batch 138: loss 0.09345, R2 0.80834\n",
      "epoch 10, batch 139: loss 0.06247, R2 0.82557\n",
      "epoch 10, batch 140: loss 0.03756, R2 0.89471\n",
      "epoch 10, batch 141: loss 0.08417, R2 0.82303\n",
      "epoch 10, batch 142: loss 0.03588, R2 0.91118\n",
      "epoch 10, batch 143: loss 0.04242, R2 0.87843\n",
      "epoch 10, batch 144: loss 0.05234, R2 0.87960\n",
      "epoch 10, batch 145: loss 0.03240, R2 0.91344\n",
      "epoch 10, batch 146: loss 0.08126, R2 0.83272\n",
      "epoch 10, batch 147: loss 0.06659, R2 0.87012\n",
      "epoch 10, batch 148: loss 0.03821, R2 0.91591\n",
      "epoch 10, batch 149: loss 0.01342, R2 0.96130\n",
      "epoch 10, batch 150: loss 0.01342, R2 0.93741\n",
      "epoch 10, batch 151: loss 0.03360, R2 0.91450\n",
      "epoch 10, batch 152: loss 0.05795, R2 0.87243\n",
      "epoch 10, batch 153: loss 0.02777, R2 0.90645\n",
      "epoch 10, batch 154: loss 0.03999, R2 0.89099\n",
      "epoch 10, batch 155: loss 0.04616, R2 0.86851\n",
      "epoch 10, batch 156: loss 0.02454, R2 0.94066\n",
      "epoch 10, batch 157: loss 0.11798, R2 0.75390\n",
      "epoch 10, batch 158: loss 0.08481, R2 0.80183\n",
      "epoch 10, batch 159: loss 0.07607, R2 0.82142\n",
      "epoch 10, batch 160: loss 0.04496, R2 0.88259\n",
      "epoch 10, batch 161: loss 0.07532, R2 0.83599\n",
      "epoch 10, batch 162: loss 0.06215, R2 0.85119\n",
      "epoch 10, batch 163: loss 0.05250, R2 0.86655\n",
      "epoch 10, batch 164: loss 0.04498, R2 0.89281\n",
      "epoch 10, batch 165: loss 0.04535, R2 0.87145\n",
      "epoch 10, batch 166: loss 0.04483, R2 0.86200\n",
      "epoch 10, batch 167: loss 0.05475, R2 0.87578\n",
      "epoch 10, batch 168: loss 0.05374, R2 0.87957\n",
      "epoch 10, batch 169: loss 0.05738, R2 0.88242\n",
      "epoch 10, batch 170: loss 0.03152, R2 0.91290\n",
      "epoch 10, batch 171: loss 0.08815, R2 0.78644\n",
      "epoch 10, batch 172: loss 0.04662, R2 0.89208\n",
      "epoch 10, batch 173: loss 0.03404, R2 0.92760\n",
      "epoch 10, batch 174: loss 0.02607, R2 0.93162\n",
      "epoch 10, batch 175: loss 0.02830, R2 0.91167\n",
      "epoch 10, batch 176: loss 0.03911, R2 0.91058\n",
      "epoch 10, batch 177: loss 0.09668, R2 0.80625\n",
      "epoch 10, batch 178: loss 0.04929, R2 0.89841\n",
      "epoch 10, batch 179: loss 0.05974, R2 0.85820\n",
      "epoch 10, batch 180: loss 0.06590, R2 0.85020\n",
      "epoch 10, batch 181: loss 0.02640, R2 0.92323\n",
      "epoch 10, batch 182: loss 0.04998, R2 0.86552\n",
      "epoch 10, batch 183: loss 0.02941, R2 0.90801\n",
      "epoch 10, batch 184: loss 0.03569, R2 0.91844\n",
      "epoch 10, batch 185: loss 0.04026, R2 0.90535\n",
      "epoch 10, batch 186: loss 0.04575, R2 0.88655\n",
      "epoch 10, batch 187: loss 0.04087, R2 0.87982\n",
      "epoch 10, batch 188: loss 0.03641, R2 0.92069\n",
      "epoch 10, batch 189: loss 0.03767, R2 0.88781\n",
      "epoch 10, batch 190: loss 0.03360, R2 0.92089\n",
      "epoch 10, batch 191: loss 0.04278, R2 0.86225\n",
      "epoch 10, batch 192: loss 0.08178, R2 0.81366\n",
      "epoch 10, batch 193: loss 0.02552, R2 0.93191\n",
      "epoch 10, batch 194: loss 0.04883, R2 0.85802\n",
      "epoch 10, batch 195: loss 0.02325, R2 0.93964\n",
      "epoch 10, batch 196: loss 0.09408, R2 0.81587\n",
      "epoch 10, batch 197: loss 0.07487, R2 0.83313\n",
      "epoch 10, batch 198: loss 0.11495, R2 0.75559\n",
      "epoch 10, batch 199: loss 0.06271, R2 0.86849\n",
      "epoch 10, batch 200: loss 0.07795, R2 0.83360\n",
      "epoch 10, batch 201: loss 0.01991, R2 0.94864\n",
      "epoch 10, batch 202: loss 0.09253, R2 0.79632\n",
      "epoch 10, batch 203: loss 0.06613, R2 0.87285\n",
      "epoch 10, batch 204: loss 0.04205, R2 0.84732\n",
      "epoch 10, batch 205: loss 0.08497, R2 0.80811\n",
      "epoch 10, batch 206: loss 0.03187, R2 0.89637\n",
      "epoch 10, batch 207: loss 0.06379, R2 0.86283\n",
      "epoch 10, batch 208: loss 0.12313, R2 0.70477\n",
      "epoch 10, batch 209: loss 0.02513, R2 0.92934\n",
      "epoch 10, batch 210: loss 0.02172, R2 0.92595\n",
      "epoch 10, batch 211: loss 0.09322, R2 0.82714\n",
      "epoch 10, batch 212: loss 0.03302, R2 0.91221\n",
      "epoch 10, batch 213: loss 0.03419, R2 0.91850\n",
      "epoch 10, batch 214: loss 0.04899, R2 0.87563\n",
      "epoch 10, batch 215: loss 0.05863, R2 0.86669\n",
      "epoch 10, batch 216: loss 0.09034, R2 0.81147\n",
      "epoch 10, batch 217: loss 0.05760, R2 0.85042\n",
      "epoch 10, batch 218: loss 0.05211, R2 0.87947\n",
      "epoch 10, batch 219: loss 0.02282, R2 0.93587\n",
      "epoch 10, batch 220: loss 0.03894, R2 0.90776\n",
      "epoch 10, batch 221: loss 0.07052, R2 0.86561\n",
      "epoch 10, batch 222: loss 0.04937, R2 0.86730\n",
      "epoch 10, batch 223: loss 0.11213, R2 0.78898\n",
      "epoch 10, batch 224: loss 0.02895, R2 0.92601\n",
      "epoch 10, batch 225: loss 0.07243, R2 0.84573\n",
      "epoch 10, batch 226: loss 0.04908, R2 0.90534\n",
      "epoch 10, batch 227: loss 0.05078, R2 0.87691\n",
      "epoch 10, batch 228: loss 0.02580, R2 0.93742\n",
      "epoch 10, batch 229: loss 0.04227, R2 0.90591\n",
      "epoch 10, batch 230: loss 0.04731, R2 0.89865\n",
      "epoch 10, batch 231: loss 0.04036, R2 0.90660\n",
      "epoch 10, batch 232: loss 0.05205, R2 0.88379\n",
      "epoch 10, batch 233: loss 0.08489, R2 0.80332\n",
      "epoch 10, batch 234: loss 0.08129, R2 0.78624\n",
      "epoch 10, batch 235: loss 0.05112, R2 0.89143\n",
      "epoch 10, batch 236: loss 0.04064, R2 0.90580\n",
      "epoch 10, batch 237: loss 0.08766, R2 0.84479\n",
      "epoch 10, batch 238: loss 0.03891, R2 0.89488\n",
      "epoch 10, batch 239: loss 0.02910, R2 0.92648\n",
      "epoch 10, batch 240: loss 0.08110, R2 0.84179\n",
      "epoch 10, batch 241: loss 0.02986, R2 0.93375\n",
      "epoch 10, batch 242: loss 0.05306, R2 0.87433\n",
      "epoch 10, batch 243: loss 0.03127, R2 0.91974\n",
      "epoch 10, batch 244: loss 0.02404, R2 0.91863\n",
      "epoch 10, batch 245: loss 0.06613, R2 0.84115\n",
      "epoch 10, batch 246: loss 0.03232, R2 0.92122\n",
      "epoch 10, batch 247: loss 0.08892, R2 0.82312\n",
      "epoch 10, batch 248: loss 0.06774, R2 0.83768\n",
      "epoch 10, batch 249: loss 0.04496, R2 0.89405\n",
      "epoch 11, batch 0: loss 0.05717, R2 0.84895\n",
      "epoch 11, batch 1: loss 0.03887, R2 0.90654\n",
      "epoch 11, batch 2: loss 0.02706, R2 0.92871\n",
      "epoch 11, batch 3: loss 0.04010, R2 0.88758\n",
      "epoch 11, batch 4: loss 0.05984, R2 0.86318\n",
      "epoch 11, batch 5: loss 0.02697, R2 0.92038\n",
      "epoch 11, batch 6: loss 0.08349, R2 0.82682\n",
      "epoch 11, batch 7: loss 0.06147, R2 0.84757\n",
      "epoch 11, batch 8: loss 0.07381, R2 0.86564\n",
      "epoch 11, batch 9: loss 0.02200, R2 0.94053\n",
      "epoch 11, batch 10: loss 0.02459, R2 0.92464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, batch 11: loss 0.04884, R2 0.84530\n",
      "epoch 11, batch 12: loss 0.05878, R2 0.86553\n",
      "epoch 11, batch 13: loss 0.07667, R2 0.83382\n",
      "epoch 11, batch 14: loss 0.04006, R2 0.88130\n",
      "epoch 11, batch 15: loss 0.02955, R2 0.92197\n",
      "epoch 11, batch 16: loss 0.03383, R2 0.92609\n",
      "epoch 11, batch 17: loss 0.03070, R2 0.90542\n",
      "epoch 11, batch 18: loss 0.02267, R2 0.91237\n",
      "epoch 11, batch 19: loss 0.10257, R2 0.79141\n",
      "epoch 11, batch 20: loss 0.05721, R2 0.83196\n",
      "epoch 11, batch 21: loss 0.07436, R2 0.83231\n",
      "epoch 11, batch 22: loss 0.10273, R2 0.81011\n",
      "epoch 11, batch 23: loss 0.06862, R2 0.85613\n",
      "epoch 11, batch 24: loss 0.04983, R2 0.88062\n",
      "epoch 11, batch 25: loss 0.06566, R2 0.84194\n",
      "epoch 11, batch 26: loss 0.10635, R2 0.76756\n",
      "epoch 11, batch 27: loss 0.03600, R2 0.90047\n",
      "epoch 11, batch 28: loss 0.07905, R2 0.84566\n",
      "epoch 11, batch 29: loss 0.04256, R2 0.89460\n",
      "epoch 11, batch 30: loss 0.05874, R2 0.87703\n",
      "epoch 11, batch 31: loss 0.02541, R2 0.92228\n",
      "epoch 11, batch 32: loss 0.02378, R2 0.91518\n",
      "epoch 11, batch 33: loss 0.05395, R2 0.82509\n",
      "epoch 11, batch 34: loss 0.01978, R2 0.93249\n",
      "epoch 11, batch 35: loss 0.07453, R2 0.85034\n",
      "epoch 11, batch 36: loss 0.06125, R2 0.86251\n",
      "epoch 11, batch 37: loss 0.04327, R2 0.88693\n",
      "epoch 11, batch 38: loss 0.03230, R2 0.92833\n",
      "epoch 11, batch 39: loss 0.06495, R2 0.83665\n",
      "epoch 11, batch 40: loss 0.06937, R2 0.85495\n",
      "epoch 11, batch 41: loss 0.09076, R2 0.81621\n",
      "epoch 11, batch 42: loss 0.07193, R2 0.79206\n",
      "epoch 11, batch 43: loss 0.04010, R2 0.89259\n",
      "epoch 11, batch 44: loss 0.04409, R2 0.88809\n",
      "epoch 11, batch 45: loss 0.05326, R2 0.87687\n",
      "epoch 11, batch 46: loss 0.03155, R2 0.91039\n",
      "epoch 11, batch 47: loss 0.04452, R2 0.90844\n",
      "epoch 11, batch 48: loss 0.03956, R2 0.88569\n",
      "epoch 11, batch 49: loss 0.03742, R2 0.91240\n",
      "epoch 11, batch 50: loss 0.07423, R2 0.83599\n",
      "epoch 11, batch 51: loss 0.05942, R2 0.86453\n",
      "epoch 11, batch 52: loss 0.05520, R2 0.87580\n",
      "epoch 11, batch 53: loss 0.02841, R2 0.90755\n",
      "epoch 11, batch 54: loss 0.04822, R2 0.89599\n",
      "epoch 11, batch 55: loss 0.04430, R2 0.90206\n",
      "epoch 11, batch 56: loss 0.03786, R2 0.90690\n",
      "epoch 11, batch 57: loss 0.06329, R2 0.83899\n",
      "epoch 11, batch 58: loss 0.03848, R2 0.90361\n",
      "epoch 11, batch 59: loss 0.01917, R2 0.94666\n",
      "epoch 11, batch 60: loss 0.07102, R2 0.86434\n",
      "epoch 11, batch 61: loss 0.02536, R2 0.93060\n",
      "epoch 11, batch 62: loss 0.03245, R2 0.90010\n",
      "epoch 11, batch 63: loss 0.06154, R2 0.81580\n",
      "epoch 11, batch 64: loss 0.06041, R2 0.86244\n",
      "epoch 11, batch 65: loss 0.02459, R2 0.93799\n",
      "epoch 11, batch 66: loss 0.08205, R2 0.83145\n",
      "epoch 11, batch 67: loss 0.04439, R2 0.88635\n",
      "epoch 11, batch 68: loss 0.03520, R2 0.89318\n",
      "epoch 11, batch 69: loss 0.03145, R2 0.91435\n",
      "epoch 11, batch 70: loss 0.09726, R2 0.75455\n",
      "epoch 11, batch 71: loss 0.04382, R2 0.87601\n",
      "epoch 11, batch 72: loss 0.12336, R2 0.76619\n",
      "epoch 11, batch 73: loss 0.07032, R2 0.83476\n",
      "epoch 11, batch 74: loss 0.05924, R2 0.86872\n",
      "epoch 11, batch 75: loss 0.07259, R2 0.86275\n",
      "epoch 11, batch 76: loss 0.03987, R2 0.90461\n",
      "epoch 11, batch 77: loss 0.02624, R2 0.92962\n",
      "epoch 11, batch 78: loss 0.02222, R2 0.93237\n",
      "epoch 11, batch 79: loss 0.03265, R2 0.92008\n",
      "epoch 11, batch 80: loss 0.06427, R2 0.85037\n",
      "epoch 11, batch 81: loss 0.04166, R2 0.85229\n",
      "epoch 11, batch 82: loss 0.05793, R2 0.87227\n",
      "epoch 11, batch 83: loss 0.02625, R2 0.92495\n",
      "epoch 11, batch 84: loss 0.02974, R2 0.90826\n",
      "epoch 11, batch 85: loss 0.02340, R2 0.93871\n",
      "epoch 11, batch 86: loss 0.04309, R2 0.88777\n",
      "epoch 11, batch 87: loss 0.03023, R2 0.93297\n",
      "epoch 11, batch 88: loss 0.08570, R2 0.82032\n",
      "epoch 11, batch 89: loss 0.04270, R2 0.90437\n",
      "epoch 11, batch 90: loss 0.05708, R2 0.87441\n",
      "epoch 11, batch 91: loss 0.03953, R2 0.87315\n",
      "epoch 11, batch 92: loss 0.03818, R2 0.84181\n",
      "epoch 11, batch 93: loss 0.04190, R2 0.89864\n",
      "epoch 11, batch 94: loss 0.04280, R2 0.85585\n",
      "epoch 11, batch 95: loss 0.08944, R2 0.78305\n",
      "epoch 11, batch 96: loss 0.05488, R2 0.83315\n",
      "epoch 11, batch 97: loss 0.07117, R2 0.86313\n",
      "epoch 11, batch 98: loss 0.04349, R2 0.91184\n",
      "epoch 11, batch 99: loss 0.03205, R2 0.92017\n",
      "epoch 11, batch 100: loss 0.04878, R2 0.87798\n",
      "epoch 11, batch 101: loss 0.02792, R2 0.92794\n",
      "epoch 11, batch 102: loss 0.05382, R2 0.85647\n",
      "epoch 11, batch 103: loss 0.01836, R2 0.92523\n",
      "epoch 11, batch 104: loss 0.02119, R2 0.93618\n",
      "epoch 11, batch 105: loss 0.03136, R2 0.89164\n",
      "epoch 11, batch 106: loss 0.06628, R2 0.87576\n",
      "epoch 11, batch 107: loss 0.04942, R2 0.89875\n",
      "epoch 11, batch 108: loss 0.03452, R2 0.91737\n",
      "epoch 11, batch 109: loss 0.01965, R2 0.93145\n",
      "epoch 11, batch 110: loss 0.02061, R2 0.93563\n",
      "epoch 11, batch 111: loss 0.01693, R2 0.94354\n",
      "epoch 11, batch 112: loss 0.09416, R2 0.80543\n",
      "epoch 11, batch 113: loss 0.05552, R2 0.88217\n",
      "epoch 11, batch 114: loss 0.11091, R2 0.74611\n",
      "epoch 11, batch 115: loss 0.02626, R2 0.93199\n",
      "epoch 11, batch 116: loss 0.03607, R2 0.89687\n",
      "epoch 11, batch 117: loss 0.04009, R2 0.91627\n",
      "epoch 11, batch 118: loss 0.11310, R2 0.75182\n",
      "epoch 11, batch 119: loss 0.05597, R2 0.88057\n",
      "epoch 11, batch 120: loss 0.02896, R2 0.91529\n",
      "epoch 11, batch 121: loss 0.06848, R2 0.85296\n",
      "epoch 11, batch 122: loss 0.05396, R2 0.86604\n",
      "epoch 11, batch 123: loss 0.13951, R2 0.67634\n",
      "epoch 11, batch 124: loss 0.07442, R2 0.84557\n",
      "epoch 11, batch 125: loss 0.02419, R2 0.94034\n",
      "epoch 11, batch 126: loss 0.05374, R2 0.88531\n",
      "epoch 11, batch 127: loss 0.04605, R2 0.85413\n",
      "epoch 11, batch 128: loss 0.03632, R2 0.91004\n",
      "epoch 11, batch 129: loss 0.06723, R2 0.86140\n",
      "epoch 11, batch 130: loss 0.03005, R2 0.91727\n",
      "epoch 11, batch 131: loss 0.05600, R2 0.87077\n",
      "epoch 11, batch 132: loss 0.05030, R2 0.81332\n",
      "epoch 11, batch 133: loss 0.02470, R2 0.93478\n",
      "epoch 11, batch 134: loss 0.05053, R2 0.87601\n",
      "epoch 11, batch 135: loss 0.03762, R2 0.91707\n",
      "epoch 11, batch 136: loss 0.04797, R2 0.87752\n",
      "epoch 11, batch 137: loss 0.13032, R2 0.73000\n",
      "epoch 11, batch 138: loss 0.07902, R2 0.84134\n",
      "epoch 11, batch 139: loss 0.04854, R2 0.87478\n",
      "epoch 11, batch 140: loss 0.05278, R2 0.87195\n",
      "epoch 11, batch 141: loss 0.03104, R2 0.89179\n",
      "epoch 11, batch 142: loss 0.03062, R2 0.91633\n",
      "epoch 11, batch 143: loss 0.03412, R2 0.90600\n",
      "epoch 11, batch 144: loss 0.05655, R2 0.88216\n",
      "epoch 11, batch 145: loss 0.08566, R2 0.80186\n",
      "epoch 11, batch 146: loss 0.03617, R2 0.91376\n",
      "epoch 11, batch 147: loss 0.06766, R2 0.84442\n",
      "epoch 11, batch 148: loss 0.07303, R2 0.83169\n",
      "epoch 11, batch 149: loss 0.04043, R2 0.90196\n",
      "epoch 11, batch 150: loss 0.07537, R2 0.83083\n",
      "epoch 11, batch 151: loss 0.03105, R2 0.91499\n",
      "epoch 11, batch 152: loss 0.03211, R2 0.93041\n",
      "epoch 11, batch 153: loss 0.03357, R2 0.91133\n",
      "epoch 11, batch 154: loss 0.03976, R2 0.90875\n",
      "epoch 11, batch 155: loss 0.07566, R2 0.83027\n",
      "epoch 11, batch 156: loss 0.08987, R2 0.80092\n",
      "epoch 11, batch 157: loss 0.09361, R2 0.79451\n",
      "epoch 11, batch 158: loss 0.10887, R2 0.74662\n",
      "epoch 11, batch 159: loss 0.02172, R2 0.94833\n",
      "epoch 11, batch 160: loss 0.03422, R2 0.90744\n",
      "epoch 11, batch 161: loss 0.07436, R2 0.83703\n",
      "epoch 11, batch 162: loss 0.04146, R2 0.90513\n",
      "epoch 11, batch 163: loss 0.04214, R2 0.90230\n",
      "epoch 11, batch 164: loss 0.03654, R2 0.89391\n",
      "epoch 11, batch 165: loss 0.07619, R2 0.81877\n",
      "epoch 11, batch 166: loss 0.03424, R2 0.90616\n",
      "epoch 11, batch 167: loss 0.02267, R2 0.91443\n",
      "epoch 11, batch 168: loss 0.03607, R2 0.91347\n",
      "epoch 11, batch 169: loss 0.03727, R2 0.91626\n",
      "epoch 11, batch 170: loss 0.02822, R2 0.91043\n",
      "epoch 11, batch 171: loss 0.08604, R2 0.80692\n",
      "epoch 11, batch 172: loss 0.01288, R2 0.95580\n",
      "epoch 11, batch 173: loss 0.05404, R2 0.83129\n",
      "epoch 11, batch 174: loss 0.01886, R2 0.93371\n",
      "epoch 11, batch 175: loss 0.06750, R2 0.86086\n",
      "epoch 11, batch 176: loss 0.14572, R2 0.66855\n",
      "epoch 11, batch 177: loss 0.09330, R2 0.80867\n",
      "epoch 11, batch 178: loss 0.03846, R2 0.88555\n",
      "epoch 11, batch 179: loss 0.05671, R2 0.85599\n",
      "epoch 11, batch 180: loss 0.10006, R2 0.78446\n",
      "epoch 11, batch 181: loss 0.05277, R2 0.86977\n",
      "epoch 11, batch 182: loss 0.03194, R2 0.92512\n",
      "epoch 11, batch 183: loss 0.04517, R2 0.86345\n",
      "epoch 11, batch 184: loss 0.05000, R2 0.89070\n",
      "epoch 11, batch 185: loss 0.04180, R2 0.90785\n",
      "epoch 11, batch 186: loss 0.02421, R2 0.93088\n",
      "epoch 11, batch 187: loss 0.03133, R2 0.92962\n",
      "epoch 11, batch 188: loss 0.05599, R2 0.86673\n",
      "epoch 11, batch 189: loss 0.04903, R2 0.85419\n",
      "epoch 11, batch 190: loss 0.04297, R2 0.90998\n",
      "epoch 11, batch 191: loss 0.06846, R2 0.80965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, batch 192: loss 0.02737, R2 0.91727\n",
      "epoch 11, batch 193: loss 0.04528, R2 0.89307\n",
      "epoch 11, batch 194: loss 0.05013, R2 0.85957\n",
      "epoch 11, batch 195: loss 0.01458, R2 0.95365\n",
      "epoch 11, batch 196: loss 0.05424, R2 0.87957\n",
      "epoch 11, batch 197: loss 0.04550, R2 0.90558\n",
      "epoch 11, batch 198: loss 0.02275, R2 0.93266\n",
      "epoch 11, batch 199: loss 0.02116, R2 0.92927\n",
      "epoch 11, batch 200: loss 0.01998, R2 0.93961\n",
      "epoch 11, batch 201: loss 0.07530, R2 0.82190\n",
      "epoch 11, batch 202: loss 0.05704, R2 0.87181\n",
      "epoch 11, batch 203: loss 0.06060, R2 0.86640\n",
      "epoch 11, batch 204: loss 0.03649, R2 0.89561\n",
      "epoch 11, batch 205: loss 0.12215, R2 0.71730\n",
      "epoch 11, batch 206: loss 0.04164, R2 0.90702\n",
      "epoch 11, batch 207: loss 0.02590, R2 0.93694\n",
      "epoch 11, batch 208: loss 0.03691, R2 0.89567\n",
      "epoch 11, batch 209: loss 0.03195, R2 0.91501\n",
      "epoch 11, batch 210: loss 0.06710, R2 0.79597\n",
      "epoch 11, batch 211: loss 0.08000, R2 0.81451\n",
      "epoch 11, batch 212: loss 0.02413, R2 0.92362\n",
      "epoch 11, batch 213: loss 0.03267, R2 0.91690\n",
      "epoch 11, batch 214: loss 0.08772, R2 0.84118\n",
      "epoch 11, batch 215: loss 0.03769, R2 0.91335\n",
      "epoch 11, batch 216: loss 0.04651, R2 0.88118\n",
      "epoch 11, batch 217: loss 0.03371, R2 0.92178\n",
      "epoch 11, batch 218: loss 0.04283, R2 0.89358\n",
      "epoch 11, batch 219: loss 0.05337, R2 0.85722\n",
      "epoch 11, batch 220: loss 0.03431, R2 0.89721\n",
      "epoch 11, batch 221: loss 0.06211, R2 0.86701\n",
      "epoch 11, batch 222: loss 0.04648, R2 0.88003\n",
      "epoch 11, batch 223: loss 0.08746, R2 0.78845\n",
      "epoch 11, batch 224: loss 0.04358, R2 0.89952\n",
      "epoch 11, batch 225: loss 0.06083, R2 0.80759\n",
      "epoch 11, batch 226: loss 0.04467, R2 0.91452\n",
      "epoch 11, batch 227: loss 0.02979, R2 0.93523\n",
      "epoch 11, batch 228: loss 0.03757, R2 0.91269\n",
      "epoch 11, batch 229: loss 0.06118, R2 0.86831\n",
      "epoch 11, batch 230: loss 0.06827, R2 0.85124\n",
      "epoch 11, batch 231: loss 0.06954, R2 0.84254\n",
      "epoch 11, batch 232: loss 0.03223, R2 0.90555\n",
      "epoch 11, batch 233: loss 0.01894, R2 0.92706\n",
      "epoch 11, batch 234: loss 0.05515, R2 0.85975\n",
      "epoch 11, batch 235: loss 0.09826, R2 0.79412\n",
      "epoch 11, batch 236: loss 0.06442, R2 0.85736\n",
      "epoch 11, batch 237: loss 0.06056, R2 0.88444\n",
      "epoch 11, batch 238: loss 0.04076, R2 0.88192\n",
      "epoch 11, batch 239: loss 0.06676, R2 0.82145\n",
      "epoch 11, batch 240: loss 0.08871, R2 0.84906\n",
      "epoch 11, batch 241: loss 0.07210, R2 0.83308\n",
      "epoch 11, batch 242: loss 0.06582, R2 0.85249\n",
      "epoch 11, batch 243: loss 0.03458, R2 0.92121\n",
      "epoch 11, batch 244: loss 0.04402, R2 0.90002\n",
      "epoch 11, batch 245: loss 0.02710, R2 0.91878\n",
      "epoch 11, batch 246: loss 0.03249, R2 0.90117\n",
      "epoch 11, batch 247: loss 0.01965, R2 0.93647\n",
      "epoch 11, batch 248: loss 0.09863, R2 0.76357\n",
      "epoch 11, batch 249: loss 0.02732, R2 0.92766\n",
      "epoch 12, batch 0: loss 0.05933, R2 0.86871\n",
      "epoch 12, batch 1: loss 0.05264, R2 0.86360\n",
      "epoch 12, batch 2: loss 0.08955, R2 0.79175\n",
      "epoch 12, batch 3: loss 0.04643, R2 0.88597\n",
      "epoch 12, batch 4: loss 0.04018, R2 0.90261\n",
      "epoch 12, batch 5: loss 0.04155, R2 0.89705\n",
      "epoch 12, batch 6: loss 0.03023, R2 0.92491\n",
      "epoch 12, batch 7: loss 0.03683, R2 0.91360\n",
      "epoch 12, batch 8: loss 0.04689, R2 0.88076\n",
      "epoch 12, batch 9: loss 0.02551, R2 0.91770\n",
      "epoch 12, batch 10: loss 0.03267, R2 0.89967\n",
      "epoch 12, batch 11: loss 0.01866, R2 0.93380\n",
      "epoch 12, batch 12: loss 0.06099, R2 0.83909\n",
      "epoch 12, batch 13: loss 0.07460, R2 0.86098\n",
      "epoch 12, batch 14: loss 0.02452, R2 0.93304\n",
      "epoch 12, batch 15: loss 0.05234, R2 0.85854\n",
      "epoch 12, batch 16: loss 0.03314, R2 0.90432\n",
      "epoch 12, batch 17: loss 0.06171, R2 0.84840\n",
      "epoch 12, batch 18: loss 0.02162, R2 0.91406\n",
      "epoch 12, batch 19: loss 0.06975, R2 0.85447\n",
      "epoch 12, batch 20: loss 0.07908, R2 0.83363\n",
      "epoch 12, batch 21: loss 0.02928, R2 0.92659\n",
      "epoch 12, batch 22: loss 0.09622, R2 0.80386\n",
      "epoch 12, batch 23: loss 0.02721, R2 0.90729\n",
      "epoch 12, batch 24: loss 0.09460, R2 0.81894\n",
      "epoch 12, batch 25: loss 0.06142, R2 0.87524\n",
      "epoch 12, batch 26: loss 0.03256, R2 0.89927\n",
      "epoch 12, batch 27: loss 0.02714, R2 0.93530\n",
      "epoch 12, batch 28: loss 0.02730, R2 0.91415\n",
      "epoch 12, batch 29: loss 0.14287, R2 0.64393\n",
      "epoch 12, batch 30: loss 0.03495, R2 0.92092\n",
      "epoch 12, batch 31: loss 0.05276, R2 0.86977\n",
      "epoch 12, batch 32: loss 0.05683, R2 0.85559\n",
      "epoch 12, batch 33: loss 0.01730, R2 0.94504\n",
      "epoch 12, batch 34: loss 0.06059, R2 0.85520\n",
      "epoch 12, batch 35: loss 0.04654, R2 0.89548\n",
      "epoch 12, batch 36: loss 0.04855, R2 0.89562\n",
      "epoch 12, batch 37: loss 0.02938, R2 0.93628\n",
      "epoch 12, batch 38: loss 0.04864, R2 0.85015\n",
      "epoch 12, batch 39: loss 0.07069, R2 0.83863\n",
      "epoch 12, batch 40: loss 0.09600, R2 0.81636\n",
      "epoch 12, batch 41: loss 0.03563, R2 0.90997\n",
      "epoch 12, batch 42: loss 0.08279, R2 0.81084\n",
      "epoch 12, batch 43: loss 0.07381, R2 0.81693\n",
      "epoch 12, batch 44: loss 0.06955, R2 0.84116\n",
      "epoch 12, batch 45: loss 0.05164, R2 0.88508\n",
      "epoch 12, batch 46: loss 0.07222, R2 0.84121\n",
      "epoch 12, batch 47: loss 0.03434, R2 0.92164\n",
      "epoch 12, batch 48: loss 0.07898, R2 0.81925\n",
      "epoch 12, batch 49: loss 0.12420, R2 0.68090\n",
      "epoch 12, batch 50: loss 0.03350, R2 0.91627\n",
      "epoch 12, batch 51: loss 0.04412, R2 0.90105\n",
      "epoch 12, batch 52: loss 0.02894, R2 0.92341\n",
      "epoch 12, batch 53: loss 0.05892, R2 0.85643\n",
      "epoch 12, batch 54: loss 0.04115, R2 0.89991\n",
      "epoch 12, batch 55: loss 0.03161, R2 0.92539\n",
      "epoch 12, batch 56: loss 0.05503, R2 0.86335\n",
      "epoch 12, batch 57: loss 0.02710, R2 0.93542\n",
      "epoch 12, batch 58: loss 0.04468, R2 0.89535\n",
      "epoch 12, batch 59: loss 0.03487, R2 0.90044\n",
      "epoch 12, batch 60: loss 0.02151, R2 0.93287\n",
      "epoch 12, batch 61: loss 0.07526, R2 0.82817\n",
      "epoch 12, batch 62: loss 0.05627, R2 0.85027\n",
      "epoch 12, batch 63: loss 0.05388, R2 0.86739\n",
      "epoch 12, batch 64: loss 0.07074, R2 0.84594\n",
      "epoch 12, batch 65: loss 0.04303, R2 0.88981\n",
      "epoch 12, batch 66: loss 0.05245, R2 0.89781\n",
      "epoch 12, batch 67: loss 0.02465, R2 0.93868\n",
      "epoch 12, batch 68: loss 0.05641, R2 0.87622\n",
      "epoch 12, batch 69: loss 0.02684, R2 0.87708\n",
      "epoch 12, batch 70: loss 0.05116, R2 0.84949\n",
      "epoch 12, batch 71: loss 0.02593, R2 0.88820\n",
      "epoch 12, batch 72: loss 0.04084, R2 0.88547\n",
      "epoch 12, batch 73: loss 0.03549, R2 0.89744\n",
      "epoch 12, batch 74: loss 0.01404, R2 0.92656\n",
      "epoch 12, batch 75: loss 0.02715, R2 0.91746\n",
      "epoch 12, batch 76: loss 0.04658, R2 0.89281\n",
      "epoch 12, batch 77: loss 0.08000, R2 0.79599\n",
      "epoch 12, batch 78: loss 0.02462, R2 0.92481\n",
      "epoch 12, batch 79: loss 0.04542, R2 0.90441\n",
      "epoch 12, batch 80: loss 0.05789, R2 0.85354\n",
      "epoch 12, batch 81: loss 0.05974, R2 0.87296\n",
      "epoch 12, batch 82: loss 0.04969, R2 0.88637\n",
      "epoch 12, batch 83: loss 0.07330, R2 0.81304\n",
      "epoch 12, batch 84: loss 0.11622, R2 0.74932\n",
      "epoch 12, batch 85: loss 0.07404, R2 0.79910\n",
      "epoch 12, batch 86: loss 0.04425, R2 0.84576\n",
      "epoch 12, batch 87: loss 0.03245, R2 0.90722\n",
      "epoch 12, batch 88: loss 0.02916, R2 0.93357\n",
      "epoch 12, batch 89: loss 0.07893, R2 0.80059\n",
      "epoch 12, batch 90: loss 0.05852, R2 0.86077\n",
      "epoch 12, batch 91: loss 0.04203, R2 0.88466\n",
      "epoch 12, batch 92: loss 0.02154, R2 0.94374\n",
      "epoch 12, batch 93: loss 0.03933, R2 0.90433\n",
      "epoch 12, batch 94: loss 0.08969, R2 0.82676\n",
      "epoch 12, batch 95: loss 0.05173, R2 0.85823\n",
      "epoch 12, batch 96: loss 0.05012, R2 0.88349\n",
      "epoch 12, batch 97: loss 0.06301, R2 0.85284\n",
      "epoch 12, batch 98: loss 0.04286, R2 0.89218\n",
      "epoch 12, batch 99: loss 0.10154, R2 0.78344\n",
      "epoch 12, batch 100: loss 0.05366, R2 0.88135\n",
      "epoch 12, batch 101: loss 0.04821, R2 0.86857\n",
      "epoch 12, batch 102: loss 0.07347, R2 0.85757\n",
      "epoch 12, batch 103: loss 0.03284, R2 0.91336\n",
      "epoch 12, batch 104: loss 0.04102, R2 0.88252\n",
      "epoch 12, batch 105: loss 0.06880, R2 0.82820\n",
      "epoch 12, batch 106: loss 0.03153, R2 0.91649\n",
      "epoch 12, batch 107: loss 0.07382, R2 0.82115\n",
      "epoch 12, batch 108: loss 0.04593, R2 0.88660\n",
      "epoch 12, batch 109: loss 0.06820, R2 0.86109\n",
      "epoch 12, batch 110: loss 0.03472, R2 0.89350\n",
      "epoch 12, batch 111: loss 0.01957, R2 0.93641\n",
      "epoch 12, batch 112: loss 0.07094, R2 0.83384\n",
      "epoch 12, batch 113: loss 0.06158, R2 0.83022\n",
      "epoch 12, batch 114: loss 0.04131, R2 0.90725\n",
      "epoch 12, batch 115: loss 0.03070, R2 0.92450\n",
      "epoch 12, batch 116: loss 0.00921, R2 0.95961\n",
      "epoch 12, batch 117: loss 0.05316, R2 0.88817\n",
      "epoch 12, batch 118: loss 0.05862, R2 0.88867\n",
      "epoch 12, batch 119: loss 0.03611, R2 0.90428\n",
      "epoch 12, batch 120: loss 0.11666, R2 0.71288\n",
      "epoch 12, batch 121: loss 0.05885, R2 0.85552\n",
      "epoch 12, batch 122: loss 0.06768, R2 0.86407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, batch 123: loss 0.06129, R2 0.86687\n",
      "epoch 12, batch 124: loss 0.04097, R2 0.91190\n",
      "epoch 12, batch 125: loss 0.05239, R2 0.87671\n",
      "epoch 12, batch 126: loss 0.06304, R2 0.84294\n",
      "epoch 12, batch 127: loss 0.04136, R2 0.90240\n",
      "epoch 12, batch 128: loss 0.06450, R2 0.84641\n",
      "epoch 12, batch 129: loss 0.05088, R2 0.88561\n",
      "epoch 12, batch 130: loss 0.03045, R2 0.89666\n",
      "epoch 12, batch 131: loss 0.03345, R2 0.93077\n",
      "epoch 12, batch 132: loss 0.02940, R2 0.92018\n",
      "epoch 12, batch 133: loss 0.04341, R2 0.89179\n",
      "epoch 12, batch 134: loss 0.05958, R2 0.85619\n",
      "epoch 12, batch 135: loss 0.04045, R2 0.89591\n",
      "epoch 12, batch 136: loss 0.02224, R2 0.94019\n",
      "epoch 12, batch 137: loss 0.03156, R2 0.91402\n",
      "epoch 12, batch 138: loss 0.06401, R2 0.85140\n",
      "epoch 12, batch 139: loss 0.01718, R2 0.95040\n",
      "epoch 12, batch 140: loss 0.07265, R2 0.84124\n",
      "epoch 12, batch 141: loss 0.03238, R2 0.89742\n",
      "epoch 12, batch 142: loss 0.08862, R2 0.81039\n",
      "epoch 12, batch 143: loss 0.03888, R2 0.90332\n",
      "epoch 12, batch 144: loss 0.02763, R2 0.90624\n",
      "epoch 12, batch 145: loss 0.08057, R2 0.82492\n",
      "epoch 12, batch 146: loss 0.06994, R2 0.85928\n",
      "epoch 12, batch 147: loss 0.05579, R2 0.87073\n",
      "epoch 12, batch 148: loss 0.09045, R2 0.79063\n",
      "epoch 12, batch 149: loss 0.04421, R2 0.90803\n",
      "epoch 12, batch 150: loss 0.05586, R2 0.85565\n",
      "epoch 12, batch 151: loss 0.03651, R2 0.88044\n",
      "epoch 12, batch 152: loss 0.03808, R2 0.90909\n",
      "epoch 12, batch 153: loss 0.03599, R2 0.92459\n",
      "epoch 12, batch 154: loss 0.06146, R2 0.82506\n",
      "epoch 12, batch 155: loss 0.04572, R2 0.87946\n",
      "epoch 12, batch 156: loss 0.06734, R2 0.86129\n",
      "epoch 12, batch 157: loss 0.07036, R2 0.87532\n",
      "epoch 12, batch 158: loss 0.02694, R2 0.87880\n",
      "epoch 12, batch 159: loss 0.06842, R2 0.85843\n",
      "epoch 12, batch 160: loss 0.08447, R2 0.82248\n",
      "epoch 12, batch 161: loss 0.05449, R2 0.87535\n",
      "epoch 12, batch 162: loss 0.07363, R2 0.83947\n",
      "epoch 12, batch 163: loss 0.04010, R2 0.90933\n",
      "epoch 12, batch 164: loss 0.03910, R2 0.90381\n",
      "epoch 12, batch 165: loss 0.02971, R2 0.93682\n",
      "epoch 12, batch 166: loss 0.05441, R2 0.86743\n",
      "epoch 12, batch 167: loss 0.04028, R2 0.88844\n",
      "epoch 12, batch 168: loss 0.03905, R2 0.91607\n",
      "epoch 12, batch 169: loss 0.02158, R2 0.92956\n",
      "epoch 12, batch 170: loss 0.04811, R2 0.88649\n",
      "epoch 12, batch 171: loss 0.06123, R2 0.87259\n",
      "epoch 12, batch 172: loss 0.04450, R2 0.89513\n",
      "epoch 12, batch 173: loss 0.08680, R2 0.77662\n",
      "epoch 12, batch 174: loss 0.01600, R2 0.95351\n",
      "epoch 12, batch 175: loss 0.05342, R2 0.88896\n",
      "epoch 12, batch 176: loss 0.07179, R2 0.83966\n",
      "epoch 12, batch 177: loss 0.05220, R2 0.89743\n",
      "epoch 12, batch 178: loss 0.03369, R2 0.91775\n",
      "epoch 12, batch 179: loss 0.04541, R2 0.90193\n",
      "epoch 12, batch 180: loss 0.03743, R2 0.90339\n",
      "epoch 12, batch 181: loss 0.03090, R2 0.92532\n",
      "epoch 12, batch 182: loss 0.03070, R2 0.91610\n",
      "epoch 12, batch 183: loss 0.04864, R2 0.83940\n",
      "epoch 12, batch 184: loss 0.05176, R2 0.84114\n",
      "epoch 12, batch 185: loss 0.03824, R2 0.88696\n",
      "epoch 12, batch 186: loss 0.06786, R2 0.83748\n",
      "epoch 12, batch 187: loss 0.05284, R2 0.89461\n",
      "epoch 12, batch 188: loss 0.05502, R2 0.83677\n",
      "epoch 12, batch 189: loss 0.07686, R2 0.80987\n",
      "epoch 12, batch 190: loss 0.04065, R2 0.89271\n",
      "epoch 12, batch 191: loss 0.08926, R2 0.80992\n",
      "epoch 12, batch 192: loss 0.04714, R2 0.88480\n",
      "epoch 12, batch 193: loss 0.10473, R2 0.78634\n",
      "epoch 12, batch 194: loss 0.03375, R2 0.91258\n",
      "epoch 12, batch 195: loss 0.03809, R2 0.87410\n",
      "epoch 12, batch 196: loss 0.03322, R2 0.90009\n",
      "epoch 12, batch 197: loss 0.09672, R2 0.78667\n",
      "epoch 12, batch 198: loss 0.04496, R2 0.91232\n",
      "epoch 12, batch 199: loss 0.11291, R2 0.76477\n",
      "epoch 12, batch 200: loss 0.07839, R2 0.84700\n",
      "epoch 12, batch 201: loss 0.04357, R2 0.90488\n",
      "epoch 12, batch 202: loss 0.07166, R2 0.82976\n",
      "epoch 12, batch 203: loss 0.01976, R2 0.93718\n",
      "epoch 12, batch 204: loss 0.02716, R2 0.92170\n",
      "epoch 12, batch 205: loss 0.02353, R2 0.94284\n",
      "epoch 12, batch 206: loss 0.02600, R2 0.93236\n",
      "epoch 12, batch 207: loss 0.05210, R2 0.87852\n",
      "epoch 12, batch 208: loss 0.05475, R2 0.85933\n",
      "epoch 12, batch 209: loss 0.05697, R2 0.86024\n",
      "epoch 12, batch 210: loss 0.02925, R2 0.92271\n",
      "epoch 12, batch 211: loss 0.10644, R2 0.77555\n",
      "epoch 12, batch 212: loss 0.05696, R2 0.86144\n",
      "epoch 12, batch 213: loss 0.07338, R2 0.83629\n",
      "epoch 12, batch 214: loss 0.03013, R2 0.92383\n",
      "epoch 12, batch 215: loss 0.03650, R2 0.88812\n",
      "epoch 12, batch 216: loss 0.01546, R2 0.95174\n",
      "epoch 12, batch 217: loss 0.02855, R2 0.93074\n",
      "epoch 12, batch 218: loss 0.09039, R2 0.81778\n",
      "epoch 12, batch 219: loss 0.05770, R2 0.88688\n",
      "epoch 12, batch 220: loss 0.02025, R2 0.94103\n",
      "epoch 12, batch 221: loss 0.05288, R2 0.87858\n",
      "epoch 12, batch 222: loss 0.01233, R2 0.95457\n",
      "epoch 12, batch 223: loss 0.02702, R2 0.90380\n",
      "epoch 12, batch 224: loss 0.05669, R2 0.88404\n",
      "epoch 12, batch 225: loss 0.05066, R2 0.89140\n",
      "epoch 12, batch 226: loss 0.05366, R2 0.88345\n",
      "epoch 12, batch 227: loss 0.08022, R2 0.82996\n",
      "epoch 12, batch 228: loss 0.04638, R2 0.88030\n",
      "epoch 12, batch 229: loss 0.03633, R2 0.90557\n",
      "epoch 12, batch 230: loss 0.08155, R2 0.83306\n",
      "epoch 12, batch 231: loss 0.01388, R2 0.95187\n",
      "epoch 12, batch 232: loss 0.03156, R2 0.91871\n",
      "epoch 12, batch 233: loss 0.04900, R2 0.88759\n",
      "epoch 12, batch 234: loss 0.08547, R2 0.79456\n",
      "epoch 12, batch 235: loss 0.04569, R2 0.89548\n",
      "epoch 12, batch 236: loss 0.08820, R2 0.80087\n",
      "epoch 12, batch 237: loss 0.05072, R2 0.86466\n",
      "epoch 12, batch 238: loss 0.05716, R2 0.87763\n",
      "epoch 12, batch 239: loss 0.07998, R2 0.79741\n",
      "epoch 12, batch 240: loss 0.05324, R2 0.88659\n",
      "epoch 12, batch 241: loss 0.04315, R2 0.91363\n",
      "epoch 12, batch 242: loss 0.05798, R2 0.88706\n",
      "epoch 12, batch 243: loss 0.02258, R2 0.92862\n",
      "epoch 12, batch 244: loss 0.07141, R2 0.85477\n",
      "epoch 12, batch 245: loss 0.02251, R2 0.91034\n",
      "epoch 12, batch 246: loss 0.03263, R2 0.91268\n",
      "epoch 12, batch 247: loss 0.02365, R2 0.92227\n",
      "epoch 12, batch 248: loss 0.02263, R2 0.92224\n",
      "epoch 12, batch 249: loss 0.04042, R2 0.91632\n",
      "epoch 13, batch 0: loss 0.08829, R2 0.80211\n",
      "epoch 13, batch 1: loss 0.05765, R2 0.85922\n",
      "epoch 13, batch 2: loss 0.08708, R2 0.81105\n",
      "epoch 13, batch 3: loss 0.10349, R2 0.70430\n",
      "epoch 13, batch 4: loss 0.05142, R2 0.87987\n",
      "epoch 13, batch 5: loss 0.03072, R2 0.92670\n",
      "epoch 13, batch 6: loss 0.03434, R2 0.91284\n",
      "epoch 13, batch 7: loss 0.02243, R2 0.93830\n",
      "epoch 13, batch 8: loss 0.06877, R2 0.85066\n",
      "epoch 13, batch 9: loss 0.08922, R2 0.78295\n",
      "epoch 13, batch 10: loss 0.09222, R2 0.80163\n",
      "epoch 13, batch 11: loss 0.02826, R2 0.89960\n",
      "epoch 13, batch 12: loss 0.07548, R2 0.82018\n",
      "epoch 13, batch 13: loss 0.04774, R2 0.84118\n",
      "epoch 13, batch 14: loss 0.11588, R2 0.72098\n",
      "epoch 13, batch 15: loss 0.06805, R2 0.84651\n",
      "epoch 13, batch 16: loss 0.05304, R2 0.88915\n",
      "epoch 13, batch 17: loss 0.05818, R2 0.84601\n",
      "epoch 13, batch 18: loss 0.03453, R2 0.90623\n",
      "epoch 13, batch 19: loss 0.03593, R2 0.92283\n",
      "epoch 13, batch 20: loss 0.04100, R2 0.91032\n",
      "epoch 13, batch 21: loss 0.04668, R2 0.87507\n",
      "epoch 13, batch 22: loss 0.02909, R2 0.91893\n",
      "epoch 13, batch 23: loss 0.03424, R2 0.92232\n",
      "epoch 13, batch 24: loss 0.03114, R2 0.88144\n",
      "epoch 13, batch 25: loss 0.06058, R2 0.86682\n",
      "epoch 13, batch 26: loss 0.07272, R2 0.83715\n",
      "epoch 13, batch 27: loss 0.03311, R2 0.91573\n",
      "epoch 13, batch 28: loss 0.06197, R2 0.87044\n",
      "epoch 13, batch 29: loss 0.03419, R2 0.89848\n",
      "epoch 13, batch 30: loss 0.02795, R2 0.92557\n",
      "epoch 13, batch 31: loss 0.05841, R2 0.89109\n",
      "epoch 13, batch 32: loss 0.07936, R2 0.83429\n",
      "epoch 13, batch 33: loss 0.02588, R2 0.93566\n",
      "epoch 13, batch 34: loss 0.03133, R2 0.90758\n",
      "epoch 13, batch 35: loss 0.05512, R2 0.88128\n",
      "epoch 13, batch 36: loss 0.04279, R2 0.90539\n",
      "epoch 13, batch 37: loss 0.05694, R2 0.84191\n",
      "epoch 13, batch 38: loss 0.06885, R2 0.84673\n",
      "epoch 13, batch 39: loss 0.04052, R2 0.90744\n",
      "epoch 13, batch 40: loss 0.06350, R2 0.87227\n",
      "epoch 13, batch 41: loss 0.05948, R2 0.85387\n",
      "epoch 13, batch 42: loss 0.02613, R2 0.93568\n",
      "epoch 13, batch 43: loss 0.05419, R2 0.87606\n",
      "epoch 13, batch 44: loss 0.05195, R2 0.88516\n",
      "epoch 13, batch 45: loss 0.06371, R2 0.84078\n",
      "epoch 13, batch 46: loss 0.02389, R2 0.94375\n",
      "epoch 13, batch 47: loss 0.03368, R2 0.92798\n",
      "epoch 13, batch 48: loss 0.06236, R2 0.86121\n",
      "epoch 13, batch 49: loss 0.06453, R2 0.86179\n",
      "epoch 13, batch 50: loss 0.06178, R2 0.82605\n",
      "epoch 13, batch 51: loss 0.02955, R2 0.91224\n",
      "epoch 13, batch 52: loss 0.03669, R2 0.88606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, batch 53: loss 0.04212, R2 0.89705\n",
      "epoch 13, batch 54: loss 0.08793, R2 0.83635\n",
      "epoch 13, batch 55: loss 0.04839, R2 0.87831\n",
      "epoch 13, batch 56: loss 0.02865, R2 0.92810\n",
      "epoch 13, batch 57: loss 0.07113, R2 0.83357\n",
      "epoch 13, batch 58: loss 0.03602, R2 0.89049\n",
      "epoch 13, batch 59: loss 0.01633, R2 0.94550\n",
      "epoch 13, batch 60: loss 0.04472, R2 0.87734\n",
      "epoch 13, batch 61: loss 0.04058, R2 0.90125\n",
      "epoch 13, batch 62: loss 0.03139, R2 0.88152\n",
      "epoch 13, batch 63: loss 0.02195, R2 0.93942\n",
      "epoch 13, batch 64: loss 0.02155, R2 0.93965\n",
      "epoch 13, batch 65: loss 0.02727, R2 0.92441\n",
      "epoch 13, batch 66: loss 0.03527, R2 0.90605\n",
      "epoch 13, batch 67: loss 0.02419, R2 0.94373\n",
      "epoch 13, batch 68: loss 0.04260, R2 0.89996\n",
      "epoch 13, batch 69: loss 0.02405, R2 0.93838\n",
      "epoch 13, batch 70: loss 0.12455, R2 0.73890\n",
      "epoch 13, batch 71: loss 0.12795, R2 0.74445\n",
      "epoch 13, batch 72: loss 0.04236, R2 0.90414\n",
      "epoch 13, batch 73: loss 0.05534, R2 0.88447\n",
      "epoch 13, batch 74: loss 0.09640, R2 0.78426\n",
      "epoch 13, batch 75: loss 0.02410, R2 0.93144\n",
      "epoch 13, batch 76: loss 0.05302, R2 0.89737\n",
      "epoch 13, batch 77: loss 0.09958, R2 0.75943\n",
      "epoch 13, batch 78: loss 0.06457, R2 0.85179\n",
      "epoch 13, batch 79: loss 0.02026, R2 0.94825\n",
      "epoch 13, batch 80: loss 0.06329, R2 0.84759\n",
      "epoch 13, batch 81: loss 0.05314, R2 0.87520\n",
      "epoch 13, batch 82: loss 0.03805, R2 0.87668\n",
      "epoch 13, batch 83: loss 0.02264, R2 0.92975\n",
      "epoch 13, batch 84: loss 0.04771, R2 0.88110\n",
      "epoch 13, batch 85: loss 0.07528, R2 0.84937\n",
      "epoch 13, batch 86: loss 0.02131, R2 0.90848\n",
      "epoch 13, batch 87: loss 0.03196, R2 0.86442\n",
      "epoch 13, batch 88: loss 0.02526, R2 0.94097\n",
      "epoch 13, batch 89: loss 0.05185, R2 0.88570\n",
      "epoch 13, batch 90: loss 0.04977, R2 0.89112\n",
      "epoch 13, batch 91: loss 0.05610, R2 0.85904\n",
      "epoch 13, batch 92: loss 0.02634, R2 0.92410\n",
      "epoch 13, batch 93: loss 0.02584, R2 0.89602\n",
      "epoch 13, batch 94: loss 0.08752, R2 0.84115\n",
      "epoch 13, batch 95: loss 0.02206, R2 0.91544\n",
      "epoch 13, batch 96: loss 0.05416, R2 0.88553\n",
      "epoch 13, batch 97: loss 0.05145, R2 0.87881\n",
      "epoch 13, batch 98: loss 0.05341, R2 0.87814\n",
      "epoch 13, batch 99: loss 0.04303, R2 0.88236\n",
      "epoch 13, batch 100: loss 0.05864, R2 0.88371\n",
      "epoch 13, batch 101: loss 0.05015, R2 0.86426\n",
      "epoch 13, batch 102: loss 0.02506, R2 0.92801\n",
      "epoch 13, batch 103: loss 0.06846, R2 0.82462\n",
      "epoch 13, batch 104: loss 0.01243, R2 0.95022\n",
      "epoch 13, batch 105: loss 0.07519, R2 0.84354\n",
      "epoch 13, batch 106: loss 0.03871, R2 0.90396\n",
      "epoch 13, batch 107: loss 0.06963, R2 0.79996\n",
      "epoch 13, batch 108: loss 0.03729, R2 0.91615\n",
      "epoch 13, batch 109: loss 0.03971, R2 0.90069\n",
      "epoch 13, batch 110: loss 0.01288, R2 0.93694\n",
      "epoch 13, batch 111: loss 0.12485, R2 0.75062\n",
      "epoch 13, batch 112: loss 0.02663, R2 0.92241\n",
      "epoch 13, batch 113: loss 0.02812, R2 0.92340\n",
      "epoch 13, batch 114: loss 0.05329, R2 0.85228\n",
      "epoch 13, batch 115: loss 0.02058, R2 0.94306\n",
      "epoch 13, batch 116: loss 0.02656, R2 0.93566\n",
      "epoch 13, batch 117: loss 0.03440, R2 0.91522\n",
      "epoch 13, batch 118: loss 0.01888, R2 0.94458\n",
      "epoch 13, batch 119: loss 0.05245, R2 0.86868\n",
      "epoch 13, batch 120: loss 0.07908, R2 0.83301\n",
      "epoch 13, batch 121: loss 0.06936, R2 0.81648\n",
      "epoch 13, batch 122: loss 0.06510, R2 0.81775\n",
      "epoch 13, batch 123: loss 0.02422, R2 0.93283\n",
      "epoch 13, batch 124: loss 0.03961, R2 0.90061\n",
      "epoch 13, batch 125: loss 0.07969, R2 0.80790\n",
      "epoch 13, batch 126: loss 0.07928, R2 0.82448\n",
      "epoch 13, batch 127: loss 0.05605, R2 0.86648\n",
      "epoch 13, batch 128: loss 0.06167, R2 0.87609\n",
      "epoch 13, batch 129: loss 0.06543, R2 0.82731\n",
      "epoch 13, batch 130: loss 0.04646, R2 0.88763\n",
      "epoch 13, batch 131: loss 0.04257, R2 0.89073\n",
      "epoch 13, batch 132: loss 0.06198, R2 0.85547\n",
      "epoch 13, batch 133: loss 0.08047, R2 0.82570\n",
      "epoch 13, batch 134: loss 0.06128, R2 0.82733\n",
      "epoch 13, batch 135: loss 0.05984, R2 0.85955\n",
      "epoch 13, batch 136: loss 0.03663, R2 0.91333\n",
      "epoch 13, batch 137: loss 0.03214, R2 0.91203\n",
      "epoch 13, batch 138: loss 0.04408, R2 0.89137\n",
      "epoch 13, batch 139: loss 0.05615, R2 0.86642\n",
      "epoch 13, batch 140: loss 0.03632, R2 0.88305\n",
      "epoch 13, batch 141: loss 0.05933, R2 0.86727\n",
      "epoch 13, batch 142: loss 0.03295, R2 0.91952\n",
      "epoch 13, batch 143: loss 0.04307, R2 0.86419\n",
      "epoch 13, batch 144: loss 0.02392, R2 0.92806\n",
      "epoch 13, batch 145: loss 0.03900, R2 0.87028\n",
      "epoch 13, batch 146: loss 0.04055, R2 0.89358\n",
      "epoch 13, batch 147: loss 0.06313, R2 0.83655\n",
      "epoch 13, batch 148: loss 0.03845, R2 0.91493\n",
      "epoch 13, batch 149: loss 0.05074, R2 0.88169\n",
      "epoch 13, batch 150: loss 0.01715, R2 0.93767\n",
      "epoch 13, batch 151: loss 0.05290, R2 0.89048\n",
      "epoch 13, batch 152: loss 0.09514, R2 0.78129\n",
      "epoch 13, batch 153: loss 0.04281, R2 0.89127\n",
      "epoch 13, batch 154: loss 0.04641, R2 0.89603\n",
      "epoch 13, batch 155: loss 0.09290, R2 0.80078\n",
      "epoch 13, batch 156: loss 0.06990, R2 0.83403\n",
      "epoch 13, batch 157: loss 0.04401, R2 0.89647\n",
      "epoch 13, batch 158: loss 0.04522, R2 0.89739\n",
      "epoch 13, batch 159: loss 0.05312, R2 0.87904\n",
      "epoch 13, batch 160: loss 0.06690, R2 0.82586\n",
      "epoch 13, batch 161: loss 0.03999, R2 0.91212\n",
      "epoch 13, batch 162: loss 0.05225, R2 0.88679\n",
      "epoch 13, batch 163: loss 0.04290, R2 0.89867\n",
      "epoch 13, batch 164: loss 0.02408, R2 0.92177\n",
      "epoch 13, batch 165: loss 0.06886, R2 0.86027\n",
      "epoch 13, batch 166: loss 0.05397, R2 0.85993\n",
      "epoch 13, batch 167: loss 0.05055, R2 0.88561\n",
      "epoch 13, batch 168: loss 0.03551, R2 0.91753\n",
      "epoch 13, batch 169: loss 0.03419, R2 0.92611\n",
      "epoch 13, batch 170: loss 0.04557, R2 0.88039\n",
      "epoch 13, batch 171: loss 0.06879, R2 0.86313\n",
      "epoch 13, batch 172: loss 0.02946, R2 0.92880\n",
      "epoch 13, batch 173: loss 0.05669, R2 0.86897\n",
      "epoch 13, batch 174: loss 0.04468, R2 0.87011\n",
      "epoch 13, batch 175: loss 0.04341, R2 0.91272\n",
      "epoch 13, batch 176: loss 0.09770, R2 0.77789\n",
      "epoch 13, batch 177: loss 0.03298, R2 0.90270\n",
      "epoch 13, batch 178: loss 0.01722, R2 0.94283\n",
      "epoch 13, batch 179: loss 0.05180, R2 0.85299\n",
      "epoch 13, batch 180: loss 0.01561, R2 0.92936\n",
      "epoch 13, batch 181: loss 0.01885, R2 0.95030\n",
      "epoch 13, batch 182: loss 0.05740, R2 0.85559\n",
      "epoch 13, batch 183: loss 0.03603, R2 0.92093\n",
      "epoch 13, batch 184: loss 0.05318, R2 0.89058\n",
      "epoch 13, batch 185: loss 0.03724, R2 0.90250\n",
      "epoch 13, batch 186: loss 0.04563, R2 0.87930\n",
      "epoch 13, batch 187: loss 0.12826, R2 0.74677\n",
      "epoch 13, batch 188: loss 0.11345, R2 0.76729\n",
      "epoch 13, batch 189: loss 0.07397, R2 0.85231\n",
      "epoch 13, batch 190: loss 0.02190, R2 0.94197\n",
      "epoch 13, batch 191: loss 0.04858, R2 0.88165\n",
      "epoch 13, batch 192: loss 0.04458, R2 0.90630\n",
      "epoch 13, batch 193: loss 0.04790, R2 0.90291\n",
      "epoch 13, batch 194: loss 0.09123, R2 0.78799\n",
      "epoch 13, batch 195: loss 0.03368, R2 0.90903\n",
      "epoch 13, batch 196: loss 0.02994, R2 0.90598\n",
      "epoch 13, batch 197: loss 0.10466, R2 0.80211\n",
      "epoch 13, batch 198: loss 0.03330, R2 0.90587\n",
      "epoch 13, batch 199: loss 0.04353, R2 0.89059\n",
      "epoch 13, batch 200: loss 0.05126, R2 0.87471\n",
      "epoch 13, batch 201: loss 0.02827, R2 0.92083\n",
      "epoch 13, batch 202: loss 0.02248, R2 0.93532\n",
      "epoch 13, batch 203: loss 0.11867, R2 0.75488\n",
      "epoch 13, batch 204: loss 0.02227, R2 0.93410\n",
      "epoch 13, batch 205: loss 0.05074, R2 0.88062\n",
      "epoch 13, batch 206: loss 0.05817, R2 0.85451\n",
      "epoch 13, batch 207: loss 0.07472, R2 0.86799\n",
      "epoch 13, batch 208: loss 0.06931, R2 0.84080\n",
      "epoch 13, batch 209: loss 0.03713, R2 0.91013\n",
      "epoch 13, batch 210: loss 0.05652, R2 0.83775\n",
      "epoch 13, batch 211: loss 0.08498, R2 0.83249\n",
      "epoch 13, batch 212: loss 0.04834, R2 0.88509\n",
      "epoch 13, batch 213: loss 0.03753, R2 0.87394\n",
      "epoch 13, batch 214: loss 0.03974, R2 0.89619\n",
      "epoch 13, batch 215: loss 0.06471, R2 0.84627\n",
      "epoch 13, batch 216: loss 0.05126, R2 0.89263\n",
      "epoch 13, batch 217: loss 0.06731, R2 0.86448\n",
      "epoch 13, batch 218: loss 0.02314, R2 0.94403\n",
      "epoch 13, batch 219: loss 0.02587, R2 0.93263\n",
      "epoch 13, batch 220: loss 0.04833, R2 0.89972\n",
      "epoch 13, batch 221: loss 0.05200, R2 0.88147\n",
      "epoch 13, batch 222: loss 0.01976, R2 0.93419\n",
      "epoch 13, batch 223: loss 0.03952, R2 0.87700\n",
      "epoch 13, batch 224: loss 0.03652, R2 0.86937\n",
      "epoch 13, batch 225: loss 0.05144, R2 0.89523\n",
      "epoch 13, batch 226: loss 0.04912, R2 0.88567\n",
      "epoch 13, batch 227: loss 0.03765, R2 0.91111\n",
      "epoch 13, batch 228: loss 0.02751, R2 0.92381\n",
      "epoch 13, batch 229: loss 0.06642, R2 0.86468\n",
      "epoch 13, batch 230: loss 0.01364, R2 0.95803\n",
      "epoch 13, batch 231: loss 0.01521, R2 0.93327\n",
      "epoch 13, batch 232: loss 0.06100, R2 0.87220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, batch 233: loss 0.07227, R2 0.84289\n",
      "epoch 13, batch 234: loss 0.06269, R2 0.86752\n",
      "epoch 13, batch 235: loss 0.04506, R2 0.88352\n",
      "epoch 13, batch 236: loss 0.09885, R2 0.78734\n",
      "epoch 13, batch 237: loss 0.08574, R2 0.75443\n",
      "epoch 13, batch 238: loss 0.08356, R2 0.80648\n",
      "epoch 13, batch 239: loss 0.06305, R2 0.82028\n",
      "epoch 13, batch 240: loss 0.02512, R2 0.92139\n",
      "epoch 13, batch 241: loss 0.03303, R2 0.90724\n",
      "epoch 13, batch 242: loss 0.05342, R2 0.88933\n",
      "epoch 13, batch 243: loss 0.06439, R2 0.82166\n",
      "epoch 13, batch 244: loss 0.02693, R2 0.92718\n",
      "epoch 13, batch 245: loss 0.04538, R2 0.90051\n",
      "epoch 13, batch 246: loss 0.06447, R2 0.85333\n",
      "epoch 13, batch 247: loss 0.04450, R2 0.89783\n",
      "epoch 13, batch 248: loss 0.04871, R2 0.86439\n",
      "epoch 13, batch 249: loss 0.02592, R2 0.93115\n",
      "epoch 14, batch 0: loss 0.04786, R2 0.87187\n",
      "epoch 14, batch 1: loss 0.02663, R2 0.94319\n",
      "epoch 14, batch 2: loss 0.02977, R2 0.91937\n",
      "epoch 14, batch 3: loss 0.04526, R2 0.90085\n",
      "epoch 14, batch 4: loss 0.03257, R2 0.92044\n",
      "epoch 14, batch 5: loss 0.02700, R2 0.91902\n",
      "epoch 14, batch 6: loss 0.04046, R2 0.92051\n",
      "epoch 14, batch 7: loss 0.02034, R2 0.93902\n",
      "epoch 14, batch 8: loss 0.04959, R2 0.88696\n",
      "epoch 14, batch 9: loss 0.06273, R2 0.87686\n",
      "epoch 14, batch 10: loss 0.05470, R2 0.88408\n",
      "epoch 14, batch 11: loss 0.02054, R2 0.94235\n",
      "epoch 14, batch 12: loss 0.13400, R2 0.67485\n",
      "epoch 14, batch 13: loss 0.06147, R2 0.83066\n",
      "epoch 14, batch 14: loss 0.08425, R2 0.80014\n",
      "epoch 14, batch 15: loss 0.03029, R2 0.92182\n",
      "epoch 14, batch 16: loss 0.08563, R2 0.80749\n",
      "epoch 14, batch 17: loss 0.02815, R2 0.91368\n",
      "epoch 14, batch 18: loss 0.05561, R2 0.87577\n",
      "epoch 14, batch 19: loss 0.03304, R2 0.92505\n",
      "epoch 14, batch 20: loss 0.03249, R2 0.90487\n",
      "epoch 14, batch 21: loss 0.06260, R2 0.85920\n",
      "epoch 14, batch 22: loss 0.07714, R2 0.80948\n",
      "epoch 14, batch 23: loss 0.03020, R2 0.90945\n",
      "epoch 14, batch 24: loss 0.02523, R2 0.93068\n",
      "epoch 14, batch 25: loss 0.06904, R2 0.82645\n",
      "epoch 14, batch 26: loss 0.08618, R2 0.80489\n",
      "epoch 14, batch 27: loss 0.05858, R2 0.86250\n",
      "epoch 14, batch 28: loss 0.02433, R2 0.93092\n",
      "epoch 14, batch 29: loss 0.09902, R2 0.76444\n",
      "epoch 14, batch 30: loss 0.05896, R2 0.86438\n",
      "epoch 14, batch 31: loss 0.06431, R2 0.86878\n",
      "epoch 14, batch 32: loss 0.09561, R2 0.82340\n",
      "epoch 14, batch 33: loss 0.03109, R2 0.92485\n",
      "epoch 14, batch 34: loss 0.05274, R2 0.87621\n",
      "epoch 14, batch 35: loss 0.02360, R2 0.93780\n",
      "epoch 14, batch 36: loss 0.03830, R2 0.89241\n",
      "epoch 14, batch 37: loss 0.02051, R2 0.93812\n",
      "epoch 14, batch 38: loss 0.04582, R2 0.88743\n",
      "epoch 14, batch 39: loss 0.04832, R2 0.86653\n",
      "epoch 14, batch 40: loss 0.03307, R2 0.89436\n",
      "epoch 14, batch 41: loss 0.02133, R2 0.95079\n",
      "epoch 14, batch 42: loss 0.06520, R2 0.85751\n",
      "epoch 14, batch 43: loss 0.02132, R2 0.93649\n",
      "epoch 14, batch 44: loss 0.06553, R2 0.84639\n",
      "epoch 14, batch 45: loss 0.04515, R2 0.89073\n",
      "epoch 14, batch 46: loss 0.08362, R2 0.80192\n",
      "epoch 14, batch 47: loss 0.04589, R2 0.87952\n",
      "epoch 14, batch 48: loss 0.06665, R2 0.84636\n",
      "epoch 14, batch 49: loss 0.03881, R2 0.91120\n",
      "epoch 14, batch 50: loss 0.03988, R2 0.89849\n",
      "epoch 14, batch 51: loss 0.06599, R2 0.82862\n",
      "epoch 14, batch 52: loss 0.05556, R2 0.86076\n",
      "epoch 14, batch 53: loss 0.06149, R2 0.79615\n",
      "epoch 14, batch 54: loss 0.03861, R2 0.89861\n",
      "epoch 14, batch 55: loss 0.04847, R2 0.90029\n",
      "epoch 14, batch 56: loss 0.03563, R2 0.91309\n",
      "epoch 14, batch 57: loss 0.01651, R2 0.94736\n",
      "epoch 14, batch 58: loss 0.02593, R2 0.93227\n",
      "epoch 14, batch 59: loss 0.03621, R2 0.87230\n",
      "epoch 14, batch 60: loss 0.07898, R2 0.81844\n",
      "epoch 14, batch 61: loss 0.11053, R2 0.77002\n",
      "epoch 14, batch 62: loss 0.08648, R2 0.79411\n",
      "epoch 14, batch 63: loss 0.04818, R2 0.89733\n",
      "epoch 14, batch 64: loss 0.05164, R2 0.86283\n",
      "epoch 14, batch 65: loss 0.08010, R2 0.84273\n",
      "epoch 14, batch 66: loss 0.05204, R2 0.88261\n",
      "epoch 14, batch 67: loss 0.05590, R2 0.89800\n",
      "epoch 14, batch 68: loss 0.08186, R2 0.80539\n",
      "epoch 14, batch 69: loss 0.03451, R2 0.91164\n",
      "epoch 14, batch 70: loss 0.09602, R2 0.76073\n",
      "epoch 14, batch 71: loss 0.02535, R2 0.94569\n",
      "epoch 14, batch 72: loss 0.03013, R2 0.91456\n",
      "epoch 14, batch 73: loss 0.08898, R2 0.82002\n",
      "epoch 14, batch 74: loss 0.09864, R2 0.76003\n",
      "epoch 14, batch 75: loss 0.08905, R2 0.84250\n",
      "epoch 14, batch 76: loss 0.02584, R2 0.93279\n",
      "epoch 14, batch 77: loss 0.06931, R2 0.85088\n",
      "epoch 14, batch 78: loss 0.02928, R2 0.91393\n",
      "epoch 14, batch 79: loss 0.10389, R2 0.79825\n",
      "epoch 14, batch 80: loss 0.03670, R2 0.89393\n",
      "epoch 14, batch 81: loss 0.03703, R2 0.91625\n",
      "epoch 14, batch 82: loss 0.06482, R2 0.87605\n",
      "epoch 14, batch 83: loss 0.02977, R2 0.91948\n",
      "epoch 14, batch 84: loss 0.03017, R2 0.91349\n",
      "epoch 14, batch 85: loss 0.03603, R2 0.91174\n",
      "epoch 14, batch 86: loss 0.04927, R2 0.84780\n",
      "epoch 14, batch 87: loss 0.04749, R2 0.86148\n",
      "epoch 14, batch 88: loss 0.03568, R2 0.90027\n",
      "epoch 14, batch 89: loss 0.03913, R2 0.90418\n",
      "epoch 14, batch 90: loss 0.03987, R2 0.90646\n",
      "epoch 14, batch 91: loss 0.04457, R2 0.84285\n",
      "epoch 14, batch 92: loss 0.01943, R2 0.95440\n",
      "epoch 14, batch 93: loss 0.03076, R2 0.92576\n",
      "epoch 14, batch 94: loss 0.01905, R2 0.92041\n",
      "epoch 14, batch 95: loss 0.05371, R2 0.85638\n",
      "epoch 14, batch 96: loss 0.03723, R2 0.87017\n",
      "epoch 14, batch 97: loss 0.02479, R2 0.92894\n",
      "epoch 14, batch 98: loss 0.03145, R2 0.92009\n",
      "epoch 14, batch 99: loss 0.10292, R2 0.76127\n",
      "epoch 14, batch 100: loss 0.02405, R2 0.92667\n",
      "epoch 14, batch 101: loss 0.03652, R2 0.91372\n",
      "epoch 14, batch 102: loss 0.09391, R2 0.78264\n",
      "epoch 14, batch 103: loss 0.10941, R2 0.78060\n",
      "epoch 14, batch 104: loss 0.03629, R2 0.87369\n",
      "epoch 14, batch 105: loss 0.05437, R2 0.88641\n",
      "epoch 14, batch 106: loss 0.02081, R2 0.93127\n",
      "epoch 14, batch 107: loss 0.05536, R2 0.88311\n",
      "epoch 14, batch 108: loss 0.05307, R2 0.85893\n",
      "epoch 14, batch 109: loss 0.07071, R2 0.86540\n",
      "epoch 14, batch 110: loss 0.03235, R2 0.92082\n",
      "epoch 14, batch 111: loss 0.05213, R2 0.87936\n",
      "epoch 14, batch 112: loss 0.01957, R2 0.94489\n",
      "epoch 14, batch 113: loss 0.03806, R2 0.90575\n",
      "epoch 14, batch 114: loss 0.05978, R2 0.87460\n",
      "epoch 14, batch 115: loss 0.05706, R2 0.87462\n",
      "epoch 14, batch 116: loss 0.07741, R2 0.81462\n",
      "epoch 14, batch 117: loss 0.04604, R2 0.89801\n",
      "epoch 14, batch 118: loss 0.06141, R2 0.84536\n",
      "epoch 14, batch 119: loss 0.03762, R2 0.92391\n",
      "epoch 14, batch 120: loss 0.05883, R2 0.85598\n",
      "epoch 14, batch 121: loss 0.01768, R2 0.94940\n",
      "epoch 14, batch 122: loss 0.04736, R2 0.89525\n",
      "epoch 14, batch 123: loss 0.06105, R2 0.85874\n",
      "epoch 14, batch 124: loss 0.02898, R2 0.92762\n",
      "epoch 14, batch 125: loss 0.05951, R2 0.86401\n",
      "epoch 14, batch 126: loss 0.04712, R2 0.90945\n",
      "epoch 14, batch 127: loss 0.02790, R2 0.93564\n",
      "epoch 14, batch 128: loss 0.04968, R2 0.85967\n",
      "epoch 14, batch 129: loss 0.08281, R2 0.81221\n",
      "epoch 14, batch 130: loss 0.03975, R2 0.90551\n",
      "epoch 14, batch 131: loss 0.01974, R2 0.94751\n",
      "epoch 14, batch 132: loss 0.08069, R2 0.83607\n",
      "epoch 14, batch 133: loss 0.02050, R2 0.94460\n",
      "epoch 14, batch 134: loss 0.03715, R2 0.90247\n",
      "epoch 14, batch 135: loss 0.05390, R2 0.84799\n",
      "epoch 14, batch 136: loss 0.03836, R2 0.89893\n",
      "epoch 14, batch 137: loss 0.05203, R2 0.88073\n",
      "epoch 14, batch 138: loss 0.04475, R2 0.90730\n",
      "epoch 14, batch 139: loss 0.04392, R2 0.90002\n",
      "epoch 14, batch 140: loss 0.08707, R2 0.80119\n",
      "epoch 14, batch 141: loss 0.04104, R2 0.88293\n",
      "epoch 14, batch 142: loss 0.08175, R2 0.82442\n",
      "epoch 14, batch 143: loss 0.04540, R2 0.88015\n",
      "epoch 14, batch 144: loss 0.04127, R2 0.90859\n",
      "epoch 14, batch 145: loss 0.09524, R2 0.77391\n",
      "epoch 14, batch 146: loss 0.06818, R2 0.83187\n",
      "epoch 14, batch 147: loss 0.03456, R2 0.90667\n",
      "epoch 14, batch 148: loss 0.02096, R2 0.92915\n",
      "epoch 14, batch 149: loss 0.04377, R2 0.88273\n",
      "epoch 14, batch 150: loss 0.05427, R2 0.88499\n",
      "epoch 14, batch 151: loss 0.02377, R2 0.88389\n",
      "epoch 14, batch 152: loss 0.05623, R2 0.85735\n",
      "epoch 14, batch 153: loss 0.05045, R2 0.88342\n",
      "epoch 14, batch 154: loss 0.02275, R2 0.91916\n",
      "epoch 14, batch 155: loss 0.05130, R2 0.87110\n",
      "epoch 14, batch 156: loss 0.08308, R2 0.83049\n",
      "epoch 14, batch 157: loss 0.01416, R2 0.95632\n",
      "epoch 14, batch 158: loss 0.05319, R2 0.83056\n",
      "epoch 14, batch 159: loss 0.04701, R2 0.89540\n",
      "epoch 14, batch 160: loss 0.03523, R2 0.86255\n",
      "epoch 14, batch 161: loss 0.01008, R2 0.94571\n",
      "epoch 14, batch 162: loss 0.08437, R2 0.83705\n",
      "epoch 14, batch 163: loss 0.02128, R2 0.93982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, batch 164: loss 0.03437, R2 0.89640\n",
      "epoch 14, batch 165: loss 0.07148, R2 0.83682\n",
      "epoch 14, batch 166: loss 0.05038, R2 0.87553\n",
      "epoch 14, batch 167: loss 0.05685, R2 0.88813\n",
      "epoch 14, batch 168: loss 0.04860, R2 0.89819\n",
      "epoch 14, batch 169: loss 0.08768, R2 0.81105\n",
      "epoch 14, batch 170: loss 0.05752, R2 0.87925\n",
      "epoch 14, batch 171: loss 0.05794, R2 0.83995\n",
      "epoch 14, batch 172: loss 0.05037, R2 0.87792\n",
      "epoch 14, batch 173: loss 0.11572, R2 0.76019\n",
      "epoch 14, batch 174: loss 0.04752, R2 0.90234\n",
      "epoch 14, batch 175: loss 0.04557, R2 0.87469\n",
      "epoch 14, batch 176: loss 0.06780, R2 0.86581\n",
      "epoch 14, batch 177: loss 0.02679, R2 0.93124\n",
      "epoch 14, batch 178: loss 0.05447, R2 0.87935\n",
      "epoch 14, batch 179: loss 0.03823, R2 0.90068\n",
      "epoch 14, batch 180: loss 0.06347, R2 0.84041\n",
      "epoch 14, batch 181: loss 0.02616, R2 0.94008\n",
      "epoch 14, batch 182: loss 0.04922, R2 0.89336\n",
      "epoch 14, batch 183: loss 0.06537, R2 0.85159\n",
      "epoch 14, batch 184: loss 0.02896, R2 0.93365\n",
      "epoch 14, batch 185: loss 0.10437, R2 0.76266\n",
      "epoch 14, batch 186: loss 0.02914, R2 0.92726\n",
      "epoch 14, batch 187: loss 0.06823, R2 0.85624\n",
      "epoch 14, batch 188: loss 0.02200, R2 0.93555\n",
      "epoch 14, batch 189: loss 0.07209, R2 0.84848\n",
      "epoch 14, batch 190: loss 0.03812, R2 0.86050\n",
      "epoch 14, batch 191: loss 0.03941, R2 0.90580\n",
      "epoch 14, batch 192: loss 0.02257, R2 0.94306\n",
      "epoch 14, batch 193: loss 0.05199, R2 0.86014\n",
      "epoch 14, batch 194: loss 0.06303, R2 0.85703\n",
      "epoch 14, batch 195: loss 0.02983, R2 0.91528\n",
      "epoch 14, batch 196: loss 0.06164, R2 0.88121\n",
      "epoch 14, batch 197: loss 0.04912, R2 0.88813\n",
      "epoch 14, batch 198: loss 0.04292, R2 0.90568\n",
      "epoch 14, batch 199: loss 0.05470, R2 0.87590\n",
      "epoch 14, batch 200: loss 0.03364, R2 0.90054\n",
      "epoch 14, batch 201: loss 0.02970, R2 0.89846\n",
      "epoch 14, batch 202: loss 0.05781, R2 0.88129\n",
      "epoch 14, batch 203: loss 0.02009, R2 0.94516\n",
      "epoch 14, batch 204: loss 0.03165, R2 0.90855\n",
      "epoch 14, batch 205: loss 0.06796, R2 0.85547\n",
      "epoch 14, batch 206: loss 0.05854, R2 0.88253\n",
      "epoch 14, batch 207: loss 0.05111, R2 0.88054\n",
      "epoch 14, batch 208: loss 0.06440, R2 0.84027\n",
      "epoch 14, batch 209: loss 0.05401, R2 0.84547\n",
      "epoch 14, batch 210: loss 0.04745, R2 0.88582\n",
      "epoch 14, batch 211: loss 0.02776, R2 0.93057\n",
      "epoch 14, batch 212: loss 0.03409, R2 0.92269\n",
      "epoch 14, batch 213: loss 0.04929, R2 0.88169\n",
      "epoch 14, batch 214: loss 0.03457, R2 0.90860\n",
      "epoch 14, batch 215: loss 0.01869, R2 0.95295\n",
      "epoch 14, batch 216: loss 0.05529, R2 0.81859\n",
      "epoch 14, batch 217: loss 0.05591, R2 0.86351\n",
      "epoch 14, batch 218: loss 0.07030, R2 0.82988\n",
      "epoch 14, batch 219: loss 0.04156, R2 0.88708\n",
      "epoch 14, batch 220: loss 0.10389, R2 0.75575\n",
      "epoch 14, batch 221: loss 0.07337, R2 0.83964\n",
      "epoch 14, batch 222: loss 0.03989, R2 0.88641\n",
      "epoch 14, batch 223: loss 0.03012, R2 0.92994\n",
      "epoch 14, batch 224: loss 0.04325, R2 0.89994\n",
      "epoch 14, batch 225: loss 0.04852, R2 0.89317\n",
      "epoch 14, batch 226: loss 0.07989, R2 0.79886\n",
      "epoch 14, batch 227: loss 0.03664, R2 0.91044\n",
      "epoch 14, batch 228: loss 0.07480, R2 0.83493\n",
      "epoch 14, batch 229: loss 0.12381, R2 0.77709\n",
      "epoch 14, batch 230: loss 0.03360, R2 0.91576\n",
      "epoch 14, batch 231: loss 0.03702, R2 0.90666\n",
      "epoch 14, batch 232: loss 0.01762, R2 0.94076\n",
      "epoch 14, batch 233: loss 0.02007, R2 0.94039\n",
      "epoch 14, batch 234: loss 0.07462, R2 0.83844\n",
      "epoch 14, batch 235: loss 0.09369, R2 0.79208\n",
      "epoch 14, batch 236: loss 0.03254, R2 0.92341\n",
      "epoch 14, batch 237: loss 0.04498, R2 0.87638\n",
      "epoch 14, batch 238: loss 0.03506, R2 0.90381\n",
      "epoch 14, batch 239: loss 0.04471, R2 0.89821\n",
      "epoch 14, batch 240: loss 0.02503, R2 0.91702\n",
      "epoch 14, batch 241: loss 0.03435, R2 0.92358\n",
      "epoch 14, batch 242: loss 0.09786, R2 0.81485\n",
      "epoch 14, batch 243: loss 0.04413, R2 0.87932\n",
      "epoch 14, batch 244: loss 0.02315, R2 0.92254\n",
      "epoch 14, batch 245: loss 0.06110, R2 0.83601\n",
      "epoch 14, batch 246: loss 0.05821, R2 0.85820\n",
      "epoch 14, batch 247: loss 0.02841, R2 0.91457\n",
      "epoch 14, batch 248: loss 0.05163, R2 0.88891\n",
      "epoch 14, batch 249: loss 0.05025, R2 0.87894\n",
      "epoch 15, batch 0: loss 0.04034, R2 0.89904\n",
      "epoch 15, batch 1: loss 0.02579, R2 0.92927\n",
      "epoch 15, batch 2: loss 0.09234, R2 0.81180\n",
      "epoch 15, batch 3: loss 0.03167, R2 0.92299\n",
      "epoch 15, batch 4: loss 0.06425, R2 0.82213\n",
      "epoch 15, batch 5: loss 0.05784, R2 0.87816\n",
      "epoch 15, batch 6: loss 0.11256, R2 0.78290\n",
      "epoch 15, batch 7: loss 0.03691, R2 0.91772\n",
      "epoch 15, batch 8: loss 0.02110, R2 0.94752\n",
      "epoch 15, batch 9: loss 0.08072, R2 0.82562\n",
      "epoch 15, batch 10: loss 0.02494, R2 0.91797\n",
      "epoch 15, batch 11: loss 0.03884, R2 0.88011\n",
      "epoch 15, batch 12: loss 0.06881, R2 0.82557\n",
      "epoch 15, batch 13: loss 0.01463, R2 0.95078\n",
      "epoch 15, batch 14: loss 0.06457, R2 0.84668\n",
      "epoch 15, batch 15: loss 0.02442, R2 0.89598\n",
      "epoch 15, batch 16: loss 0.04423, R2 0.88744\n",
      "epoch 15, batch 17: loss 0.07986, R2 0.84149\n",
      "epoch 15, batch 18: loss 0.07452, R2 0.82620\n",
      "epoch 15, batch 19: loss 0.07563, R2 0.80605\n",
      "epoch 15, batch 20: loss 0.03879, R2 0.90869\n",
      "epoch 15, batch 21: loss 0.06885, R2 0.86084\n",
      "epoch 15, batch 22: loss 0.05345, R2 0.86334\n",
      "epoch 15, batch 23: loss 0.04209, R2 0.90883\n",
      "epoch 15, batch 24: loss 0.04075, R2 0.88303\n",
      "epoch 15, batch 25: loss 0.03044, R2 0.92336\n",
      "epoch 15, batch 26: loss 0.02100, R2 0.93354\n",
      "epoch 15, batch 27: loss 0.04680, R2 0.88978\n",
      "epoch 15, batch 28: loss 0.02510, R2 0.94184\n",
      "epoch 15, batch 29: loss 0.04221, R2 0.91201\n",
      "epoch 15, batch 30: loss 0.09006, R2 0.79827\n",
      "epoch 15, batch 31: loss 0.06662, R2 0.84808\n",
      "epoch 15, batch 32: loss 0.04342, R2 0.90447\n",
      "epoch 15, batch 33: loss 0.04738, R2 0.86463\n",
      "epoch 15, batch 34: loss 0.03240, R2 0.86009\n",
      "epoch 15, batch 35: loss 0.06753, R2 0.83967\n",
      "epoch 15, batch 36: loss 0.03704, R2 0.87942\n",
      "epoch 15, batch 37: loss 0.04028, R2 0.91742\n",
      "epoch 15, batch 38: loss 0.03987, R2 0.89922\n",
      "epoch 15, batch 39: loss 0.05636, R2 0.88584\n",
      "epoch 15, batch 40: loss 0.04799, R2 0.89613\n",
      "epoch 15, batch 41: loss 0.06522, R2 0.82190\n",
      "epoch 15, batch 42: loss 0.06456, R2 0.84756\n",
      "epoch 15, batch 43: loss 0.08246, R2 0.81354\n",
      "epoch 15, batch 44: loss 0.04802, R2 0.82584\n",
      "epoch 15, batch 45: loss 0.08422, R2 0.81172\n",
      "epoch 15, batch 46: loss 0.08235, R2 0.85342\n",
      "epoch 15, batch 47: loss 0.04678, R2 0.88120\n",
      "epoch 15, batch 48: loss 0.02747, R2 0.93080\n",
      "epoch 15, batch 49: loss 0.06233, R2 0.86026\n",
      "epoch 15, batch 50: loss 0.03375, R2 0.91240\n",
      "epoch 15, batch 51: loss 0.05935, R2 0.84748\n",
      "epoch 15, batch 52: loss 0.05569, R2 0.86780\n",
      "epoch 15, batch 53: loss 0.02167, R2 0.93909\n",
      "epoch 15, batch 54: loss 0.03340, R2 0.88801\n",
      "epoch 15, batch 55: loss 0.05498, R2 0.89156\n",
      "epoch 15, batch 56: loss 0.04503, R2 0.90514\n",
      "epoch 15, batch 57: loss 0.02843, R2 0.91716\n",
      "epoch 15, batch 58: loss 0.05684, R2 0.87878\n",
      "epoch 15, batch 59: loss 0.03508, R2 0.88216\n",
      "epoch 15, batch 60: loss 0.06026, R2 0.86556\n",
      "epoch 15, batch 61: loss 0.08425, R2 0.80656\n",
      "epoch 15, batch 62: loss 0.02436, R2 0.92691\n",
      "epoch 15, batch 63: loss 0.06814, R2 0.85524\n",
      "epoch 15, batch 64: loss 0.04906, R2 0.87638\n",
      "epoch 15, batch 65: loss 0.04923, R2 0.88839\n",
      "epoch 15, batch 66: loss 0.03528, R2 0.92504\n",
      "epoch 15, batch 67: loss 0.04765, R2 0.88903\n",
      "epoch 15, batch 68: loss 0.09774, R2 0.77430\n",
      "epoch 15, batch 69: loss 0.03687, R2 0.91115\n",
      "epoch 15, batch 70: loss 0.02188, R2 0.93328\n",
      "epoch 15, batch 71: loss 0.01892, R2 0.93988\n",
      "epoch 15, batch 72: loss 0.03562, R2 0.92392\n",
      "epoch 15, batch 73: loss 0.03540, R2 0.90341\n",
      "epoch 15, batch 74: loss 0.03497, R2 0.90551\n",
      "epoch 15, batch 75: loss 0.04720, R2 0.88055\n",
      "epoch 15, batch 76: loss 0.10908, R2 0.81923\n",
      "epoch 15, batch 77: loss 0.02866, R2 0.91472\n",
      "epoch 15, batch 78: loss 0.01412, R2 0.94767\n",
      "epoch 15, batch 79: loss 0.04070, R2 0.91599\n",
      "epoch 15, batch 80: loss 0.07772, R2 0.86318\n",
      "epoch 15, batch 81: loss 0.07099, R2 0.84208\n",
      "epoch 15, batch 82: loss 0.02863, R2 0.92957\n",
      "epoch 15, batch 83: loss 0.05756, R2 0.87917\n",
      "epoch 15, batch 84: loss 0.07807, R2 0.82415\n",
      "epoch 15, batch 85: loss 0.07155, R2 0.83779\n",
      "epoch 15, batch 86: loss 0.02923, R2 0.90885\n",
      "epoch 15, batch 87: loss 0.11891, R2 0.76259\n",
      "epoch 15, batch 88: loss 0.05106, R2 0.85333\n",
      "epoch 15, batch 89: loss 0.03318, R2 0.91351\n",
      "epoch 15, batch 90: loss 0.04850, R2 0.89011\n",
      "epoch 15, batch 91: loss 0.04583, R2 0.89167\n",
      "epoch 15, batch 92: loss 0.03566, R2 0.86693\n",
      "epoch 15, batch 93: loss 0.02341, R2 0.93445\n",
      "epoch 15, batch 94: loss 0.07887, R2 0.84588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, batch 95: loss 0.05608, R2 0.85861\n",
      "epoch 15, batch 96: loss 0.02412, R2 0.94470\n",
      "epoch 15, batch 97: loss 0.09746, R2 0.79725\n",
      "epoch 15, batch 98: loss 0.03431, R2 0.91113\n",
      "epoch 15, batch 99: loss 0.05425, R2 0.88318\n",
      "epoch 15, batch 100: loss 0.04949, R2 0.86084\n",
      "epoch 15, batch 101: loss 0.04718, R2 0.89409\n",
      "epoch 15, batch 102: loss 0.04210, R2 0.90588\n",
      "epoch 15, batch 103: loss 0.04503, R2 0.88945\n",
      "epoch 15, batch 104: loss 0.02662, R2 0.93210\n",
      "epoch 15, batch 105: loss 0.03050, R2 0.91593\n",
      "epoch 15, batch 106: loss 0.04540, R2 0.88738\n",
      "epoch 15, batch 107: loss 0.07320, R2 0.81642\n",
      "epoch 15, batch 108: loss 0.05570, R2 0.87840\n",
      "epoch 15, batch 109: loss 0.03023, R2 0.91258\n",
      "epoch 15, batch 110: loss 0.08543, R2 0.75744\n",
      "epoch 15, batch 111: loss 0.04978, R2 0.88106\n",
      "epoch 15, batch 112: loss 0.04919, R2 0.90040\n",
      "epoch 15, batch 113: loss 0.03446, R2 0.92246\n",
      "epoch 15, batch 114: loss 0.09232, R2 0.84776\n",
      "epoch 15, batch 115: loss 0.04383, R2 0.89192\n",
      "epoch 15, batch 116: loss 0.07623, R2 0.84134\n",
      "epoch 15, batch 117: loss 0.10251, R2 0.79781\n",
      "epoch 15, batch 118: loss 0.02782, R2 0.94091\n",
      "epoch 15, batch 119: loss 0.04702, R2 0.83474\n",
      "epoch 15, batch 120: loss 0.07028, R2 0.83242\n",
      "epoch 15, batch 121: loss 0.02583, R2 0.92108\n",
      "epoch 15, batch 122: loss 0.06227, R2 0.84827\n",
      "epoch 15, batch 123: loss 0.05181, R2 0.88151\n",
      "epoch 15, batch 124: loss 0.05676, R2 0.85310\n",
      "epoch 15, batch 125: loss 0.02302, R2 0.93186\n",
      "epoch 15, batch 126: loss 0.05186, R2 0.87017\n",
      "epoch 15, batch 127: loss 0.05910, R2 0.85811\n",
      "epoch 15, batch 128: loss 0.05991, R2 0.87971\n",
      "epoch 15, batch 129: loss 0.04612, R2 0.90436\n",
      "epoch 15, batch 130: loss 0.03343, R2 0.91481\n",
      "epoch 15, batch 131: loss 0.05773, R2 0.84493\n",
      "epoch 15, batch 132: loss 0.02781, R2 0.92731\n",
      "epoch 15, batch 133: loss 0.02848, R2 0.92955\n",
      "epoch 15, batch 134: loss 0.08864, R2 0.81411\n",
      "epoch 15, batch 135: loss 0.05303, R2 0.87197\n",
      "epoch 15, batch 136: loss 0.02322, R2 0.92512\n",
      "epoch 15, batch 137: loss 0.03879, R2 0.90457\n",
      "epoch 15, batch 138: loss 0.01441, R2 0.94867\n",
      "epoch 15, batch 139: loss 0.03257, R2 0.90031\n",
      "epoch 15, batch 140: loss 0.02202, R2 0.92431\n",
      "epoch 15, batch 141: loss 0.03077, R2 0.92187\n",
      "epoch 15, batch 142: loss 0.02497, R2 0.92631\n",
      "epoch 15, batch 143: loss 0.04775, R2 0.88254\n",
      "epoch 15, batch 144: loss 0.07053, R2 0.86631\n",
      "epoch 15, batch 145: loss 0.03422, R2 0.91644\n",
      "epoch 15, batch 146: loss 0.04449, R2 0.81580\n",
      "epoch 15, batch 147: loss 0.07442, R2 0.84884\n",
      "epoch 15, batch 148: loss 0.04444, R2 0.90474\n",
      "epoch 15, batch 149: loss 0.02762, R2 0.92529\n",
      "epoch 15, batch 150: loss 0.07940, R2 0.83411\n",
      "epoch 15, batch 151: loss 0.02854, R2 0.93325\n",
      "epoch 15, batch 152: loss 0.06523, R2 0.83799\n",
      "epoch 15, batch 153: loss 0.12426, R2 0.70777\n",
      "epoch 15, batch 154: loss 0.03197, R2 0.90609\n",
      "epoch 15, batch 155: loss 0.05730, R2 0.86452\n",
      "epoch 15, batch 156: loss 0.13085, R2 0.71910\n",
      "epoch 15, batch 157: loss 0.03464, R2 0.90585\n",
      "epoch 15, batch 158: loss 0.05574, R2 0.87230\n",
      "epoch 15, batch 159: loss 0.05217, R2 0.87877\n",
      "epoch 15, batch 160: loss 0.05729, R2 0.83424\n",
      "epoch 15, batch 161: loss 0.02958, R2 0.91856\n",
      "epoch 15, batch 162: loss 0.05172, R2 0.89723\n",
      "epoch 15, batch 163: loss 0.03787, R2 0.90633\n",
      "epoch 15, batch 164: loss 0.02768, R2 0.90325\n",
      "epoch 15, batch 165: loss 0.02418, R2 0.92585\n",
      "epoch 15, batch 166: loss 0.01875, R2 0.93899\n",
      "epoch 15, batch 167: loss 0.12771, R2 0.63954\n",
      "epoch 15, batch 168: loss 0.11084, R2 0.75340\n",
      "epoch 15, batch 169: loss 0.03005, R2 0.90461\n",
      "epoch 15, batch 170: loss 0.05240, R2 0.89203\n",
      "epoch 15, batch 171: loss 0.03967, R2 0.91460\n",
      "epoch 15, batch 172: loss 0.07288, R2 0.86320\n",
      "epoch 15, batch 173: loss 0.02559, R2 0.93748\n",
      "epoch 15, batch 174: loss 0.03435, R2 0.90973\n",
      "epoch 15, batch 175: loss 0.04456, R2 0.89912\n",
      "epoch 15, batch 176: loss 0.01923, R2 0.92951\n",
      "epoch 15, batch 177: loss 0.06740, R2 0.84547\n",
      "epoch 15, batch 178: loss 0.02948, R2 0.92493\n",
      "epoch 15, batch 179: loss 0.06615, R2 0.84039\n",
      "epoch 15, batch 180: loss 0.06052, R2 0.86861\n",
      "epoch 15, batch 181: loss 0.05314, R2 0.87481\n",
      "epoch 15, batch 182: loss 0.06832, R2 0.83384\n",
      "epoch 15, batch 183: loss 0.06727, R2 0.84389\n",
      "epoch 15, batch 184: loss 0.06118, R2 0.86570\n",
      "epoch 15, batch 185: loss 0.05222, R2 0.88913\n",
      "epoch 15, batch 186: loss 0.03910, R2 0.87183\n",
      "epoch 15, batch 187: loss 0.04868, R2 0.89537\n",
      "epoch 15, batch 188: loss 0.02953, R2 0.92032\n",
      "epoch 15, batch 189: loss 0.01946, R2 0.94757\n",
      "epoch 15, batch 190: loss 0.04302, R2 0.87987\n",
      "epoch 15, batch 191: loss 0.02945, R2 0.92319\n",
      "epoch 15, batch 192: loss 0.04177, R2 0.89169\n",
      "epoch 15, batch 193: loss 0.02559, R2 0.92881\n",
      "epoch 15, batch 194: loss 0.09913, R2 0.72586\n",
      "epoch 15, batch 195: loss 0.10994, R2 0.76653\n",
      "epoch 15, batch 196: loss 0.17799, R2 0.61495\n",
      "epoch 15, batch 197: loss 0.06726, R2 0.82055\n",
      "epoch 15, batch 198: loss 0.01726, R2 0.95415\n",
      "epoch 15, batch 199: loss 0.02533, R2 0.93273\n",
      "epoch 15, batch 200: loss 0.05457, R2 0.87536\n",
      "epoch 15, batch 201: loss 0.02824, R2 0.93363\n",
      "epoch 15, batch 202: loss 0.03655, R2 0.92343\n",
      "epoch 15, batch 203: loss 0.04761, R2 0.89133\n",
      "epoch 15, batch 204: loss 0.03391, R2 0.92312\n",
      "epoch 15, batch 205: loss 0.02629, R2 0.91657\n",
      "epoch 15, batch 206: loss 0.04294, R2 0.91139\n",
      "epoch 15, batch 207: loss 0.05326, R2 0.89014\n",
      "epoch 15, batch 208: loss 0.03889, R2 0.88623\n",
      "epoch 15, batch 209: loss 0.09846, R2 0.78059\n",
      "epoch 15, batch 210: loss 0.04130, R2 0.86599\n",
      "epoch 15, batch 211: loss 0.03324, R2 0.92480\n",
      "epoch 15, batch 212: loss 0.03929, R2 0.87709\n",
      "epoch 15, batch 213: loss 0.02495, R2 0.92656\n",
      "epoch 15, batch 214: loss 0.08539, R2 0.82696\n",
      "epoch 15, batch 215: loss 0.04419, R2 0.89047\n",
      "epoch 15, batch 216: loss 0.09466, R2 0.76375\n",
      "epoch 15, batch 217: loss 0.03319, R2 0.91023\n",
      "epoch 15, batch 218: loss 0.05323, R2 0.89616\n",
      "epoch 15, batch 219: loss 0.04051, R2 0.89975\n",
      "epoch 15, batch 220: loss 0.06215, R2 0.82885\n",
      "epoch 15, batch 221: loss 0.05861, R2 0.87464\n",
      "epoch 15, batch 222: loss 0.03461, R2 0.86959\n",
      "epoch 15, batch 223: loss 0.07352, R2 0.85463\n",
      "epoch 15, batch 224: loss 0.04707, R2 0.86271\n",
      "epoch 15, batch 225: loss 0.05068, R2 0.89088\n",
      "epoch 15, batch 226: loss 0.04878, R2 0.88904\n",
      "epoch 15, batch 227: loss 0.02549, R2 0.94058\n",
      "epoch 15, batch 228: loss 0.06661, R2 0.84566\n",
      "epoch 15, batch 229: loss 0.04205, R2 0.90571\n",
      "epoch 15, batch 230: loss 0.06988, R2 0.84802\n",
      "epoch 15, batch 231: loss 0.06152, R2 0.87778\n",
      "epoch 15, batch 232: loss 0.08505, R2 0.80797\n",
      "epoch 15, batch 233: loss 0.08461, R2 0.82618\n",
      "epoch 15, batch 234: loss 0.02326, R2 0.93691\n",
      "epoch 15, batch 235: loss 0.03213, R2 0.90585\n",
      "epoch 15, batch 236: loss 0.06052, R2 0.86937\n",
      "epoch 15, batch 237: loss 0.01978, R2 0.95134\n",
      "epoch 15, batch 238: loss 0.02320, R2 0.92861\n",
      "epoch 15, batch 239: loss 0.03727, R2 0.91671\n",
      "epoch 15, batch 240: loss 0.03567, R2 0.88836\n",
      "epoch 15, batch 241: loss 0.04670, R2 0.86140\n",
      "epoch 15, batch 242: loss 0.05402, R2 0.86770\n",
      "epoch 15, batch 243: loss 0.03733, R2 0.90231\n",
      "epoch 15, batch 244: loss 0.02684, R2 0.92673\n",
      "epoch 15, batch 245: loss 0.06296, R2 0.87352\n",
      "epoch 15, batch 246: loss 0.03431, R2 0.91163\n",
      "epoch 15, batch 247: loss 0.05579, R2 0.86180\n",
      "epoch 15, batch 248: loss 0.04888, R2 0.89487\n",
      "epoch 15, batch 249: loss 0.03069, R2 0.91082\n",
      "epoch 16, batch 0: loss 0.02767, R2 0.91982\n",
      "epoch 16, batch 1: loss 0.06993, R2 0.85085\n",
      "epoch 16, batch 2: loss 0.03080, R2 0.91966\n",
      "epoch 16, batch 3: loss 0.04622, R2 0.89829\n",
      "epoch 16, batch 4: loss 0.04210, R2 0.89247\n",
      "epoch 16, batch 5: loss 0.03281, R2 0.90205\n",
      "epoch 16, batch 6: loss 0.02840, R2 0.90084\n",
      "epoch 16, batch 7: loss 0.04354, R2 0.90297\n",
      "epoch 16, batch 8: loss 0.02848, R2 0.93184\n",
      "epoch 16, batch 9: loss 0.02867, R2 0.91703\n",
      "epoch 16, batch 10: loss 0.03845, R2 0.90377\n",
      "epoch 16, batch 11: loss 0.02290, R2 0.93189\n",
      "epoch 16, batch 12: loss 0.03021, R2 0.93004\n",
      "epoch 16, batch 13: loss 0.07444, R2 0.79737\n",
      "epoch 16, batch 14: loss 0.05228, R2 0.86357\n",
      "epoch 16, batch 15: loss 0.04717, R2 0.87997\n",
      "epoch 16, batch 16: loss 0.06313, R2 0.88087\n",
      "epoch 16, batch 17: loss 0.05263, R2 0.88245\n",
      "epoch 16, batch 18: loss 0.01997, R2 0.93589\n",
      "epoch 16, batch 19: loss 0.02487, R2 0.87629\n",
      "epoch 16, batch 20: loss 0.02188, R2 0.94585\n",
      "epoch 16, batch 21: loss 0.07827, R2 0.82567\n",
      "epoch 16, batch 22: loss 0.03990, R2 0.90747\n",
      "epoch 16, batch 23: loss 0.05880, R2 0.83556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, batch 24: loss 0.10809, R2 0.74935\n",
      "epoch 16, batch 25: loss 0.05428, R2 0.85642\n",
      "epoch 16, batch 26: loss 0.05976, R2 0.85736\n",
      "epoch 16, batch 27: loss 0.05862, R2 0.86209\n",
      "epoch 16, batch 28: loss 0.03781, R2 0.91155\n",
      "epoch 16, batch 29: loss 0.07676, R2 0.81953\n",
      "epoch 16, batch 30: loss 0.06866, R2 0.81684\n",
      "epoch 16, batch 31: loss 0.01152, R2 0.95117\n",
      "epoch 16, batch 32: loss 0.04964, R2 0.90108\n",
      "epoch 16, batch 33: loss 0.04045, R2 0.87634\n",
      "epoch 16, batch 34: loss 0.04758, R2 0.90310\n",
      "epoch 16, batch 35: loss 0.01457, R2 0.94726\n",
      "epoch 16, batch 36: loss 0.07439, R2 0.84066\n",
      "epoch 16, batch 37: loss 0.04409, R2 0.89548\n",
      "epoch 16, batch 38: loss 0.07717, R2 0.82282\n",
      "epoch 16, batch 39: loss 0.02478, R2 0.91309\n",
      "epoch 16, batch 40: loss 0.03334, R2 0.91331\n",
      "epoch 16, batch 41: loss 0.07158, R2 0.83031\n",
      "epoch 16, batch 42: loss 0.03729, R2 0.87985\n",
      "epoch 16, batch 43: loss 0.07254, R2 0.81229\n",
      "epoch 16, batch 44: loss 0.02074, R2 0.93620\n",
      "epoch 16, batch 45: loss 0.06818, R2 0.86982\n",
      "epoch 16, batch 46: loss 0.03473, R2 0.90311\n",
      "epoch 16, batch 47: loss 0.02992, R2 0.93447\n",
      "epoch 16, batch 48: loss 0.07349, R2 0.79178\n",
      "epoch 16, batch 49: loss 0.05084, R2 0.86829\n",
      "epoch 16, batch 50: loss 0.03627, R2 0.90247\n",
      "epoch 16, batch 51: loss 0.06144, R2 0.88609\n",
      "epoch 16, batch 52: loss 0.08568, R2 0.80067\n",
      "epoch 16, batch 53: loss 0.03486, R2 0.88643\n",
      "epoch 16, batch 54: loss 0.07923, R2 0.82540\n",
      "epoch 16, batch 55: loss 0.02697, R2 0.92349\n",
      "epoch 16, batch 56: loss 0.07012, R2 0.85083\n",
      "epoch 16, batch 57: loss 0.07946, R2 0.83772\n",
      "epoch 16, batch 58: loss 0.05108, R2 0.89574\n",
      "epoch 16, batch 59: loss 0.03732, R2 0.91168\n",
      "epoch 16, batch 60: loss 0.06270, R2 0.88394\n",
      "epoch 16, batch 61: loss 0.04614, R2 0.87008\n",
      "epoch 16, batch 62: loss 0.03790, R2 0.91494\n",
      "epoch 16, batch 63: loss 0.06256, R2 0.85706\n",
      "epoch 16, batch 64: loss 0.02964, R2 0.87026\n",
      "epoch 16, batch 65: loss 0.06847, R2 0.81889\n",
      "epoch 16, batch 66: loss 0.04294, R2 0.86529\n",
      "epoch 16, batch 67: loss 0.12782, R2 0.73335\n",
      "epoch 16, batch 68: loss 0.03049, R2 0.92457\n",
      "epoch 16, batch 69: loss 0.05759, R2 0.85305\n",
      "epoch 16, batch 70: loss 0.03176, R2 0.91058\n",
      "epoch 16, batch 71: loss 0.04576, R2 0.89835\n",
      "epoch 16, batch 72: loss 0.05000, R2 0.89668\n",
      "epoch 16, batch 73: loss 0.05643, R2 0.89038\n",
      "epoch 16, batch 74: loss 0.04230, R2 0.88887\n",
      "epoch 16, batch 75: loss 0.02284, R2 0.94036\n",
      "epoch 16, batch 76: loss 0.03260, R2 0.92407\n",
      "epoch 16, batch 77: loss 0.02806, R2 0.90607\n",
      "epoch 16, batch 78: loss 0.01385, R2 0.94415\n",
      "epoch 16, batch 79: loss 0.03743, R2 0.90099\n",
      "epoch 16, batch 80: loss 0.07092, R2 0.85101\n",
      "epoch 16, batch 81: loss 0.03515, R2 0.90349\n",
      "epoch 16, batch 82: loss 0.04159, R2 0.90234\n",
      "epoch 16, batch 83: loss 0.02578, R2 0.93806\n",
      "epoch 16, batch 84: loss 0.03774, R2 0.91703\n",
      "epoch 16, batch 85: loss 0.03083, R2 0.92932\n",
      "epoch 16, batch 86: loss 0.02439, R2 0.93452\n",
      "epoch 16, batch 87: loss 0.06385, R2 0.86987\n",
      "epoch 16, batch 88: loss 0.06030, R2 0.87151\n",
      "epoch 16, batch 89: loss 0.04590, R2 0.90529\n",
      "epoch 16, batch 90: loss 0.02829, R2 0.93194\n",
      "epoch 16, batch 91: loss 0.03219, R2 0.90849\n",
      "epoch 16, batch 92: loss 0.07516, R2 0.83348\n",
      "epoch 16, batch 93: loss 0.03829, R2 0.90161\n",
      "epoch 16, batch 94: loss 0.05704, R2 0.87396\n",
      "epoch 16, batch 95: loss 0.04323, R2 0.88176\n",
      "epoch 16, batch 96: loss 0.04458, R2 0.88829\n",
      "epoch 16, batch 97: loss 0.02579, R2 0.93206\n",
      "epoch 16, batch 98: loss 0.04955, R2 0.88696\n",
      "epoch 16, batch 99: loss 0.08667, R2 0.79786\n",
      "epoch 16, batch 100: loss 0.07088, R2 0.80114\n",
      "epoch 16, batch 101: loss 0.02541, R2 0.93528\n",
      "epoch 16, batch 102: loss 0.04671, R2 0.88876\n",
      "epoch 16, batch 103: loss 0.05460, R2 0.85461\n",
      "epoch 16, batch 104: loss 0.07172, R2 0.80446\n",
      "epoch 16, batch 105: loss 0.03505, R2 0.91745\n",
      "epoch 16, batch 106: loss 0.08639, R2 0.75073\n",
      "epoch 16, batch 107: loss 0.06533, R2 0.85771\n",
      "epoch 16, batch 108: loss 0.04428, R2 0.88947\n",
      "epoch 16, batch 109: loss 0.02909, R2 0.91494\n",
      "epoch 16, batch 110: loss 0.03416, R2 0.89823\n",
      "epoch 16, batch 111: loss 0.02842, R2 0.93805\n",
      "epoch 16, batch 112: loss 0.03306, R2 0.90455\n",
      "epoch 16, batch 113: loss 0.04856, R2 0.89158\n",
      "epoch 16, batch 114: loss 0.04180, R2 0.87913\n",
      "epoch 16, batch 115: loss 0.08528, R2 0.82435\n",
      "epoch 16, batch 116: loss 0.07604, R2 0.83173\n",
      "epoch 16, batch 117: loss 0.02131, R2 0.94427\n",
      "epoch 16, batch 118: loss 0.04581, R2 0.89121\n",
      "epoch 16, batch 119: loss 0.02824, R2 0.91970\n",
      "epoch 16, batch 120: loss 0.06636, R2 0.80872\n",
      "epoch 16, batch 121: loss 0.04017, R2 0.90583\n",
      "epoch 16, batch 122: loss 0.09052, R2 0.76880\n",
      "epoch 16, batch 123: loss 0.02233, R2 0.94746\n",
      "epoch 16, batch 124: loss 0.09131, R2 0.81323\n",
      "epoch 16, batch 125: loss 0.07129, R2 0.85875\n",
      "epoch 16, batch 126: loss 0.03322, R2 0.91231\n",
      "epoch 16, batch 127: loss 0.02687, R2 0.93253\n",
      "epoch 16, batch 128: loss 0.03459, R2 0.90024\n",
      "epoch 16, batch 129: loss 0.02214, R2 0.93693\n",
      "epoch 16, batch 130: loss 0.03509, R2 0.90779\n",
      "epoch 16, batch 131: loss 0.04524, R2 0.89711\n",
      "epoch 16, batch 132: loss 0.09489, R2 0.80026\n",
      "epoch 16, batch 133: loss 0.05284, R2 0.84742\n",
      "epoch 16, batch 134: loss 0.02781, R2 0.90807\n",
      "epoch 16, batch 135: loss 0.03694, R2 0.91069\n",
      "epoch 16, batch 136: loss 0.03334, R2 0.90154\n",
      "epoch 16, batch 137: loss 0.03420, R2 0.91381\n",
      "epoch 16, batch 138: loss 0.04930, R2 0.88534\n",
      "epoch 16, batch 139: loss 0.05406, R2 0.87502\n",
      "epoch 16, batch 140: loss 0.09121, R2 0.79610\n",
      "epoch 16, batch 141: loss 0.05738, R2 0.87662\n",
      "epoch 16, batch 142: loss 0.04559, R2 0.87438\n",
      "epoch 16, batch 143: loss 0.03538, R2 0.92154\n",
      "epoch 16, batch 144: loss 0.07667, R2 0.78682\n",
      "epoch 16, batch 145: loss 0.05264, R2 0.89752\n",
      "epoch 16, batch 146: loss 0.06528, R2 0.86364\n",
      "epoch 16, batch 147: loss 0.08017, R2 0.81577\n",
      "epoch 16, batch 148: loss 0.04372, R2 0.90482\n",
      "epoch 16, batch 149: loss 0.02419, R2 0.93584\n",
      "epoch 16, batch 150: loss 0.04496, R2 0.90021\n",
      "epoch 16, batch 151: loss 0.04627, R2 0.87014\n",
      "epoch 16, batch 152: loss 0.04493, R2 0.89772\n",
      "epoch 16, batch 153: loss 0.05553, R2 0.87352\n",
      "epoch 16, batch 154: loss 0.01797, R2 0.94361\n",
      "epoch 16, batch 155: loss 0.08588, R2 0.83251\n",
      "epoch 16, batch 156: loss 0.01946, R2 0.93995\n",
      "epoch 16, batch 157: loss 0.03407, R2 0.90173\n",
      "epoch 16, batch 158: loss 0.02714, R2 0.92966\n",
      "epoch 16, batch 159: loss 0.06511, R2 0.84554\n",
      "epoch 16, batch 160: loss 0.05933, R2 0.87875\n",
      "epoch 16, batch 161: loss 0.04578, R2 0.90562\n",
      "epoch 16, batch 162: loss 0.02538, R2 0.92738\n",
      "epoch 16, batch 163: loss 0.02151, R2 0.93092\n",
      "epoch 16, batch 164: loss 0.03273, R2 0.91390\n",
      "epoch 16, batch 165: loss 0.05086, R2 0.87017\n",
      "epoch 16, batch 166: loss 0.06149, R2 0.86912\n",
      "epoch 16, batch 167: loss 0.06767, R2 0.85778\n",
      "epoch 16, batch 168: loss 0.03549, R2 0.92139\n",
      "epoch 16, batch 169: loss 0.06028, R2 0.84331\n",
      "epoch 16, batch 170: loss 0.02929, R2 0.92431\n",
      "epoch 16, batch 171: loss 0.10012, R2 0.80059\n",
      "epoch 16, batch 172: loss 0.04165, R2 0.89340\n",
      "epoch 16, batch 173: loss 0.10740, R2 0.79186\n",
      "epoch 16, batch 174: loss 0.03360, R2 0.91057\n",
      "epoch 16, batch 175: loss 0.08409, R2 0.80951\n",
      "epoch 16, batch 176: loss 0.06667, R2 0.85530\n",
      "epoch 16, batch 177: loss 0.05734, R2 0.86139\n",
      "epoch 16, batch 178: loss 0.06148, R2 0.85953\n",
      "epoch 16, batch 179: loss 0.07415, R2 0.84932\n",
      "epoch 16, batch 180: loss 0.02496, R2 0.93805\n",
      "epoch 16, batch 181: loss 0.02372, R2 0.93184\n",
      "epoch 16, batch 182: loss 0.04595, R2 0.88742\n",
      "epoch 16, batch 183: loss 0.03257, R2 0.85894\n",
      "epoch 16, batch 184: loss 0.09999, R2 0.77171\n",
      "epoch 16, batch 185: loss 0.09955, R2 0.79845\n",
      "epoch 16, batch 186: loss 0.04708, R2 0.88404\n",
      "epoch 16, batch 187: loss 0.08887, R2 0.82007\n",
      "epoch 16, batch 188: loss 0.09447, R2 0.76077\n",
      "epoch 16, batch 189: loss 0.06084, R2 0.86876\n",
      "epoch 16, batch 190: loss 0.03482, R2 0.84967\n",
      "epoch 16, batch 191: loss 0.03185, R2 0.91767\n",
      "epoch 16, batch 192: loss 0.06976, R2 0.87013\n",
      "epoch 16, batch 193: loss 0.05275, R2 0.86730\n",
      "epoch 16, batch 194: loss 0.09119, R2 0.80178\n",
      "epoch 16, batch 195: loss 0.04614, R2 0.88081\n",
      "epoch 16, batch 196: loss 0.02961, R2 0.92973\n",
      "epoch 16, batch 197: loss 0.07518, R2 0.85039\n",
      "epoch 16, batch 198: loss 0.06389, R2 0.85811\n",
      "epoch 16, batch 199: loss 0.08188, R2 0.82045\n",
      "epoch 16, batch 200: loss 0.04971, R2 0.88927\n",
      "epoch 16, batch 201: loss 0.11789, R2 0.73153\n",
      "epoch 16, batch 202: loss 0.02112, R2 0.93503\n",
      "epoch 16, batch 203: loss 0.03127, R2 0.92316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, batch 204: loss 0.06340, R2 0.87013\n",
      "epoch 16, batch 205: loss 0.05504, R2 0.86364\n",
      "epoch 16, batch 206: loss 0.07064, R2 0.83546\n",
      "epoch 16, batch 207: loss 0.05487, R2 0.88464\n",
      "epoch 16, batch 208: loss 0.04580, R2 0.90247\n",
      "epoch 16, batch 209: loss 0.01850, R2 0.93878\n",
      "epoch 16, batch 210: loss 0.03168, R2 0.92278\n",
      "epoch 16, batch 211: loss 0.05056, R2 0.89699\n",
      "epoch 16, batch 212: loss 0.04505, R2 0.89136\n",
      "epoch 16, batch 213: loss 0.02386, R2 0.92577\n",
      "epoch 16, batch 214: loss 0.09948, R2 0.79540\n",
      "epoch 16, batch 215: loss 0.05582, R2 0.85746\n",
      "epoch 16, batch 216: loss 0.01727, R2 0.94871\n",
      "epoch 16, batch 217: loss 0.02168, R2 0.94777\n",
      "epoch 16, batch 218: loss 0.02380, R2 0.94430\n",
      "epoch 16, batch 219: loss 0.03531, R2 0.92961\n",
      "epoch 16, batch 220: loss 0.04178, R2 0.90645\n",
      "epoch 16, batch 221: loss 0.04193, R2 0.89874\n",
      "epoch 16, batch 222: loss 0.04441, R2 0.87348\n",
      "epoch 16, batch 223: loss 0.03046, R2 0.89864\n",
      "epoch 16, batch 224: loss 0.06723, R2 0.83443\n",
      "epoch 16, batch 225: loss 0.04072, R2 0.90119\n",
      "epoch 16, batch 226: loss 0.00976, R2 0.96308\n",
      "epoch 16, batch 227: loss 0.05016, R2 0.88921\n",
      "epoch 16, batch 228: loss 0.08930, R2 0.80921\n",
      "epoch 16, batch 229: loss 0.10658, R2 0.78753\n",
      "epoch 16, batch 230: loss 0.03026, R2 0.92511\n",
      "epoch 16, batch 231: loss 0.03644, R2 0.91144\n",
      "epoch 16, batch 232: loss 0.02348, R2 0.95068\n",
      "epoch 16, batch 233: loss 0.04530, R2 0.90353\n",
      "epoch 16, batch 234: loss 0.02047, R2 0.93714\n",
      "epoch 16, batch 235: loss 0.08286, R2 0.84612\n",
      "epoch 16, batch 236: loss 0.05961, R2 0.86107\n",
      "epoch 16, batch 237: loss 0.06157, R2 0.88254\n",
      "epoch 16, batch 238: loss 0.01693, R2 0.94961\n",
      "epoch 16, batch 239: loss 0.01240, R2 0.95505\n",
      "epoch 16, batch 240: loss 0.09908, R2 0.79549\n",
      "epoch 16, batch 241: loss 0.06368, R2 0.84268\n",
      "epoch 16, batch 242: loss 0.03041, R2 0.91617\n",
      "epoch 16, batch 243: loss 0.03765, R2 0.90418\n",
      "epoch 16, batch 244: loss 0.04636, R2 0.89425\n",
      "epoch 16, batch 245: loss 0.05746, R2 0.86951\n",
      "epoch 16, batch 246: loss 0.03833, R2 0.87116\n",
      "epoch 16, batch 247: loss 0.06791, R2 0.86456\n",
      "epoch 16, batch 248: loss 0.07325, R2 0.78758\n",
      "epoch 16, batch 249: loss 0.05534, R2 0.88834\n",
      "epoch 17, batch 0: loss 0.08228, R2 0.81666\n",
      "epoch 17, batch 1: loss 0.08927, R2 0.79626\n",
      "epoch 17, batch 2: loss 0.02158, R2 0.95033\n",
      "epoch 17, batch 3: loss 0.01833, R2 0.95228\n",
      "epoch 17, batch 4: loss 0.02957, R2 0.92276\n",
      "epoch 17, batch 5: loss 0.05876, R2 0.85212\n",
      "epoch 17, batch 6: loss 0.10997, R2 0.80369\n",
      "epoch 17, batch 7: loss 0.02288, R2 0.93399\n",
      "epoch 17, batch 8: loss 0.03230, R2 0.92319\n",
      "epoch 17, batch 9: loss 0.02824, R2 0.91199\n",
      "epoch 17, batch 10: loss 0.02067, R2 0.94260\n",
      "epoch 17, batch 11: loss 0.02160, R2 0.94518\n",
      "epoch 17, batch 12: loss 0.03534, R2 0.90298\n",
      "epoch 17, batch 13: loss 0.04727, R2 0.89969\n",
      "epoch 17, batch 14: loss 0.02497, R2 0.93737\n",
      "epoch 17, batch 15: loss 0.02029, R2 0.92841\n",
      "epoch 17, batch 16: loss 0.05057, R2 0.87100\n",
      "epoch 17, batch 17: loss 0.04899, R2 0.89858\n",
      "epoch 17, batch 18: loss 0.03709, R2 0.87296\n",
      "epoch 17, batch 19: loss 0.04351, R2 0.88894\n",
      "epoch 17, batch 20: loss 0.11073, R2 0.77984\n",
      "epoch 17, batch 21: loss 0.04477, R2 0.89706\n",
      "epoch 17, batch 22: loss 0.05810, R2 0.86202\n",
      "epoch 17, batch 23: loss 0.06214, R2 0.85944\n",
      "epoch 17, batch 24: loss 0.04461, R2 0.89339\n",
      "epoch 17, batch 25: loss 0.06083, R2 0.85671\n",
      "epoch 17, batch 26: loss 0.02809, R2 0.93569\n",
      "epoch 17, batch 27: loss 0.05389, R2 0.89909\n",
      "epoch 17, batch 28: loss 0.03775, R2 0.85905\n",
      "epoch 17, batch 29: loss 0.09231, R2 0.83222\n",
      "epoch 17, batch 30: loss 0.09759, R2 0.78316\n",
      "epoch 17, batch 31: loss 0.07895, R2 0.81783\n",
      "epoch 17, batch 32: loss 0.06750, R2 0.85425\n",
      "epoch 17, batch 33: loss 0.04368, R2 0.87520\n",
      "epoch 17, batch 34: loss 0.09573, R2 0.79155\n",
      "epoch 17, batch 35: loss 0.04323, R2 0.89921\n",
      "epoch 17, batch 36: loss 0.02191, R2 0.94305\n",
      "epoch 17, batch 37: loss 0.05521, R2 0.88421\n",
      "epoch 17, batch 38: loss 0.03198, R2 0.93381\n",
      "epoch 17, batch 39: loss 0.08368, R2 0.80084\n",
      "epoch 17, batch 40: loss 0.11537, R2 0.68235\n",
      "epoch 17, batch 41: loss 0.02393, R2 0.91279\n",
      "epoch 17, batch 42: loss 0.03333, R2 0.89762\n",
      "epoch 17, batch 43: loss 0.03668, R2 0.92014\n",
      "epoch 17, batch 44: loss 0.04210, R2 0.89975\n",
      "epoch 17, batch 45: loss 0.02262, R2 0.93257\n",
      "epoch 17, batch 46: loss 0.07984, R2 0.82462\n",
      "epoch 17, batch 47: loss 0.07652, R2 0.83847\n",
      "epoch 17, batch 48: loss 0.03865, R2 0.91269\n",
      "epoch 17, batch 49: loss 0.04554, R2 0.89104\n",
      "epoch 17, batch 50: loss 0.03859, R2 0.90255\n",
      "epoch 17, batch 51: loss 0.05538, R2 0.85768\n",
      "epoch 17, batch 52: loss 0.11533, R2 0.77660\n",
      "epoch 17, batch 53: loss 0.04631, R2 0.88816\n",
      "epoch 17, batch 54: loss 0.06959, R2 0.82518\n",
      "epoch 17, batch 55: loss 0.04649, R2 0.91133\n",
      "epoch 17, batch 56: loss 0.02780, R2 0.93929\n",
      "epoch 17, batch 57: loss 0.01150, R2 0.95414\n",
      "epoch 17, batch 58: loss 0.05649, R2 0.84667\n",
      "epoch 17, batch 59: loss 0.07275, R2 0.85000\n",
      "epoch 17, batch 60: loss 0.05382, R2 0.88364\n",
      "epoch 17, batch 61: loss 0.04614, R2 0.87885\n",
      "epoch 17, batch 62: loss 0.03220, R2 0.91490\n",
      "epoch 17, batch 63: loss 0.04123, R2 0.88187\n",
      "epoch 17, batch 64: loss 0.01987, R2 0.93622\n",
      "epoch 17, batch 65: loss 0.01845, R2 0.93296\n",
      "epoch 17, batch 66: loss 0.06282, R2 0.85712\n",
      "epoch 17, batch 67: loss 0.08628, R2 0.80125\n",
      "epoch 17, batch 68: loss 0.01549, R2 0.95345\n",
      "epoch 17, batch 69: loss 0.03806, R2 0.90699\n",
      "epoch 17, batch 70: loss 0.06601, R2 0.86194\n",
      "epoch 17, batch 71: loss 0.01936, R2 0.94370\n",
      "epoch 17, batch 72: loss 0.04851, R2 0.87790\n",
      "epoch 17, batch 73: loss 0.05142, R2 0.87916\n",
      "epoch 17, batch 74: loss 0.02055, R2 0.93755\n",
      "epoch 17, batch 75: loss 0.03016, R2 0.91858\n",
      "epoch 17, batch 76: loss 0.05919, R2 0.86763\n",
      "epoch 17, batch 77: loss 0.05520, R2 0.86916\n",
      "epoch 17, batch 78: loss 0.06048, R2 0.86527\n",
      "epoch 17, batch 79: loss 0.02531, R2 0.94155\n",
      "epoch 17, batch 80: loss 0.10392, R2 0.74557\n",
      "epoch 17, batch 81: loss 0.02831, R2 0.88687\n",
      "epoch 17, batch 82: loss 0.03528, R2 0.90654\n",
      "epoch 17, batch 83: loss 0.07356, R2 0.84726\n",
      "epoch 17, batch 84: loss 0.03111, R2 0.88892\n",
      "epoch 17, batch 85: loss 0.04063, R2 0.91409\n",
      "epoch 17, batch 86: loss 0.10313, R2 0.78742\n",
      "epoch 17, batch 87: loss 0.03868, R2 0.89954\n",
      "epoch 17, batch 88: loss 0.07907, R2 0.84281\n",
      "epoch 17, batch 89: loss 0.09082, R2 0.80023\n",
      "epoch 17, batch 90: loss 0.03958, R2 0.88801\n",
      "epoch 17, batch 91: loss 0.03578, R2 0.89948\n",
      "epoch 17, batch 92: loss 0.06068, R2 0.85541\n",
      "epoch 17, batch 93: loss 0.04809, R2 0.88969\n",
      "epoch 17, batch 94: loss 0.03378, R2 0.84696\n",
      "epoch 17, batch 95: loss 0.03390, R2 0.89432\n",
      "epoch 17, batch 96: loss 0.03941, R2 0.89101\n",
      "epoch 17, batch 97: loss 0.04563, R2 0.89698\n",
      "epoch 17, batch 98: loss 0.03132, R2 0.91533\n",
      "epoch 17, batch 99: loss 0.04875, R2 0.87689\n",
      "epoch 17, batch 100: loss 0.07022, R2 0.84198\n",
      "epoch 17, batch 101: loss 0.09380, R2 0.78388\n",
      "epoch 17, batch 102: loss 0.04860, R2 0.89604\n",
      "epoch 17, batch 103: loss 0.03883, R2 0.87226\n",
      "epoch 17, batch 104: loss 0.05042, R2 0.88614\n",
      "epoch 17, batch 105: loss 0.03178, R2 0.92648\n",
      "epoch 17, batch 106: loss 0.02085, R2 0.93435\n",
      "epoch 17, batch 107: loss 0.02160, R2 0.93637\n",
      "epoch 17, batch 108: loss 0.09344, R2 0.82425\n",
      "epoch 17, batch 109: loss 0.03468, R2 0.90244\n",
      "epoch 17, batch 110: loss 0.02849, R2 0.93071\n",
      "epoch 17, batch 111: loss 0.03747, R2 0.88042\n",
      "epoch 17, batch 112: loss 0.09878, R2 0.77848\n",
      "epoch 17, batch 113: loss 0.03400, R2 0.91317\n",
      "epoch 17, batch 114: loss 0.03713, R2 0.85299\n",
      "epoch 17, batch 115: loss 0.04125, R2 0.88090\n",
      "epoch 17, batch 116: loss 0.02792, R2 0.92602\n",
      "epoch 17, batch 117: loss 0.02824, R2 0.91316\n",
      "epoch 17, batch 118: loss 0.05859, R2 0.86179\n",
      "epoch 17, batch 119: loss 0.05500, R2 0.88804\n",
      "epoch 17, batch 120: loss 0.04595, R2 0.90260\n",
      "epoch 17, batch 121: loss 0.05319, R2 0.88295\n",
      "epoch 17, batch 122: loss 0.02581, R2 0.92179\n",
      "epoch 17, batch 123: loss 0.04509, R2 0.90483\n",
      "epoch 17, batch 124: loss 0.08323, R2 0.83285\n",
      "epoch 17, batch 125: loss 0.05782, R2 0.86414\n",
      "epoch 17, batch 126: loss 0.03742, R2 0.90407\n",
      "epoch 17, batch 127: loss 0.03193, R2 0.92279\n",
      "epoch 17, batch 128: loss 0.01065, R2 0.96021\n",
      "epoch 17, batch 129: loss 0.06620, R2 0.85058\n",
      "epoch 17, batch 130: loss 0.11967, R2 0.75274\n",
      "epoch 17, batch 131: loss 0.03376, R2 0.92424\n",
      "epoch 17, batch 132: loss 0.02868, R2 0.92577\n",
      "epoch 17, batch 133: loss 0.07979, R2 0.83146\n",
      "epoch 17, batch 134: loss 0.06341, R2 0.86665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, batch 135: loss 0.05594, R2 0.88959\n",
      "epoch 17, batch 136: loss 0.09950, R2 0.79009\n",
      "epoch 17, batch 137: loss 0.04248, R2 0.88283\n",
      "epoch 17, batch 138: loss 0.07193, R2 0.80801\n",
      "epoch 17, batch 139: loss 0.04656, R2 0.90141\n",
      "epoch 17, batch 140: loss 0.06592, R2 0.87191\n",
      "epoch 17, batch 141: loss 0.04836, R2 0.88818\n",
      "epoch 17, batch 142: loss 0.02379, R2 0.93222\n",
      "epoch 17, batch 143: loss 0.03523, R2 0.90542\n",
      "epoch 17, batch 144: loss 0.05072, R2 0.88211\n",
      "epoch 17, batch 145: loss 0.02144, R2 0.91886\n",
      "epoch 17, batch 146: loss 0.05433, R2 0.87816\n",
      "epoch 17, batch 147: loss 0.04141, R2 0.88891\n",
      "epoch 17, batch 148: loss 0.02264, R2 0.93291\n",
      "epoch 17, batch 149: loss 0.04689, R2 0.86955\n",
      "epoch 17, batch 150: loss 0.05365, R2 0.87615\n",
      "epoch 17, batch 151: loss 0.04171, R2 0.88240\n",
      "epoch 17, batch 152: loss 0.03007, R2 0.91406\n",
      "epoch 17, batch 153: loss 0.03141, R2 0.91596\n",
      "epoch 17, batch 154: loss 0.08669, R2 0.81374\n",
      "epoch 17, batch 155: loss 0.03852, R2 0.87826\n",
      "epoch 17, batch 156: loss 0.03394, R2 0.91607\n",
      "epoch 17, batch 157: loss 0.03330, R2 0.90243\n",
      "epoch 17, batch 158: loss 0.04118, R2 0.84197\n",
      "epoch 17, batch 159: loss 0.02758, R2 0.91847\n",
      "epoch 17, batch 160: loss 0.02974, R2 0.92145\n",
      "epoch 17, batch 161: loss 0.05830, R2 0.85004\n",
      "epoch 17, batch 162: loss 0.04788, R2 0.90536\n",
      "epoch 17, batch 163: loss 0.10913, R2 0.77485\n",
      "epoch 17, batch 164: loss 0.04090, R2 0.91379\n",
      "epoch 17, batch 165: loss 0.09113, R2 0.78803\n",
      "epoch 17, batch 166: loss 0.04597, R2 0.85335\n",
      "epoch 17, batch 167: loss 0.04753, R2 0.89708\n",
      "epoch 17, batch 168: loss 0.01426, R2 0.95188\n",
      "epoch 17, batch 169: loss 0.06248, R2 0.86536\n",
      "epoch 17, batch 170: loss 0.07492, R2 0.80081\n",
      "epoch 17, batch 171: loss 0.06531, R2 0.85771\n",
      "epoch 17, batch 172: loss 0.02421, R2 0.92933\n",
      "epoch 17, batch 173: loss 0.06859, R2 0.84481\n",
      "epoch 17, batch 174: loss 0.03601, R2 0.88199\n",
      "epoch 17, batch 175: loss 0.01820, R2 0.95693\n",
      "epoch 17, batch 176: loss 0.03780, R2 0.92181\n",
      "epoch 17, batch 177: loss 0.07323, R2 0.84685\n",
      "epoch 17, batch 178: loss 0.02510, R2 0.92704\n",
      "epoch 17, batch 179: loss 0.04627, R2 0.88804\n",
      "epoch 17, batch 180: loss 0.03874, R2 0.91843\n",
      "epoch 17, batch 181: loss 0.03450, R2 0.90076\n",
      "epoch 17, batch 182: loss 0.06531, R2 0.87137\n",
      "epoch 17, batch 183: loss 0.10170, R2 0.75276\n",
      "epoch 17, batch 184: loss 0.03582, R2 0.91926\n",
      "epoch 17, batch 185: loss 0.05203, R2 0.86263\n",
      "epoch 17, batch 186: loss 0.02523, R2 0.90935\n",
      "epoch 17, batch 187: loss 0.06397, R2 0.87759\n",
      "epoch 17, batch 188: loss 0.01898, R2 0.93838\n",
      "epoch 17, batch 189: loss 0.05276, R2 0.88373\n",
      "epoch 17, batch 190: loss 0.07921, R2 0.81675\n",
      "epoch 17, batch 191: loss 0.03990, R2 0.88993\n",
      "epoch 17, batch 192: loss 0.09839, R2 0.80434\n",
      "epoch 17, batch 193: loss 0.07809, R2 0.84455\n",
      "epoch 17, batch 194: loss 0.02727, R2 0.90176\n",
      "epoch 17, batch 195: loss 0.04089, R2 0.90639\n",
      "epoch 17, batch 196: loss 0.02130, R2 0.93845\n",
      "epoch 17, batch 197: loss 0.05757, R2 0.87444\n",
      "epoch 17, batch 198: loss 0.04944, R2 0.90227\n",
      "epoch 17, batch 199: loss 0.03611, R2 0.89654\n",
      "epoch 17, batch 200: loss 0.06494, R2 0.84437\n",
      "epoch 17, batch 201: loss 0.05688, R2 0.88524\n",
      "epoch 17, batch 202: loss 0.04584, R2 0.87969\n",
      "epoch 17, batch 203: loss 0.04803, R2 0.86932\n",
      "epoch 17, batch 204: loss 0.08383, R2 0.79591\n",
      "epoch 17, batch 205: loss 0.03924, R2 0.89428\n",
      "epoch 17, batch 206: loss 0.06414, R2 0.86417\n",
      "epoch 17, batch 207: loss 0.08652, R2 0.81488\n",
      "epoch 17, batch 208: loss 0.05313, R2 0.88776\n",
      "epoch 17, batch 209: loss 0.05765, R2 0.87309\n",
      "epoch 17, batch 210: loss 0.04939, R2 0.90216\n",
      "epoch 17, batch 211: loss 0.14283, R2 0.69865\n",
      "epoch 17, batch 212: loss 0.02483, R2 0.93325\n",
      "epoch 17, batch 213: loss 0.08935, R2 0.79602\n",
      "epoch 17, batch 214: loss 0.02477, R2 0.93249\n",
      "epoch 17, batch 215: loss 0.02518, R2 0.93541\n",
      "epoch 17, batch 216: loss 0.05330, R2 0.87159\n",
      "epoch 17, batch 217: loss 0.04602, R2 0.90181\n",
      "epoch 17, batch 218: loss 0.05157, R2 0.86476\n",
      "epoch 17, batch 219: loss 0.08361, R2 0.79388\n",
      "epoch 17, batch 220: loss 0.02908, R2 0.92495\n",
      "epoch 17, batch 221: loss 0.03190, R2 0.88195\n",
      "epoch 17, batch 222: loss 0.04795, R2 0.88433\n",
      "epoch 17, batch 223: loss 0.04857, R2 0.87610\n",
      "epoch 17, batch 224: loss 0.03302, R2 0.92112\n",
      "epoch 17, batch 225: loss 0.04996, R2 0.88373\n",
      "epoch 17, batch 226: loss 0.04124, R2 0.90617\n",
      "epoch 17, batch 227: loss 0.05394, R2 0.88519\n",
      "epoch 17, batch 228: loss 0.03041, R2 0.93350\n",
      "epoch 17, batch 229: loss 0.05696, R2 0.87334\n",
      "epoch 17, batch 230: loss 0.02391, R2 0.93801\n",
      "epoch 17, batch 231: loss 0.03892, R2 0.90291\n",
      "epoch 17, batch 232: loss 0.05293, R2 0.87405\n",
      "epoch 17, batch 233: loss 0.03382, R2 0.91910\n",
      "epoch 17, batch 234: loss 0.06091, R2 0.86095\n",
      "epoch 17, batch 235: loss 0.04411, R2 0.89191\n",
      "epoch 17, batch 236: loss 0.02947, R2 0.92601\n",
      "epoch 17, batch 237: loss 0.04278, R2 0.89611\n",
      "epoch 17, batch 238: loss 0.02500, R2 0.93061\n",
      "epoch 17, batch 239: loss 0.01369, R2 0.94734\n",
      "epoch 17, batch 240: loss 0.06451, R2 0.85289\n",
      "epoch 17, batch 241: loss 0.02709, R2 0.93279\n",
      "epoch 17, batch 242: loss 0.02817, R2 0.91236\n",
      "epoch 17, batch 243: loss 0.10417, R2 0.74905\n",
      "epoch 17, batch 244: loss 0.03480, R2 0.91615\n",
      "epoch 17, batch 245: loss 0.03973, R2 0.89681\n",
      "epoch 17, batch 246: loss 0.05874, R2 0.86448\n",
      "epoch 17, batch 247: loss 0.04829, R2 0.87775\n",
      "epoch 17, batch 248: loss 0.09991, R2 0.71606\n",
      "epoch 17, batch 249: loss 0.04031, R2 0.91083\n",
      "epoch 18, batch 0: loss 0.06788, R2 0.82257\n",
      "epoch 18, batch 1: loss 0.03559, R2 0.92051\n",
      "epoch 18, batch 2: loss 0.05039, R2 0.89623\n",
      "epoch 18, batch 3: loss 0.06435, R2 0.82537\n",
      "epoch 18, batch 4: loss 0.08683, R2 0.81327\n",
      "epoch 18, batch 5: loss 0.04854, R2 0.89081\n",
      "epoch 18, batch 6: loss 0.03520, R2 0.90536\n",
      "epoch 18, batch 7: loss 0.02811, R2 0.93298\n",
      "epoch 18, batch 8: loss 0.03251, R2 0.90284\n",
      "epoch 18, batch 9: loss 0.03551, R2 0.91233\n",
      "epoch 18, batch 10: loss 0.02012, R2 0.94035\n",
      "epoch 18, batch 11: loss 0.05407, R2 0.87866\n",
      "epoch 18, batch 12: loss 0.02865, R2 0.91499\n",
      "epoch 18, batch 13: loss 0.03484, R2 0.87211\n",
      "epoch 18, batch 14: loss 0.05425, R2 0.87303\n",
      "epoch 18, batch 15: loss 0.02106, R2 0.94470\n",
      "epoch 18, batch 16: loss 0.03016, R2 0.93333\n",
      "epoch 18, batch 17: loss 0.05918, R2 0.88444\n",
      "epoch 18, batch 18: loss 0.13475, R2 0.73053\n",
      "epoch 18, batch 19: loss 0.04780, R2 0.88335\n",
      "epoch 18, batch 20: loss 0.02019, R2 0.90879\n",
      "epoch 18, batch 21: loss 0.02220, R2 0.94720\n",
      "epoch 18, batch 22: loss 0.02904, R2 0.92655\n",
      "epoch 18, batch 23: loss 0.03753, R2 0.92486\n",
      "epoch 18, batch 24: loss 0.08400, R2 0.83825\n",
      "epoch 18, batch 25: loss 0.02182, R2 0.93939\n",
      "epoch 18, batch 26: loss 0.02864, R2 0.91774\n",
      "epoch 18, batch 27: loss 0.10240, R2 0.78520\n",
      "epoch 18, batch 28: loss 0.06944, R2 0.85015\n",
      "epoch 18, batch 29: loss 0.04304, R2 0.90209\n",
      "epoch 18, batch 30: loss 0.05656, R2 0.80905\n",
      "epoch 18, batch 31: loss 0.05135, R2 0.88012\n",
      "epoch 18, batch 32: loss 0.04270, R2 0.88880\n",
      "epoch 18, batch 33: loss 0.02051, R2 0.94097\n",
      "epoch 18, batch 34: loss 0.06532, R2 0.87793\n",
      "epoch 18, batch 35: loss 0.06362, R2 0.85058\n",
      "epoch 18, batch 36: loss 0.06828, R2 0.84595\n",
      "epoch 18, batch 37: loss 0.03305, R2 0.91682\n",
      "epoch 18, batch 38: loss 0.02411, R2 0.93471\n",
      "epoch 18, batch 39: loss 0.03724, R2 0.91721\n",
      "epoch 18, batch 40: loss 0.10984, R2 0.75135\n",
      "epoch 18, batch 41: loss 0.06544, R2 0.85729\n",
      "epoch 18, batch 42: loss 0.02931, R2 0.92135\n",
      "epoch 18, batch 43: loss 0.02951, R2 0.92470\n",
      "epoch 18, batch 44: loss 0.07478, R2 0.82437\n",
      "epoch 18, batch 45: loss 0.07644, R2 0.83432\n",
      "epoch 18, batch 46: loss 0.13916, R2 0.65737\n",
      "epoch 18, batch 47: loss 0.02185, R2 0.94437\n",
      "epoch 18, batch 48: loss 0.08196, R2 0.80181\n",
      "epoch 18, batch 49: loss 0.05619, R2 0.87403\n",
      "epoch 18, batch 50: loss 0.07397, R2 0.83407\n",
      "epoch 18, batch 51: loss 0.04304, R2 0.90047\n",
      "epoch 18, batch 52: loss 0.07459, R2 0.85518\n",
      "epoch 18, batch 53: loss 0.03492, R2 0.91456\n",
      "epoch 18, batch 54: loss 0.03152, R2 0.91517\n",
      "epoch 18, batch 55: loss 0.04943, R2 0.89454\n",
      "epoch 18, batch 56: loss 0.02681, R2 0.90549\n",
      "epoch 18, batch 57: loss 0.05977, R2 0.83300\n",
      "epoch 18, batch 58: loss 0.04309, R2 0.90247\n",
      "epoch 18, batch 59: loss 0.03568, R2 0.92619\n",
      "epoch 18, batch 60: loss 0.04006, R2 0.89974\n",
      "epoch 18, batch 61: loss 0.06845, R2 0.83648\n",
      "epoch 18, batch 62: loss 0.03565, R2 0.89658\n",
      "epoch 18, batch 63: loss 0.05886, R2 0.87989\n",
      "epoch 18, batch 64: loss 0.13130, R2 0.68800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, batch 65: loss 0.02525, R2 0.92943\n",
      "epoch 18, batch 66: loss 0.04266, R2 0.90126\n",
      "epoch 18, batch 67: loss 0.05552, R2 0.85146\n",
      "epoch 18, batch 68: loss 0.06236, R2 0.82756\n",
      "epoch 18, batch 69: loss 0.05917, R2 0.86126\n",
      "epoch 18, batch 70: loss 0.09166, R2 0.76200\n",
      "epoch 18, batch 71: loss 0.04167, R2 0.89249\n",
      "epoch 18, batch 72: loss 0.02437, R2 0.92984\n",
      "epoch 18, batch 73: loss 0.03971, R2 0.90838\n",
      "epoch 18, batch 74: loss 0.03927, R2 0.88326\n",
      "epoch 18, batch 75: loss 0.02728, R2 0.93010\n",
      "epoch 18, batch 76: loss 0.02849, R2 0.93765\n",
      "epoch 18, batch 77: loss 0.02653, R2 0.92596\n",
      "epoch 18, batch 78: loss 0.06323, R2 0.85478\n",
      "epoch 18, batch 79: loss 0.08372, R2 0.84000\n",
      "epoch 18, batch 80: loss 0.03697, R2 0.90198\n",
      "epoch 18, batch 81: loss 0.03623, R2 0.90810\n",
      "epoch 18, batch 82: loss 0.02678, R2 0.92462\n",
      "epoch 18, batch 83: loss 0.02667, R2 0.93696\n",
      "epoch 18, batch 84: loss 0.02610, R2 0.94077\n",
      "epoch 18, batch 85: loss 0.02866, R2 0.93125\n",
      "epoch 18, batch 86: loss 0.03264, R2 0.91488\n",
      "epoch 18, batch 87: loss 0.07669, R2 0.84988\n",
      "epoch 18, batch 88: loss 0.03558, R2 0.89017\n",
      "epoch 18, batch 89: loss 0.03751, R2 0.90455\n",
      "epoch 18, batch 90: loss 0.05798, R2 0.84949\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import GNN\n",
    "from dataset import MyGraphDataset, load_wave_data, normalise_data\n",
    "from config import Config\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "from scipy.io import savemat\n",
    "\n",
    "cfg = Config(\n",
    "    snapshots=3,\n",
    "    data_dir=\"\", \n",
    "    neighbourhood_size=5,\n",
    "    normalization=\"norm_01\",\n",
    "    train_period=(0, 1000),\n",
    "    test_period=(1000, 1400),\n",
    "    convolution_kernels = (64, 128),\n",
    "    node_var_observ= ['hs', 'ub_bot', 'wlen', 'pwave_bot', 'tpeak', 'dirm'],\n",
    "    node_var_target=['hs', 'ub_bot', 'wlen', 'pwave_bot', 'tpeak', 'dirm'],\n",
    "    train_batch_size=4,\n",
    "    forward_time=1,\n",
    "    max_epoches=100,\n",
    "    test_batch_size=3,\n",
    ")\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)   \n",
    "    \n",
    "def main(config:Config):\n",
    "    trn_dl, test_dl = prepare_train_test_dataloaders(cfg)\n",
    "    # setup the model and optimiser\n",
    "    model = GNN(cfg)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    r2_scores = []\n",
    "    mse =[]\n",
    " \n",
    "    # Create lists to store training and testing data\n",
    "    #train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for epoch in range(cfg.max_epoches):\n",
    "        for i, (data_x, y) in enumerate(trn_dl):\n",
    "            optimizer.zero_grad()            #clar gradients from previous iteration\n",
    "\n",
    "            # compute the loss for this batch\n",
    "            # TODO: to wrap in a proper loss function\n",
    "            tmp = model(data_x.x, data_x.edge_index)\n",
    "            batch_pred = unbatch(tmp, data_x.batch)\n",
    "            \n",
    "            loss = 0\n",
    "            for pi, yi in zip(batch_pred, y):\n",
    "                loss += loss_fn(pi, yi)\n",
    "\n",
    "            \n",
    "            loss.backward()                   #gradients of the model's parameters are computed with respect to the loss using backpropagation\n",
    "            optimizer.step()                  #The optimizer updates the model's parameters based on the computed gradients using the chosen optimization algorithm (e.g., Adam)\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            y_true_int = y.detach().numpy()\n",
    "            y_true = y_true_int.reshape(-1, 6)\n",
    "            y_pred = torch.cat(batch_pred).detach().numpy()\n",
    "            ss_res = np.sum((y_true - y_pred)**2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            r2_scores.append(r2)\n",
    "            \n",
    "    \n",
    "            print(f\"epoch {epoch}, batch {i}: loss {loss.item():.5f}, R2 {r2:.5f}\")\n",
    "    \n",
    "    # Test the model using test data\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_r2 = 0\n",
    "    test_loss = 0\n",
    "    for i, (data_x, y) in enumerate(test_dl):\n",
    "        test_data_out = model(data_x.x, data_x.edge_index)\n",
    "        batch_pred = unbatch(test_data_out, data_x.batch)\n",
    "        \n",
    "        y_true_int = y.detach().numpy()\n",
    "        y_true = y_true_int.reshape(-1, 6)\n",
    "        y_pred = torch.cat(batch_pred).detach().numpy()\n",
    "        ss_res = np.sum((y_true - y_pred)**2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        test_r2 += r2\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        test_loss += mse\n",
    "        \n",
    "         # Create dictionaries for lon, lat, time, and variable data\n",
    "        data = load_wave_data(Config)\n",
    "        lon = data['lon']\n",
    "        lat = data['lat']\n",
    "        time= data['t']\n",
    "        lon_lat_time_data = {\n",
    "            'lon': lon,  # Use data_x.node_index as an index to extract lon values\n",
    "            'lat': lat,  # Use data_x.node_index as an index to extract lat values\n",
    "            'time': time,  # Use data_x.time as an index to extract time values\n",
    "        }\n",
    "        \n",
    "        variable_pred_data = {\n",
    "            'hs': y_pred[:, 0],  # Adjust the index as needed\n",
    "            'ub_bot': y_pred[:, 1],  # Adjust the index as needed\n",
    "            'wlen': y_pred[:, 2],  # Adjust the index as needed\n",
    "            'pwave_bot': y_pred[:, 3],  # Adjust the index as needed\n",
    "            'tpeak': y_pred[:, 4],  # Adjust the index as needed\n",
    "            'dirm' : y_pred[:, 5],\n",
    "        }\n",
    "        \n",
    "        variable_true_data = {\n",
    "            'hs': y_true[:, 0],  # Adjust the index as needed\n",
    "            'ub_bot': y_true[:, 1],  # Adjust the index as needed\n",
    "            'wlen': y_true[:, 2],  # Adjust the index as needed\n",
    "            'pwave_bot': y_true[:, 3],  # Adjust the index as needed\n",
    "            'tpeak': y_true[:, 4],  # Adjust the index as needed\n",
    "            'dirm' : y_true[:, 5],\n",
    "        }\n",
    "        \n",
    "        test_data.append({\n",
    "            'lon_lat_time_data': lon_lat_time_data,\n",
    "            'variable_pred_data': variable_pred_data,\n",
    "            'variable_true_data': variable_true_data\n",
    "        })\n",
    "                \n",
    "    test_r2 /= len(test_dl)    \n",
    "    # Calculate average test loss (mean squared error)\n",
    "    average_mse = test_loss / len(test_dl)\n",
    "    # Calculate RMSE from average MSE\n",
    "    rmse = math.sqrt(average_mse)\n",
    "\n",
    "    print(f\"Test R2: {test_r2:.5f}, loss {rmse:.5f}\")     \n",
    "    \n",
    "    # Convert the lists to tensors if needed\n",
    "    test_data = np.array(test_data)\n",
    "    \n",
    "    # Save training and testing data to separate files\n",
    "    save_data(test_data, 'test.pkl')\n",
    "\n",
    "    print(\"testing data saved.\")\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    main(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfa5e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fb51e4-2a1d-42a4-b642-0f5f1f3def8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c15238-780b-4a9c-968f-af9af1e99c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
